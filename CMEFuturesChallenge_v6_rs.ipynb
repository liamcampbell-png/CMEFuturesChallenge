{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef69f29",
   "metadata": {},
   "source": [
    "# Notebook for CME Futures Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64da73",
   "metadata": {},
   "source": [
    "# Downloading historical data for indices (S&P, NASDAQ, DJIA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f4420",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f8bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import re\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8196f13",
   "metadata": {},
   "source": [
    "Make get_data function for downloading from yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe = '1000mo' # set timeframe\n",
    "\n",
    "def get_data(tickers: List):\n",
    "    data_dictionary = {}\n",
    "    for ticker in tickers:\n",
    "        data_dictionary[ticker] = yf.download(ticker, period=timeframe, interval='1d')\n",
    "    return data_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fd38e6",
   "metadata": {},
   "source": [
    "# Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e00db",
   "metadata": {},
   "source": [
    "We need continuized data for returns information about futures since they roll over at expiration dates. I bought some continuous data for cheap, it's not back adjusted, which we could do - but I may try doing CV folds on each contract instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_df = pd.read_csv('ES.txt') # E S&P\n",
    "nq_df = pd.read_csv('NQ.txt') # E Nasdaq\n",
    "fv_df = pd.read_csv('FV.txt') # 5yr us treasury note contracts\n",
    "ty_df = pd.read_csv('TY.txt') # 10yr US treasury bond contracts\n",
    "us_df = pd.read_csv('US.txt') # 30yr US treasury bond contracts\n",
    "gc_df = pd.read_csv('GC.txt') # Gold contracts\n",
    "cl_df = pd.read_csv('CL.txt') # Crude oil contracts\n",
    "jy_df = pd.read_csv('JY.txt') # JPY/USD contracts\n",
    "bp_df = pd.read_csv('BP.txt') # BP/USD contracts\n",
    "eu_df = pd.read_csv('EU.txt') # EURO/USD contracts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c16cc89",
   "metadata": {},
   "source": [
    "Clean these up a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97079487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_price_columns(data, item):\n",
    "    # Rename columns\n",
    "    data.rename(columns={'Open': f'{item}_open', 'High': f'{item}_high', 'Low': f'{item}_low', 'Close': f'{item}_close', 'Volume': f'{item}_volume'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d300c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_data = {\n",
    "    'ES': es_df,\n",
    "    'NQ': nq_df,\n",
    "    'FV': fv_df,\n",
    "    'TY': ty_df,\n",
    "    'US': us_df, \n",
    "    'GC': gc_df,\n",
    "    'CL': cl_df,\n",
    "    'JY': jy_df,\n",
    "    'BP': bp_df,\n",
    "    'EU': eu_df\n",
    "}\n",
    "\n",
    "for contract, data in contract_data.items():\n",
    "    # Make sure date is datetime and set as index\n",
    "    try:\n",
    "        data['Date'] = pd.to_datetime(data['Date'])\n",
    "        data.set_index(data['Date'], inplace=True)\n",
    "        data.drop(columns=['Date'], inplace=True)\n",
    "    except:\n",
    "        print('Index already fixed.')\n",
    "\n",
    "    # Rename columns\n",
    "    rename_price_columns(data, contract)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a39f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_data['ES']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c9267",
   "metadata": {},
   "source": [
    "References for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78115057",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = ['S&P', 'NASDAQ', 'DJIA']\n",
    "contracts = contract_data.keys() # These are the ones we are training on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaa46d2",
   "metadata": {},
   "source": [
    "Get yahoo finance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['^GSPC', '^IXIC', '^DJI'] # S&P, NASDAQ, DJIA\n",
    "\n",
    "# Download data\n",
    "data_dictionary = get_data(tickers)\n",
    "\n",
    "# Turn to df\n",
    "s_p = pd.DataFrame(data_dictionary['^GSPC'])\n",
    "nasdaq = pd.DataFrame(data_dictionary['^IXIC'])\n",
    "djia = pd.DataFrame(data_dictionary['^DJI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5f7db",
   "metadata": {},
   "source": [
    "Fix structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten df so ticker doesn't span the top\n",
    "s_p = s_p.droplevel(1, axis=1)\n",
    "nasdaq = nasdaq.droplevel(1, axis=1)\n",
    "djia = djia.droplevel(1, axis=1)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "#s_p.drop(columns=['High', 'Low', 'Open'], inplace=True)\n",
    "#nasdaq.drop(columns=['High', 'Low', 'Open'], inplace=True)\n",
    "#djia.drop(columns=['High', 'Low', 'Open'], inplace=True)\n",
    "\n",
    "index_data = {\n",
    "    'S&P': s_p,\n",
    "    'NASDAQ': nasdaq,\n",
    "    'DJIA': djia,\n",
    "}\n",
    "\n",
    "for index, data in index_data.items():\n",
    "    # Rename columns\n",
    "    rename_price_columns(data, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb7ee6e",
   "metadata": {},
   "source": [
    "# Downloading historical data for our factor model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7547dc9e",
   "metadata": {},
   "source": [
    "We are going to model the index as a geometric brownian motion, with the mu factor being a linear regression model with numerous inputs.  \n",
    "\n",
    "## Factor considerations:  \n",
    "### <u>Term structure</u>\n",
    "###### Term spread (10Y-3M)\n",
    "\n",
    "### <u>Credit conditions</u>\n",
    "###### IG spread (BAA-AAA)\n",
    "\n",
    "### <u>Valuation</u>\n",
    "###### Forward E/P - real 10Y\n",
    "###### Dividend yield\n",
    "\n",
    "### <u>Economic</u>\n",
    "###### Fed funds\n",
    "###### Inflation (CPI)\n",
    "###### DXY change (dollar index)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb92a74",
   "metadata": {},
   "source": [
    "### Some of these we can get from yahoo finance:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13407860",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\n",
    "    # Term structure\n",
    "    '^TNX', # 10yr CBOE\n",
    "    '^IRX', # 3m bill (on discount basis, need to convert to yield)\n",
    "\n",
    "    # Economic\n",
    "    'DX-Y.NYB', # Dollar index\n",
    "\n",
    "    # Volatility\n",
    "    '^VIX',\n",
    "\n",
    "    # Gold could be a measure of macro activity?\n",
    "    \"GLD\", # Gold ETF\n",
    "\n",
    "    # Other metals\n",
    "    \"XME\", # SPDR Metals and mining ETF\n",
    "\n",
    "    # Credit\n",
    "    'FLRN', # Bloomberg IG Credit ETF\n",
    "    'JNK', # Bloomberg HY Credit ETF\n",
    "\n",
    "]\n",
    "\n",
    "data_dictionary = get_data(tickers)\n",
    "\n",
    "for ticker, data in data_dictionary.items():\n",
    "    data_dictionary[ticker] = pd.DataFrame(data_dictionary[ticker])\n",
    "    data_dictionary[ticker] = data_dictionary[ticker].droplevel(1, axis=1)\n",
    "    rename_price_columns(data_dictionary[ticker], ticker)\n",
    "\n",
    "ten_yr = data_dictionary['^TNX']\n",
    "three_m = data_dictionary['^IRX']\n",
    "dollar_index = data_dictionary['DX-Y.NYB']\n",
    "vix = data_dictionary['^VIX']\n",
    "\n",
    "# Need to drop volume column here\n",
    "ten_yr.drop(columns=['^TNX_volume'], inplace=True)\n",
    "three_m.drop(columns=['^IRX_volume'], inplace=True)\n",
    "dollar_index.drop(columns=['DX-Y.NYB_volume'], inplace=True)\n",
    "vix.drop(columns=['^VIX_volume'], inplace=True)\n",
    "\n",
    "\n",
    "#gold = data_dictionary['GLD']\n",
    "#metals = data_dictionary['XME']\n",
    "#igb = pd.DataFrame(data_dictionary['FLRN'])\n",
    "#hyb = data_dictionary['JNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14965ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "yf_macros = [vix, ten_yr, three_m, dollar_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9734f02",
   "metadata": {},
   "source": [
    "We should get dividend yield too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "div_data = {}\n",
    "etfs = ['SPY', 'QQQ', 'DIA']\n",
    "\n",
    "for etf in etfs:\n",
    "    ticker = yf.Ticker(etf)\n",
    "    div = ticker.dividends\n",
    "    price = ticker.history(timeframe)['Close']\n",
    "\n",
    "    # Calculate dividend yield\n",
    "    div_12m = div.rolling(window='365D', min_periods=1).sum()\n",
    "    div_12m = div_12m.reindex(price.index, method='ffill')\n",
    "    div_yield = div_12m / price\n",
    "    div_data[etf] = div_yield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fffc352",
   "metadata": {},
   "source": [
    "Fix index for all 3 and rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac371df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "div_data['SPY'].index = pd.to_datetime(div_data['SPY'].index).normalize().tz_localize(None) # Normalize puts date in format we want\n",
    "div_data['QQQ'].index = pd.to_datetime(div_data['QQQ'].index).normalize().tz_localize(None) # Localize (none) makes sure it doesn't add our timezone\n",
    "div_data['DIA'].index = pd.to_datetime(div_data['DIA'].index).normalize().tz_localize(None)\n",
    "\n",
    "div_data['SPY'].name = 'S&P_div'\n",
    "div_data['QQQ'].name = 'NASDAQ_div'\n",
    "div_data['DIA'].name = 'DJIA_div'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52265c2b",
   "metadata": {},
   "source": [
    "### pandas_datareader lets us download fred data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f617557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(1990,1,1) # Start date for download\n",
    "\n",
    "# Macroeconomic data\n",
    "gdp = pdr.DataReader(\"GDP\", \"fred\", start)\n",
    "cpi = pdr.DataReader(\"CPIAUCSL\", \"fred\", start)\n",
    "fedfunds = pdr.DataReader(\"FEDFUNDS\", \"fred\", start)\n",
    "consumer_sentiment = pdr.DataReader(\"UMCSENT\", \"fred\", start)\n",
    "inflation_expectation = pdr.DataReader(\"MICH\", \"fred\", start)\n",
    "\n",
    "# For some reason this download doesn't have the most recent fed funds rate\n",
    "fedfunds = pd.concat([fedfunds['FEDFUNDS'], pd.Series([4.08], index=[datetime(2025,9,17)])])\n",
    "\n",
    "# Credit risk data\n",
    "ig_spread = pdr.DataReader(\"BAMLC0A4CBBB\", \"fred\", start)   # BofA BBB corp minus Treasuries\n",
    "#hy_spread = pdr.DataReader(\"BAMLH0A0HYM2\", \"fred\", start)   # BofA US High Yield spread\n",
    "#baa_spread = pdr.DataReader(\"BAA10Y\", \"fred\", start)        # Moody’s Baa – 10Y Treasury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b0f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_data = [gdp, cpi, fedfunds, ig_spread, consumer_sentiment, inflation_expectation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def72632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last business day <= today\n",
    "last_bday = pd.bdate_range(end=pd.Timestamp.today().normalize().tz_localize(None), periods=1)[0]\n",
    "\n",
    "for i, df in enumerate(fred_data):\n",
    "    s = df.squeeze() # make it a Series\n",
    "    # Build a business-day index from the series start to last_bday\n",
    "    bidx = pd.bdate_range(start=s.index.min(), end=last_bday)\n",
    "    # Reindex to business days and forward-fill\n",
    "    s = s.reindex(bidx, method='ffill')\n",
    "    # Write back as a 1-col DataFrame with a proper name\n",
    "    name = s.name if s.name else f\"series_{i}\"\n",
    "    fred_data[i] = s.to_frame(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ea15b",
   "metadata": {},
   "source": [
    "Let's build a master dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf2055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "data = s_p.join([nasdaq, djia])\n",
    "data = data.join(contract for contract in contract_data.values())\n",
    "data = data.join(macro for macro in yf_macros)\n",
    "data = data.join(div for div in div_data.values())\n",
    "data = data.join([fd for fd in fred_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae193efe",
   "metadata": {},
   "source": [
    "Rename some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'CPIAUCSL': 'CPI', 'BAMLC0A4CBBB': 'credit_spread', 'UMCSENT': 'consumer_sentiment', 'MICH': 'inflation_expectation', 'series_2': 'fed_funds'}, inplace=True)\n",
    "data[data.index.year > 2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00b0cb",
   "metadata": {},
   "source": [
    "# Linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd38fe",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1333d",
   "metadata": {},
   "source": [
    "Let's check for NaNs (there's a ton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3373e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf740444",
   "metadata": {},
   "source": [
    "Check data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75773430",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a8b04",
   "metadata": {},
   "source": [
    "Fill forward for any gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a84a729",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace(0, np.nan)\n",
    "data = data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f31121",
   "metadata": {},
   "source": [
    "Drop others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c572d98d",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1957e49",
   "metadata": {},
   "source": [
    "We need to be careful to not include things such as raw moving averages that will leak volatility information into our drift prediction  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd9c64",
   "metadata": {},
   "source": [
    "Forward adjusting futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb0c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "roll_rules = { # Rollover occurances: (months, days)\n",
    "    'ES': (0,5),\n",
    "    'NQ': (0,5),\n",
    "    'FV': (1,1),\n",
    "    'TY': (1,1),\n",
    "    'US': (1,1),\n",
    "    'GC': (1,2),\n",
    "    'CL': (0,3),\n",
    "    'JY': (0,2),\n",
    "    'BP': (0,2),\n",
    "    'EU': (0,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01fb87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMM_QTR = {3, 6, 9, 12}\n",
    "GOLD_MONTHS = {2, 4, 6, 8, 10, 12}\n",
    "\n",
    "def _third_fridays_quarterly(start, end):\n",
    "    d = pd.date_range(start, end, freq=\"WOM-3FRI\")\n",
    "    return d[d.month.isin(IMM_QTR)]\n",
    "\n",
    "def _bmonth_end_quarterly(start, end):\n",
    "    # Last business day of IMM months\n",
    "    all_month_ends = pd.date_range(start.normalize(), end.normalize(), freq=\"BM\")\n",
    "    return all_month_ends[all_month_ends.month.isin(IMM_QTR)]\n",
    "\n",
    "def _third_wednesdays_quarterly(start, end):\n",
    "    # 3rd Wednesday of IMM months\n",
    "    weds = pd.date_range(start, end, freq=\"WOM-3WED\")\n",
    "    return weds[weds.month.isin(IMM_QTR)]\n",
    "\n",
    "def _gold_expiries(start, end):\n",
    "    # 3rd-last business day of bi-monthly GC months (anchor only)\n",
    "    mends = pd.date_range(start.normalize(), end.normalize(), freq=\"BM\")\n",
    "    mends = mends[mends.month.isin(GOLD_MONTHS)]\n",
    "    # 3rd last BD in that month = BM - 2 BDays\n",
    "    return (mends - pd.offsets.BusinessDay(2))\n",
    "\n",
    "def _cl_expiries(start, end):\n",
    "    # Crude \"expiration\" day (last trade day): 3 business days before the 25th of the PRIOR month\n",
    "    # We'll generate it for EVERY contract month in range.\n",
    "    cl_expiries = []\n",
    "    # iterate month starts to month ends\n",
    "    month_starts = pd.date_range(start.normalize(), end.normalize(), freq=\"MS\")\n",
    "    for ms in month_starts:\n",
    "        # prior month 25th\n",
    "        prior_month_25th = (ms - pd.offsets.MonthBegin(1)) + pd.offsets.Day(24)  # 25th of prior month\n",
    "        # adjust 25th to previous business day if needed, then subtract 3 business days\n",
    "        # (approximation: we ignore exchange holidays and use business days = weekdays)\n",
    "        # Find the previous business day <= prior_month_25th\n",
    "        prev_bd = prior_month_25th\n",
    "        # step back to weekday if falls on weekend\n",
    "        while prev_bd.weekday() >= 5:\n",
    "            prev_bd -= pd.Timedelta(days=1)\n",
    "        ltd = prev_bd - pd.offsets.BusinessDay(3)\n",
    "        cl_expiries.append(ltd.normalize())\n",
    "    return pd.DatetimeIndex(sorted(set(cl_expiries)))\n",
    "\n",
    "def _expiry_calendar(symbol, start, end):\n",
    "    s = symbol.upper()\n",
    "    if s in {\"ES\", \"NQ\"}:\n",
    "        return _third_fridays_quarterly(start, end)\n",
    "    if s in {\"FV\", \"TY\", \"US\"}:\n",
    "        return _bmonth_end_quarterly(start, end)\n",
    "    if s == \"GC\":\n",
    "        return _gold_expiries(start, end)\n",
    "    if s == \"CL\":\n",
    "        return _cl_expiries(start, end)\n",
    "    if s in {\"JY\", \"BP\", \"EU\"}:\n",
    "        return _third_wednesdays_quarterly(start, end)\n",
    "    # Fallback: quarterly 3rd Fridays (reasonable default)\n",
    "    return _third_fridays_quarterly(start, end)\n",
    "\n",
    "# -----------------------\n",
    "# main: compute rollover flags, episodes, and DTR\n",
    "# -----------------------\n",
    "def get_rollover_data(data: pd.DataFrame, item: str, roll_rules: dict):\n",
    "    months_off, days_off = roll_rules[item]  # (months, days)\n",
    "\n",
    "    # We add a buffer so we can see the next expiry at the end of the dataset\n",
    "    start = pd.Timestamp(data.index.min()).normalize()\n",
    "    end = (pd.Timestamp(data.index.max()).normalize() + pd.DateOffset(months=4))\n",
    "\n",
    "    expiries = _expiry_calendar(item, start, end)\n",
    "\n",
    "    # Compute roll dates per rule\n",
    "    if months_off == 0:\n",
    "        roll_dates = expiries - pd.offsets.BusinessDay(days_off)\n",
    "    else:\n",
    "        # For each expiry, go back N months to that month's LAST business day, then step back 'days_off'\n",
    "        anchor_month_ends = (expiries - pd.offsets.MonthBegin(months_off))  # first of prior month(s)\n",
    "        # move to that month's last business day\n",
    "        anchor_bm_end = pd.to_datetime(anchor_month_ends).to_period('M').to_timestamp('M')\n",
    "        # ensure it's a business day (BM gives last business day; if not, use BMonthEnd)\n",
    "        anchor_bm_end = pd.DatetimeIndex(anchor_bm_end)  # calendar month-end\n",
    "        # Convert to last BUSINESS day of that month\n",
    "        anchor_last_bd = pd.DatetimeIndex([pd.Timestamp(d).to_period('M').to_timestamp('M') for d in anchor_bm_end])\n",
    "        # Replace with BusinessMonthEnd for precision\n",
    "        anchor_last_bd = pd.date_range(anchor_last_bd.min(), anchor_last_bd.max(), freq=\"BM\").intersection(anchor_last_bd)\n",
    "        # The above intersection collapses; safer approach:\n",
    "        # Recompute last BD robustly per expiry:\n",
    "        last_bds = []\n",
    "        for e in expiries:\n",
    "            # month = e - months_off\n",
    "            mstart = (e - pd.offsets.MonthBegin(months_off))\n",
    "            # last business day of that month:\n",
    "            lbd = (mstart + pd.offsets.BMonthEnd(0))\n",
    "            last_bds.append(lbd)\n",
    "        anchor_last_bd = pd.DatetimeIndex(last_bds)\n",
    "\n",
    "        roll_dates = anchor_last_bd - pd.offsets.BusinessDay(days_off)\n",
    "\n",
    "    roll_dates = pd.DatetimeIndex(sorted(set(pd.to_datetime(roll_dates))))\n",
    "    # Keep only roll dates that appear in our data index (typical)\n",
    "    roll_dates = roll_dates.intersection(pd.DatetimeIndex(data.index))\n",
    "\n",
    "    # Flags and episodes\n",
    "    rflag = f\"{item}_Rollover\"\n",
    "    eph = f\"{item}_Episode\"\n",
    "    data[rflag] = 0\n",
    "    if len(roll_dates):\n",
    "        data.loc[roll_dates, rflag] = 1\n",
    "    data[eph] = data[rflag].cumsum()\n",
    "\n",
    "    # Days until next rollover (trading days)\n",
    "    # Forward-fill the next roll date and count business days\n",
    "    next_roll = (\n",
    "        pd.Series(roll_dates, index=roll_dates)\n",
    "        .reindex(data.index, method=\"bfill\")\n",
    "        .to_numpy()\n",
    "    )\n",
    "    days = np.full(len(data), np.nan)\n",
    "    mask = ~pd.isna(next_roll)\n",
    "    # convert to daily (no time) for busday_count\n",
    "    left = data.index.to_numpy(dtype=\"datetime64[D]\")\n",
    "    right = pd.to_datetime(next_roll).to_numpy(dtype=\"datetime64[D]\")\n",
    "    days[mask] = np.busday_count(left[mask], right[mask])  # excludes the right endpoint\n",
    "    data[f\"{item}_Days_Until_Rollover\"] = days\n",
    "\n",
    "# -----------------------\n",
    "# forward adjustment on roll days\n",
    "# -----------------------\n",
    "def rollover_forward_adjustment(data: pd.DataFrame, contract: str):\n",
    "    rflag = f\"{contract}_Rollover\"\n",
    "    close = f\"{contract}_close\"\n",
    "    # Jump only on the roll day = today's close minus yesterday's close\n",
    "    jump = data[rflag].fillna(0).astype(int) * (data[close] - data[close].shift(1).fillna(0.0))\n",
    "    cum_adj = jump.cumsum()\n",
    "    for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "        c = f\"{contract}_{col}\"\n",
    "        if c in data.columns:\n",
    "            data[c] = data[c] - cum_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f756f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for contract in contracts:\n",
    "    # Set info about expirations\n",
    "    get_rollover_data(data, contract, roll_rules)\n",
    "\n",
    "    # Forward adjust rollover\n",
    "    rollover_forward_adjustment(data, contract)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aef311",
   "metadata": {},
   "source": [
    "Function definitions to help out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_mean(data, window):\n",
    "    return data.rolling(window, min_periods=window).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e655271",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c0096",
   "metadata": {},
   "source": [
    "Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(data, items, cfg=None, dropna=True):\n",
    "    if cfg is None:\n",
    "        cfg = {\n",
    "            \"rsi_n\": 14,\n",
    "            \"macd_fast\": 12, \"macd_slow\": 26, \"macd_signal\": 9,\n",
    "            \"bb_n\": 20, \"bb_k\": 2.0,\n",
    "            \"atr_n\": 14\n",
    "        }\n",
    "    out = data.copy()\n",
    "\n",
    "    for item in items:\n",
    "        o = out[f\"{item}_open\"].astype(float)\n",
    "        h = out[f\"{item}_high\"].astype(float)\n",
    "        l = out[f\"{item}_low\"].astype(float)\n",
    "        c = out[f\"{item}_close\"].astype(float)\n",
    "        v = out[f\"{item}_volume\"].astype(float)\n",
    "\n",
    "        # --- 1) RSI (Wilder smoothing) ---\n",
    "        n = cfg[\"rsi_n\"]\n",
    "        delta = c.diff()\n",
    "        gain = delta.clip(lower=0.0)\n",
    "        loss = (-delta).clip(lower=0.0)\n",
    "        avg_gain = gain.ewm(alpha=1/n, adjust=False, min_periods=n).mean()\n",
    "        avg_loss = loss.ewm(alpha=1/n, adjust=False, min_periods=n).mean()\n",
    "        rs = avg_gain / (avg_loss.replace(0, np.nan))\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        out[f\"{item}_rsi_{n}\"] = rsi\n",
    "\n",
    "        # --- 2) MACD (EMA fast/slow, signal, hist) ---\n",
    "        ema_fast = c.ewm(span=cfg[\"macd_fast\"], adjust=False).mean()\n",
    "        ema_slow = c.ewm(span=cfg[\"macd_slow\"], adjust=False).mean()\n",
    "        macd = ema_fast - ema_slow\n",
    "        macd_signal = macd.ewm(span=cfg[\"macd_signal\"], adjust=False).mean()\n",
    "        macd_hist = macd - macd_signal\n",
    "        out[f\"{item}_macd\"] = macd\n",
    "        out[f\"{item}_macd_signal\"] = macd_signal\n",
    "        out[f\"{item}_macd_hist\"] = macd_hist\n",
    "\n",
    "        # --- 3) Bollinger Bands (+ %B, Bandwidth) ---\n",
    "        bb_n = cfg[\"bb_n\"]; bb_k = cfg[\"bb_k\"]\n",
    "        mavg = c.rolling(bb_n, min_periods=bb_n).mean()\n",
    "        mstd = c.rolling(bb_n, min_periods=bb_n).std(ddof=0)\n",
    "        upper = mavg + bb_k * mstd\n",
    "        lower = mavg - bb_k * mstd\n",
    "        pct_b = (c - lower) / (upper - lower)\n",
    "        bandwidth = (upper - lower) / mavg\n",
    "        out[f\"{item}_bb_mean_{bb_n}\"] = mavg\n",
    "        out[f\"{item}_bb_upper_{bb_n}_{int(bb_k)}\"] = upper\n",
    "        out[f\"{item}_bb_lower_{bb_n}_{int(bb_k)}\"] = lower\n",
    "        out[f\"{item}_bb_percent_b\"] = pct_b\n",
    "        out[f\"{item}_bb_bandwidth\"] = bandwidth\n",
    "\n",
    "        # --- 4) ATR (Wilder, uses True Range) ---\n",
    "        prev_c = c.shift(1)\n",
    "        tr = np.maximum(h - l, np.maximum((h - prev_c).abs(), (l - prev_c).abs()))\n",
    "        atr = tr.ewm(alpha=1/cfg[\"atr_n\"], adjust=False, min_periods=cfg[\"atr_n\"]).mean()\n",
    "        out[f\"{item}_atr_{cfg['atr_n']}\"] = atr\n",
    "\n",
    "        # --- 5) OBV (On-Balance Volume) ---\n",
    "        sign = np.sign(c.diff().fillna(0.0))\n",
    "        obv = (sign * v).fillna(0.0).cumsum()\n",
    "        out[f\"{item}_obv\"] = obv\n",
    "\n",
    "    if dropna:\n",
    "        out = out.dropna()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f44c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_technical_indicators(data, contracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac8b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_price(data, items):\n",
    "    data = data.copy()\n",
    "    \n",
    "    ohlcv = ['open', 'high', 'low', 'close', 'volume']\n",
    "    for item in items:\n",
    "        for key in ohlcv:\n",
    "            data[f'{item}_log_{key}'] = np.log(data[f'{item}_{key}'])\n",
    "            data.drop(columns=[f'{item}_{key}'], inplace=True) # Drop originals\n",
    "\n",
    "        data.dropna(inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_returns(data, items):\n",
    "    data = data.copy()\n",
    "    \n",
    "    ohlcv = ['open', 'high', 'low', 'close', 'volume']\n",
    "    for item in items:\n",
    "        for key in ohlcv:\n",
    "            data[f'{item}_log_{key}_ret'] = data[f'{item}_log_{key}'].diff()\n",
    "\n",
    "        data[f'{item}_log_open_close_ret'] = data[f'{item}_log_close'] - data[f'{item}_log_open']\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get log transforms of price data\n",
    "log_price_keys = indexes + list(contract_data.keys()) #+ ['GLD', 'XME', 'JNK']\n",
    "\n",
    "# Log prices\n",
    "data = get_log_price(data, log_price_keys)\n",
    "\n",
    "# Log returns\n",
    "data = get_log_returns(data, log_price_keys)\n",
    "\n",
    "# Log for other price data (GDP)\n",
    "data['log_GDP'] = np.log(data['GDP'])\n",
    "data.drop(columns=['GDP'], inplace=True)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e8b758",
   "metadata": {},
   "source": [
    "Price/Volume Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524eaa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price_volume_features(data, items):\n",
    "    data = data.copy()\n",
    "    \n",
    "    for item in items:\n",
    "        # Price-based\n",
    "        data[f'{item}_mom_1w'] = data[f'{item}_log_close'].diff(5) # Total price change / momentum indicator\n",
    "        data[f'{item}_mom_3m'] = data[f'{item}_log_close'].diff(63)\n",
    "        data[f'{item}_1m_rolling_price'] = rolling_mean(data[f'{item}_log_close'], 21)\n",
    "        data[f'{item}_3m_rolling_price'] = rolling_mean(data[f'{item}_log_close'], 63)\n",
    "        data[f'{item}_6m_rolling_price'] = rolling_mean(data[f'{item}_log_close'], 126)\n",
    "        data[f'{item}_trend_speed_price'] = data[f'{item}_3m_rolling_price'].diff(5)  # How fast the 3m trend is changing on a weekly basis\n",
    "        data[f'{item}_trend_dist_price'] = data[f'{item}_log_close'] - data[f'{item}_3m_rolling_price']\n",
    "\n",
    "        # Volume-based (essentially the same as price for now)\n",
    "        data[f'{item}_vlm_1w'] = data[f'{item}_log_volume'].diff(5) # Total volume change / momentum indicator\n",
    "        data[f'{item}_vlm_1m'] = data[f'{item}_log_volume'].diff(21)\n",
    "        data[f'{item}_vlm_3m'] = data[f'{item}_log_volume'].diff(63)\n",
    "        data[f'{item}_3m_rolling_volume'] = rolling_mean(data[f'{item}_log_volume'], 63)\n",
    "        data[f'{item}_trend_speed_volume'] = data[f'{item}_3m_rolling_volume'].diff(5)  # How fast the 3m trend is changing on a weekly basis\n",
    "        data[f'{item}_trend_dist_volume'] = data[f'{item}_log_volume'] - data[f'{item}_3m_rolling_volume']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a0279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_price_volume_features(data, log_price_keys)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978bc76f",
   "metadata": {},
   "source": [
    "Macro features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a9458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['log_GDP_ret'] = data['log_GDP'].diff()\n",
    "data['log_GDP_ret_1y'] = data['log_GDP'].diff(252)\n",
    "data['inflation_expectation_ret'] = data['inflation_expectation'].pct_change()\n",
    "data['consumer_sentiment_ret'] = data['consumer_sentiment'].pct_change()\n",
    "data['cpi_ret'] = data['CPI'].pct_change()\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4464ef",
   "metadata": {},
   "source": [
    "Spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08509e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Spread_NQ_ES_ret'] = data['ES_log_close_ret'] - data['NQ_log_close_ret']\n",
    "data['Spread_NQ_ES_volume'] = data['ES_3m_rolling_volume'] - data['NQ_3m_rolling_volume']\n",
    "data['Spread_ES_GC_ret'] = data['ES_log_close_ret'] - data['GC_log_close_ret']\n",
    "data['Spread_ES_GC_volume'] = data['ES_3m_rolling_volume'] - data['GC_3m_rolling_volume']\n",
    "data['Spread_FV_US_ret'] = data['FV_log_close_ret'] - data['US_log_close_ret']\n",
    "data['Spread_FV_US_volume'] = data['FV_3m_rolling_volume'] - data['US_3m_rolling_volume']\n",
    "data['Spread_TY_US_ret'] = data['TY_log_close_ret'] - data['US_log_close_ret']\n",
    "data['Spread_TY_US_volume'] = data['TY_3m_rolling_volume'] - data['US_3m_rolling_volume']\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71374fec",
   "metadata": {},
   "source": [
    "Calendar dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    month_dummies = pd.get_dummies(data.index.month, prefix=\"month\", dtype=float)\n",
    "    month_dummies.set_index(data.index, inplace=True)\n",
    "    day_dummies = pd.get_dummies(data.index.dayofweek, prefix=\"weekday\", dtype=float)\n",
    "    day_dummies.set_index(data.index, inplace=True)\n",
    "    presidency_year_dummies = pd.get_dummies(data.index.year % 4, prefix='pres_year', dtype='float')\n",
    "    presidency_year_dummies.set_index(data.index, inplace=True)\n",
    "    \n",
    "    data = data.join([month_dummies, day_dummies, presidency_year_dummies])\n",
    "except:\n",
    "    print('Error: probably already created dummies.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6ccd9c",
   "metadata": {},
   "source": [
    "Volatility / Price Features (From ChatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32fff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_vix_vol = data['^VIX_close'] / (100 * np.sqrt(252))\n",
    "data['vix_implied_var'] = np.square(daily_vix_vol)\n",
    "    # Measures the percentage change in the ^VIX over 1 week and 1 month\n",
    "data['vix_mom_1w'] = data['^VIX_close'].pct_change(periods=5)\n",
    "data['vix_mom_1m'] = data['^VIX_close'].pct_change(periods=21)\n",
    "\n",
    "data[f'vix_trend_1m'] = rolling_mean(data[f'^VIX_close'], 21)\n",
    "data[f'vix_trend_3m'] = rolling_mean(data[f'^VIX_close'], 63)\n",
    "data[f'vix_trend_6m'] = rolling_mean(data[f'^VIX_close'], 126)\n",
    "\n",
    "# Calculate how far the current VIX is from its trend (as a percentage)\n",
    "data['vix_trend_dist'] = (data['^VIX_close'] - data['vix_trend_3m']) / data['vix_trend_3m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad2524",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-12\n",
    "\n",
    "# If you already have a rolling_mean helper, keep it. Otherwise:\n",
    "def rolling_mean(s, w):\n",
    "    return s.rolling(w, min_periods=max(2, int(w*0.6))).mean()\n",
    "\n",
    "def ewma_vol(r, lam=0.94):\n",
    "    # EWMA variance per RiskMetrics: sigma_t^2 = (1-lam)*r_{t-1}^2 + lam*sigma_{t-1}^2\n",
    "    # Use pandas ewm for convenience\n",
    "    return r.pow(2).ewm(alpha=(1-lam), adjust=False).mean().clip(lower=0)\n",
    "\n",
    "def rolling_autocorr(x, lag=1, window=63):\n",
    "    # Rolling autocorrelation of x at a given lag\n",
    "    # For stability, require at least ~60% of window\n",
    "    minp = max(10, int(window*0.6))\n",
    "    x0 = x\n",
    "    x1 = x.shift(lag)\n",
    "    return x0.rolling(window, min_periods=minp).corr(x1)\n",
    "\n",
    "def realized_quarticity(r, window=63):\n",
    "    # 3-month robust quarticity proxy (if daily): sum r^4 * (n / 3) approximation\n",
    "    # Here we simply provide rolling sum of r^4; scaling optional depending on use\n",
    "    minp = max(10, int(window*0.6))\n",
    "    return (r.pow(4)).rolling(window, min_periods=minp).sum()\n",
    "\n",
    "def build_vol_features(data, prefix, day_w=21, qtr_w=63, yr_w=252, ewma_lambda=0.94, target='close'):\n",
    "    \"\"\"\n",
    "    Expects:\n",
    "      data[f'{prefix}_log_price'] (daily log price)\n",
    "      data[f'{prefix}_log_volume'] (daily log volume)\n",
    "    Produces a suite of volatility-centric features for that prefix.\n",
    "    \"\"\"\n",
    "    \n",
    "    lp = data[f\"{prefix}_log_{target}\"]\n",
    "    lv = data.get(f\"{prefix}_log_volume\", None)\n",
    "\n",
    "    # Daily log return\n",
    "    r = lp.diff()  # already log-price, so diff = log-return\n",
    "\n",
    "    # --- Realized volatility proxies ---\n",
    "    data[f\"{prefix}_rv_1m\"]  = r.rolling(day_w, min_periods=int(day_w*0.6)).var().clip(lower=0)          # variance\n",
    "    data[f\"{prefix}_rv_3m\"]  = r.rolling(qtr_w, min_periods=int(qtr_w*0.6)).var().clip(lower=0)\n",
    "    data[f\"{prefix}_rv_1y\"]  = r.rolling(yr_w,  min_periods=int(yr_w*0.6)).var().clip(lower=0)\n",
    "    data[f\"{prefix}_absrv_1m\"] = r.abs().rolling(day_w, min_periods=int(day_w*0.6)).mean()               # mean |r|\n",
    "    data[f\"{prefix}_absrv_3m\"] = r.abs().rolling(qtr_w, min_periods=int(qtr_w*0.6)).mean()\n",
    "\n",
    "    # EWMA volatility (RiskMetrics-style)\n",
    "    data[f\"{prefix}_ewma_var\"] = ewma_vol(r, lam=ewma_lambda)\n",
    "    data[f\"{prefix}_ewma_vol\"] = np.sqrt(data[f\"{prefix}_ewma_var\"])\n",
    "\n",
    "    # Volatility-of-volatility (how fast vol is changing)\n",
    "    data[f\"{prefix}_vol_speed_1w\"] = data[f\"{prefix}_rv_3m\"].diff(5)                                      # weekly change in 3m var\n",
    "    data[f\"{prefix}_vol_mom_1m\"]   = data[f\"{prefix}_rv_3m\"] - data[f\"{prefix}_rv_1m\"]                    # 3m vs 1m\n",
    "    data[f\"{prefix}_vol_mom_1y\"]   = data[f\"{prefix}_rv_1y\"] - data[f\"{prefix}_rv_3m\"]\n",
    "\n",
    "    # Volatility clustering proxies\n",
    "    data[f\"{prefix}_acf_sqret_lag1_3m\"] = rolling_autocorr(r.pow(2), lag=1, window=qtr_w)\n",
    "    data[f\"{prefix}_acf_absret_lag1_3m\"] = rolling_autocorr(r.abs(), lag=1, window=qtr_w)\n",
    "\n",
    "    # Leverage effect proxy (contemporaneous corr between return and next day's vol)\n",
    "    # Negative returns often precede higher vol; we proxy with corr(r_t, |r|_{t+1})\n",
    "    data[f\"{prefix}_lev_proxy_3m\"] = r.shift(1).rolling(qtr_w, min_periods=int(qtr_w*0.6)).corr(r.abs()).shift(1)\n",
    "\n",
    "    # Quarticity (heavy tails proxy)\n",
    "    data[f\"{prefix}_quarticity_3m\"] = realized_quarticity(r, window=qtr_w)\n",
    "\n",
    "    # Ratio features (normalized vol levels)\n",
    "    data[f\"{prefix}_vol_ratio_1m_3m\"] = (data[f\"{prefix}_rv_1m\"] / (data[f\"{prefix}_rv_3m\"] + EPS))\n",
    "    data[f\"{prefix}_vol_ratio_3m_1y\"] = (data[f\"{prefix}_rv_3m\"] / (data[f\"{prefix}_rv_1y\"] + EPS))\n",
    "    data[f\"{prefix}_ewma_over_3m\"]    = (data[f\"{prefix}_ewma_var\"] / (data[f\"{prefix}_rv_3m\"] + EPS))\n",
    "\n",
    "    # Price–volatility relation: distance from trend as a stress proxy\n",
    "    data[f\"{prefix}_price_trend_3m\"]  = rolling_mean(lp, qtr_w)\n",
    "    data[f\"{prefix}_price_trend_dist\"] = lp - data[f\"{prefix}_price_trend_3m\"]\n",
    "    # Volatility when far below trend often spikes; include interaction\n",
    "    data[f\"{prefix}_vol_x_trend_dist\"] = data[f\"{prefix}_rv_1m\"] * data[f\"{prefix}_price_trend_dist\"]\n",
    "\n",
    "    # Volume–volatility links (if volume available)\n",
    "    if lv is not None:\n",
    "        dv = lv.diff()  # log-volume change\n",
    "        data[f\"{prefix}_vlm_var_1m\"] = dv.rolling(day_w, min_periods=int(day_w*0.6)).var().clip(lower=0)\n",
    "        data[f\"{prefix}_vlm_var_3m\"] = dv.rolling(qtr_w, min_periods=int(qtr_w*0.6)).var().clip(lower=0)\n",
    "        # Corr between |r| and volume changes (vol–volume clustering)\n",
    "        data[f\"{prefix}_corr_absr_dlv_3m\"] = r.abs().rolling(qtr_w, min_periods=int(qtr_w*0.6)).corr(dv)\n",
    "        # Volume surprise proxy: current vs 3m trend\n",
    "        data[f\"{prefix}_vlm_trend_3m\"] = rolling_mean(lv, qtr_w)\n",
    "        data[f\"{prefix}_vlm_trend_dist\"] = lv - data[f\"{prefix}_vlm_trend_3m\"]\n",
    "        # Vol reacts to volume surprises\n",
    "        data[f\"{prefix}_vol_x_vlm_surprise\"] = data[f\"{prefix}_rv_1m\"] * data[f\"{prefix}_vlm_trend_dist\"]\n",
    "\n",
    "    # 2. Calculate the spread against a realized variance feature you already have\n",
    "    # We'll use a 1-month realized variance (rv_1m) for the S&P 500 as an example.\n",
    "    # Ensure you have 'S&P_rv_1m' or a similar column from your previous code.\n",
    "    data['{prefix}_vrp_spread'] = data['vix_implied_var'] - data[f'{prefix}_rv_1m']\n",
    "\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# ---- Apply to all price series ---- #### both close and close_ret\n",
    "for item in log_price_keys:\n",
    "    data = build_vol_features(data, index, day_w=21, qtr_w=63, yr_w=252, ewma_lambda=0.94)\n",
    "    data = build_vol_features(data, item, day_w=21, qtr_w=63, yr_w=252, ewma_lambda=0.94, target='close_ret')\n",
    "\n",
    "# ---- Cross-index spillover features (optional but useful) ----\n",
    "# Differences/spreads in contemporaneous vol across indices capture contagion/regime moves\n",
    "data[\"SPX_minus_NDX_vol_1m\"] = data[\"S&P_rv_1m\"] - data[\"NASDAQ_rv_1m\"]\n",
    "data[\"SPX_minus_DJIA_vol_1m\"] = data[\"S&P_rv_1m\"] - data[\"DJIA_rv_1m\"]\n",
    "data[\"NDX_minus_DJIA_vol_1m\"] = data[\"NASDAQ_rv_1m\"] - data[\"DJIA_rv_1m\"]\n",
    "data['ES_minus_NQ_vol_1m'] = data[\"ES_rv_1m\"] - data[\"NQ_rv_1m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f9a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e57460",
   "metadata": {},
   "source": [
    "### Split data\n",
    "\n",
    "Training/testing 80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b1b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4e2899",
   "metadata": {},
   "source": [
    "### Set target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cced463",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_target = 'next_ret'\n",
    "drift = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94577183",
   "metadata": {},
   "outputs": [],
   "source": [
    "for contract in contracts:\n",
    "    drift[f'{contract}_{linear_model_target}'] = drift[f'{contract}_log_close_ret'].shift(-1)\n",
    "drift.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fdde3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split=0.8, embargo=2):\n",
    "    cutoff = math.floor(len(data)*split)\n",
    "    training_data = data.iloc[:(cutoff - embargo)]\n",
    "    testing_data = data.iloc[(cutoff+embargo):]\n",
    "    return training_data.copy(), testing_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbaea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_training_data, drift_testing_data = split_data(drift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c33903",
   "metadata": {},
   "source": [
    "### Determine feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459bd4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa8ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_targets_features(target, training_data, contracts=contracts, additional_exclusions=[], additional_unscalables=[]):\n",
    "    # Make sure we only fit on training_data and explanatory variables\n",
    "    targets = [f'{contract}_{target}' for contract in contracts]\n",
    "    month_dummies = [f'month_{month}' for month in range(1,13)]\n",
    "    day_dummies = [f'weekday_{day}' for day in range(5)]\n",
    "    presidency_year_dummies = day_dummies = [f'pres_year_{year}' for year in range(4)]\n",
    "    rollover_dummy = [f'{contract}_Rollover' for contract in contracts]\n",
    "    dummies = month_dummies + day_dummies + presidency_year_dummies + rollover_dummy\n",
    "    additional_exclusions.extend([f'{contract}_Episode'])\n",
    "\n",
    "\n",
    "    columns_to_exclude = [] + additional_exclusions\n",
    "\n",
    "    unscalable_features = dummies + additional_unscalables\n",
    "    scalable_features = [column for column in training_data.columns if column not in targets and column not in unscalable_features and column not in columns_to_exclude]\n",
    "    all_features = unscalable_features + scalable_features\n",
    "\n",
    "    return targets, all_features, unscalable_features, scalable_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_targets, drift_all_features, drift_unscale_features, drift_scale_features = set_targets_features(linear_model_target, drift_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cae89f",
   "metadata": {},
   "source": [
    "Let's try using exp weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4350b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_feature_importance(training_data, testing_data, features, target, contracts=contracts, mode='permutation', permutation_threshold = 0, selector_threshold='median'):\n",
    "    pruned_features = {}\n",
    "\n",
    "    for contract in contracts:\n",
    "        y_train = training_data[f'{contract}_{target}']\n",
    "        y_test = testing_data[f'{contract}_{target}']\n",
    "        X_train = training_data[features]\n",
    "        X_test = testing_data[features]\n",
    "\n",
    "        # Fit random forest\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators = 500,\n",
    "            max_depth=None,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        if mode == 'permutation':\n",
    "            result = permutation_importance(\n",
    "                rf, X_test, y_test, # Make sure on test data\n",
    "                n_repeats = 30,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "\n",
    "            importances = pd.Series(result.importances_mean, index=X_train.columns).sort_values(ascending=False)\n",
    "            selected_features = importances[importances > permutation_threshold].index.tolist()\n",
    "\n",
    "            pruned_features[contract] = {\n",
    "                'train_reduced': X_train[selected_features],\n",
    "                'test_reduced': X_test[selected_features],\n",
    "                'selected_features': selected_features\n",
    "            }\n",
    "\n",
    "\n",
    "        if mode == 'random_forest':\n",
    "            selector = SelectFromModel(rf, threshold=selector_threshold, prefit=True)\n",
    "            selected_features = training_data[features].columns[selector.get_support()]\n",
    "\n",
    "            pruned_features[contract] = {\n",
    "                'train_reduced': selector.transform(X_train),\n",
    "                'test_reduced': selector.transform(X_test),\n",
    "                'selected_features': selected_features\n",
    "            }\n",
    "\n",
    "    return pruned_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab256b",
   "metadata": {},
   "source": [
    "Pickling for saving compute expensive dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8807a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, gzip, bz2, lzma\n",
    "from pathlib import Path\n",
    "from typing import Callable, Any, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeeed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _opener(path: Path, mode: str):\n",
    "    ext = path.suffix.lower()\n",
    "    if ext in ('.pkl', '.pickle'):\n",
    "        return path.open(mode)\n",
    "    if ext == '.gz':\n",
    "        return gzip.open(path, mode)\n",
    "    if ext == '.bz2':\n",
    "        return bz2.open(path, mode)\n",
    "    if ext in ('.xz', '.lzma'):\n",
    "        return lzma.open(path, mode)\n",
    "    raise ValueError(f\"Unsupported extension: {ext}\")\n",
    "\n",
    "def get_or_build_results(\n",
    "    path: str,\n",
    "    build_fn: Callable[..., dict],\n",
    "    *args, **kwargs\n",
    ") -> Tuple[dict, str]:\n",
    "\n",
    "    p = Path(path)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Try load\n",
    "    try:\n",
    "        with _opener(p, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "\n",
    "        try:\n",
    "            # 2) Build\n",
    "            results = build_fn(*args, **kwargs)\n",
    "\n",
    "            # 3) Save\n",
    "            with _opener(p, 'wb') as f:\n",
    "                pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print(f'Exception occured: {e}')\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest for now because it's faster\n",
    "pruned_features = get_or_build_results('model_data/drift_features_rf.pkl.gz', determine_feature_importance, drift_training_data, drift_testing_data, drift_all_features, 'next_ret', mode='random_forest')\n",
    "\n",
    "for contract in contracts:\n",
    "    print(pruned_features[contract]['selected_features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f599689",
   "metadata": {},
   "source": [
    "### Normalize inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4039bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(training_data, testing_data, features):\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(training_data[features]) # Fitting on training data\n",
    "\n",
    "    train_scaled = training_data.copy()\n",
    "    test_scaled = testing_data.copy()\n",
    "\n",
    "    # Scale\n",
    "    train_scaled[features] = scaler.transform(training_data[features])\n",
    "    test_scaled[features] = scaler.transform(testing_data[features])\n",
    "\n",
    "    # Save info on standardization for later\n",
    "    #scaler_mu = pd.Series(scaler.mean_, index=features)\n",
    "    #scaler_std = pd.Series(scaler.scale_, index=features)\n",
    "    return train_scaled, test_scaled #, scaler_mu, scaler_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781255bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_train_scaled, drift_test_scaled = scale_features(drift_training_data, drift_testing_data, drift_scale_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678fe365",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df85bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from mlxtend.evaluate import GroupTimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ac9fb",
   "metadata": {},
   "source": [
    "We are going to test with multiple different linear models to account for collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which models we want to use\n",
    "linear_models = ['ridge', 'lasso', 'enet', 'pca'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8090b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get print results from the models\n",
    "def eval_and_report(y_true, y_pred, model_name):\n",
    "    print(f\"{model_name:18s} | R^2: {r2_score(y_true, y_pred):.4f} | RMSE: {root_mean_squared_error(y_true, y_pred):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf60305",
   "metadata": {},
   "source": [
    "Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train_scaled, test_scaled, target, contracts=contracts, pruned_features=pruned_features, models=linear_models):\n",
    "    # Suppress all convergence warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for contract in contracts:\n",
    "        results[contract] = {}\n",
    "        features = pruned_features[contract]['selected_features']\n",
    "\n",
    "        print(f\"\\n=== Target: {f'{contract}_{target}'} ===\")\n",
    "        X_train = train_scaled[features].copy()\n",
    "        y_train = train_scaled[f'{contract}_{target}'].copy()\n",
    "        X_test = test_scaled[features].copy()\n",
    "        y_test = test_scaled[f'{contract}_{target}'].copy()\n",
    "\n",
    "        # Create time series splits that isolate each contract\n",
    "        ep_train = train_scaled[f'{contract}_Episode']\n",
    "        gts = GroupTimeSeriesSplit(n_splits = ep_train.nunique()-2, test_size=1, window_type='expanding', gap_size=1)\n",
    "        \n",
    "        tscv = list(gts.split(\n",
    "            X = X_train,\n",
    "            y = y_train,\n",
    "            groups=ep_train.values\n",
    "        ))\n",
    "\n",
    "        # 1. Ordinary Least Squares (OLS)\n",
    "        if 'ols' in models:\n",
    "            ols = LinearRegression()\n",
    "            ols.fit(X_train, y_train)\n",
    "            yhat_ols = ols.predict(X_test)\n",
    "            eval_and_report(y_test, yhat_ols, \"OLS\")\n",
    "\n",
    "            # Print top coefficients\n",
    "            ols_coef = pd.Series(ols.coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "            print(\"Top OLS coeffs:\\n\", ols_coef.head(10))\n",
    "\n",
    "            results[contract].update({\n",
    "                \"ols_model\": ols,\n",
    "                \"ols_coefs\": ols_coef,\n",
    "                \"train_data_ols\": pd.Series(ols.predict(X_train), index=y_train.index, name=f\"ols_train\"),\n",
    "                \"yhat_ols\": pd.Series(yhat_ols, index=y_test.index, name=f\"{contract}_{target}_ols_pred\"),\n",
    "            })\n",
    "\n",
    "        # 2. Ridge with CV over alphas (time-series CV)\n",
    "        if 'ridge' in models:\n",
    "            alphas = np.logspace(-4, 6, 100)\n",
    "\n",
    "            ridge = RidgeCV(alphas=alphas, cv=tscv, fit_intercept=True)\n",
    "            ridge.fit(X_train, y_train)\n",
    "            yhat_ridge = ridge.predict(X_test)\n",
    "            eval_and_report(y_test, yhat_ridge, f\"Ridge (alpha={ridge.alpha_:.4g})\")\n",
    "\n",
    "            # Print top coefficients\n",
    "            ridge_coef = pd.Series(ridge.coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "            print(\"Top Ridge coeffs:\\n\", ridge_coef.head(10))\n",
    "            \n",
    "            results[contract].update({\n",
    "                \"ridge_model\": ridge,\n",
    "                \"ridge_coefs\": ridge_coef,\n",
    "                \"train_data_ridge\": pd.Series(ridge.predict(X_train), index=y_train.index, name=f\"ridge_train\"),\n",
    "                \"yhat_ridge\": pd.Series(yhat_ridge, index=y_test.index, name=f\"{contract}_{target}_ridge_pred\"),\n",
    "            })\n",
    "\n",
    "        # 3. Lasso with CV over alphas (time-series CV)\n",
    "        if 'lasso' in models and not 'lasso_ridge' in models:\n",
    "            alphas = np.logspace(-4, 6, 100)\n",
    "\n",
    "            lasso = LassoCV(alphas=alphas, cv=tscv, fit_intercept=True, n_jobs=-1)\n",
    "            lasso.fit(X_train, y_train)\n",
    "            yhat_lasso = lasso.predict(X_test)\n",
    "            eval_and_report(y_test, yhat_lasso, f\"Lasso (alpha={lasso.alpha_:.4g})\")\n",
    "\n",
    "            # Print top coefficients\n",
    "            lasso_coef = pd.Series(lasso.coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "            print(\"Top Lasso coeffs:\\n\", lasso_coef.head(10))\n",
    "\n",
    "            results[contract].update({\n",
    "                \"lasso_model\": lasso,\n",
    "                \"lasso_coefs\": lasso_coef,\n",
    "                \"train_data_lasso\": pd.Series(lasso.predict(X_train), index=y_train.index, name=f\"lasso_train\"),\n",
    "                \"yhat_lasso\": pd.Series(yhat_lasso, index=y_test.index, name=f\"{contract}_{target}_lasso_pred\"),\n",
    "            })\n",
    "\n",
    "        # 4. ElasticNet with CV over alphas and l1_ratios (time-series CV)\n",
    "        if 'enet' in models:\n",
    "            alphas = np.logspace(-4, 6, 100)\n",
    "            l1_ratios = np.arange(.1, 1, .1)   # 1.0 == Lasso, 0.0 == Ridge\n",
    "\n",
    "            enet = ElasticNetCV(\n",
    "                alphas=alphas,\n",
    "                l1_ratio=l1_ratios,\n",
    "                cv=tscv,\n",
    "                fit_intercept=True,\n",
    "                max_iter=20000,\n",
    "                n_jobs = -1\n",
    "            )\n",
    "            enet.fit(X_train, y_train)\n",
    "\n",
    "            yhat_enet = enet.predict(X_test)\n",
    "            eval_and_report(y_test, yhat_enet, f\"ElasticNet (alpha={enet.alpha_:.4g}, l1_ratio={enet.l1_ratio_})\")\n",
    "\n",
    "            # Print top coefficients\n",
    "            enet_coef = pd.Series(enet.coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "            print(\"Top ElasticNet coeffs:\\n\", enet_coef.head(10))\n",
    "\n",
    "            results[contract].update({\n",
    "                \"enet_model\": enet,\n",
    "                \"enet_coegs\": enet_coef,\n",
    "                \"train_data_enet\": pd.Series(enet.predict(X_train), index=y_train.index, name=f\"enet_train\"),\n",
    "                \"yhat_enet\": pd.Series(yhat_enet, index=y_test.index, name=f\"{contract}_{target}_enet_pred\"),\n",
    "            })\n",
    "\n",
    "        # 5. PCA on OLS\n",
    "        if 'pca' in models:\n",
    "            pca = PCA(n_components=.9).fit(X_train) # keep x% of variance and fit to training set\n",
    "            train_pca = pca.transform(X_train)\n",
    "            test_pca = pca.transform(X_test)\n",
    "\n",
    "            ols_pca = LinearRegression()\n",
    "            ols_pca.fit(train_pca, y_train)\n",
    "            yhat_pca = ols_pca.predict(test_pca)\n",
    "            eval_and_report(y_test, yhat_pca, \"OLS+PCA\")\n",
    "\n",
    "            results[contract].update({\n",
    "                \"pca_model\": ols_pca,\n",
    "                \"train_data_pca\": pd.Series(ols_pca.predict(train_pca), index=y_train.index, name=f\"pca_train\"),\n",
    "                \"yhat_pca\": pd.Series(yhat_pca, index=y_test.index, name=f\"{contract}_{target}_pca_pred\")\n",
    "            })\n",
    "\n",
    "        # 6. Use lasso into ridge\n",
    "        if 'lasso_ridge' in models:\n",
    "            alphas = np.logspace(-4, 6, 100)\n",
    "\n",
    "            lasso = LassoCV(alphas=alphas, cv=tscv, fit_intercept=True)\n",
    "            lasso.fit(X_train, y_train)\n",
    "            yhat_lasso = lasso.predict(X_test)\n",
    "            eval_and_report(y_test, yhat_lasso, f\"Lasso (alpha={lasso.alpha_:.4g})\")\n",
    "\n",
    "            # Print top coefficients\n",
    "            lasso_coef = pd.Series(lasso.coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "            print(\"Top Lasso coeffs:\\n\", lasso_coef.head(10))\n",
    "\n",
    "            results[contract].update({\n",
    "                \"lasso_model\": lasso,\n",
    "                \"lasso_coefs\": lasso_coef,\n",
    "                \"train_data_lasso\": pd.Series(lasso.predict(X_train), index=y_train.index, name=f\"lasso_train\"),\n",
    "                \"yhat_lasso\": pd.Series(yhat_lasso, index=y_test.index, name=f\"{contract}_{target}_lasso_pred\"),\n",
    "            })\n",
    "        \n",
    "            # Get columns where coefficients are nonzero\n",
    "            important_features = X_train.columns[np.abs(lasso.coef_) > 0]\n",
    "            X_train_reduced = X_train[important_features]\n",
    "            X_test_reduced = X_test[important_features]\n",
    "\n",
    "            alphas = np.logspace(-4, 3, 30)\n",
    "\n",
    "            lasso_ridge = RidgeCV(alphas=alphas, cv=tscv, fit_intercept=True)\n",
    "            lasso_ridge.fit(X_train_reduced, y_train)\n",
    "            yhat_lasso_ridge = lasso_ridge.predict(X_test_reduced)\n",
    "            eval_and_report(y_test, yhat_lasso_ridge, f\"Lasso_Ridge (alpha={lasso_ridge.alpha_:.4g})\")\n",
    "\n",
    "            # Print top coefficients\n",
    "            lasso_ridge_coef = pd.Series(lasso_ridge.coef_, index=important_features).sort_values(key=np.abs, ascending=False)\n",
    "            print(\"Top Lasso_Ridge coeffs:\\n\", lasso_ridge_coef.head(10))\n",
    "\n",
    "            results[contract].update({\n",
    "                \"lasso_ridge_model\": lasso_ridge,\n",
    "                \"lasso_ridge_coefs\": lasso_ridge_coef,\n",
    "                \"train_data_lasso_ridge\": pd.Series(lasso_ridge.predict(X_train_reduced), index=y_train.index, name=f\"lasso_ridge_train\"),\n",
    "                \"yhat_lasso_ridge\": pd.Series(yhat_lasso_ridge, index=y_test.index, name=f\"{contract}_{target}_lasso_ridge_pred\"),\n",
    "            })\n",
    "\n",
    "        # 7. Partial Least Squares (PLS) Regression\n",
    "        if 'pls' in models:\n",
    "            # First, find the best number of components using cross-validation\n",
    "            best_n = -1\n",
    "            best_score = -np.inf\n",
    "            \n",
    "            # Test a range of components (e.g., from 1 to 20)\n",
    "            for n in range(1, 8):\n",
    "                pls_cv = PLSRegression(n_components=n)\n",
    "                # Use a time-series friendly CV\n",
    "                scores = cross_val_score(pls_cv, X_train, y_train, cv=tscv, scoring='r2')\n",
    "                \n",
    "                if np.mean(scores) > best_score:\n",
    "                    best_score = np.mean(scores)\n",
    "                    best_n = n\n",
    "\n",
    "            print(f\"PLS best n_components: {best_n}\")\n",
    "\n",
    "            # Now, fit the final PLS model with the optimal number of components\n",
    "            pls = PLSRegression(n_components=best_n)\n",
    "            pls.fit(X_train, y_train)\n",
    "            yhat_pls = pls.predict(X_test)\n",
    "            eval_and_report(y_test, yhat_pls, f\"PLS (n={best_n})\")\n",
    "\n",
    "            # can get coefficients if needed, but they are in PLS component space\n",
    "            # pls_coef = pd.Series(pls.coef_, index=features)\n",
    "\n",
    "            # Store results\n",
    "            results[contract].update({\n",
    "                \"pls_model\": pls,\n",
    "                # \"pls_coefs\": pls_coef,\n",
    "                \"train_data_pls\": pd.Series(pls.predict(X_train).ravel(), index=y_train.index, name=f\"pls_train\"),\n",
    "                \"yhat_pls\": pd.Series(yhat_pls.ravel(), index=y_test.index, name=f\"{contract}_{target}_pls_pred\"),\n",
    "            })\n",
    "            \n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d9f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_results = get_or_build_results('model_data/drift_training_results.pkl.gz', train_models, drift_train_scaled, drift_test_scaled, linear_model_target, models=linear_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5724d19",
   "metadata": {},
   "source": [
    "Make into new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85911f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_predictions(data, target, results, contracts=contracts, models=linear_models):\n",
    "    df = data.copy()\n",
    "\n",
    "    for contract in contracts:\n",
    "        for model in models:\n",
    "            df[f'{contract}_{target}_pred_{model}'] = pd.concat([results[contract][f'train_data_{model}'],results[contract][f'yhat_{model}']])\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120748fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift = df_from_predictions(drift, linear_model_target, drift_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595774e",
   "metadata": {},
   "source": [
    "Plot these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9644fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(data, baseline, measures, title, subtitles, contracts=contracts, models=linear_models):\n",
    "    fig = make_subplots(\n",
    "        rows=len(contracts), cols=1, shared_xaxes=True, vertical_spacing=0.06,\n",
    "        subplot_titles=[contract for contract in contracts]\n",
    "    )\n",
    "\n",
    "    for i, (contract) in enumerate(contracts, start=1):\n",
    "        show_leg = (i == 1)\n",
    "\n",
    "        # True next_ret\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=data.index, y=data[f'{contract}_{baseline}'], name=f'{contract}_{baseline}',\n",
    "                    mode=\"lines\", line=dict(width=1.6),\n",
    "                    showlegend=show_leg, legendgroup=\"true\"),\n",
    "            row=i, col=1\n",
    "        )\n",
    "\n",
    "        # Plot predictions\n",
    "        for model in models:\n",
    "            for measure in measures:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=data.index, y=data[f'{contract}_{measure}_{model}'], name=f\"{model} prediction\",\n",
    "                            mode=\"lines\", line=dict(width=1.4, dash=\"dot\"),\n",
    "                            showlegend=show_leg, legendgroup=f\"{model}\"),\n",
    "                    row=i, col=1\n",
    "                )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        height=2500,\n",
    "        hovermode=\"x unified\",\n",
    "        template=\"plotly_white\",\n",
    "        margin=dict(t=80, r=30, b=80, l=70),\n",
    "        legend=dict(orientation=\"h\", yanchor=\"top\", y=-0.12, xanchor=\"left\", x=0)\n",
    "    )\n",
    "\n",
    "    for r in range(1, len(contracts) + 1):\n",
    "        fig.update_yaxes(title_text=subtitles, row=r, col=1)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7117fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(drift, 'log_close_ret', [f'{linear_model_target}_pred'], 'Linear Regression on Drift', 'Log Return')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9793f600",
   "metadata": {},
   "source": [
    "Overall returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67800fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_price_path(df, pred, linear_model_target=linear_model_target, contracts=contracts, models=linear_models):\n",
    "    df = df.copy()\n",
    "\n",
    "    for contract in contracts:\n",
    "        for model in models:\n",
    "            df[f'{contract}_{pred}_pred_{model}'] = np.exp(df[f'{contract}_log_close'].iloc[0]) * np.exp(df[f'{contract}_{linear_model_target}_pred_{model}'].cumsum())\n",
    "\n",
    "            df[f'{contract}_close'] = np.exp(df[f'{contract}_log_close'].iloc[0]) * np.exp(df[f'{contract}_log_close_ret'].cumsum())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_drift = build_price_path(drift, 'price')\n",
    "plot_results(plot_drift, 'close', ['price_pred'], 'Price Path Evolution of Drift', 'Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6491cf0d",
   "metadata": {},
   "source": [
    "# Volatility Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c40168",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_data = data.copy()\n",
    "vol_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared residuals from previous -- using ElasticNet\n",
    "# Turns out we actually need to also take the log because they are so tiny\n",
    "\n",
    "linear_drift_model = 'enet'\n",
    "volatility_model_target = 'log_squared_residual'\n",
    "\n",
    "for contract in contracts:\n",
    "    vol_data[f'{contract}_{volatility_model_target}'] = np.log(np.square(drift[f'{contract}_{linear_model_target}'] - drift[f'{contract}_{linear_model_target}_pred_{linear_drift_model}']))\n",
    "vol_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c11cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_training_data, vol_testing_data = split_data(vol_data)\n",
    "vol_targets, vol_all_features, vol_unscale_features, vol_scale_features = set_targets_features(volatility_model_target, vol_training_data)\n",
    "\n",
    "\n",
    "# Random forest for now because it's faster\n",
    "vol_pruned_features = get_or_build_results('model_data/vol_features.pkl.gz', determine_feature_importance, vol_training_data, vol_testing_data, vol_all_features, volatility_model_target, mode='random_forest')\n",
    "\n",
    "for contract in contracts:\n",
    "    print(vol_pruned_features[contract]['selected_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd4472",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_train_scaled, vol_test_scaled = scale_features(vol_training_data, vol_testing_data, vol_scale_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af70788",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_test_scaled.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d247f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "volatility_models = ['ridge', 'lasso', 'pca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e386364",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_results = get_or_build_results('model_data/vol_results.pkl.gz', train_models, vol_train_scaled, vol_test_scaled, volatility_model_target, pruned_features=vol_pruned_features, models=volatility_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdeb113",
   "metadata": {},
   "source": [
    "Make into new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = df_from_predictions(vol_data, volatility_model_target, vol_results, models=volatility_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3602b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(diffusion, volatility_model_target, [f'{volatility_model_target}_pred'], 'Linear Regression on Diffusion', 'Log Residuals', models=volatility_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2080f5be",
   "metadata": {},
   "source": [
    "Convert to variance / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for contract in contracts:\n",
    "    for model in volatility_models:\n",
    "        diffusion[f'{contract}_variance_pred_{model}'] = np.exp(diffusion[f'{contract}_{volatility_model_target}_pred_{model}'])\n",
    "        diffusion[f'{contract}_std_pred_{model}'] = np.sqrt(diffusion[f'{contract}_variance_pred_{model}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.merge(drift, diffusion, left_index=True, right_index=True, how='inner', suffixes=('', '_to_drop'))\n",
    "predictions = predictions.drop(columns=[col for col in predictions.columns if col.endswith('_to_drop')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b6834f",
   "metadata": {},
   "source": [
    "Let's add a rolling IC column to test with damping mu projections later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for contract in contracts:\n",
    "    predictions[f'{contract}_{linear_model_target}'] = predictions[f'{contract}_log_close'].diff().shift(-1).dropna(inplace=True)\n",
    "    for model in linear_models:\n",
    "        predictions[f'{contract}_IC_{model}'] = predictions[f'{contract}_{linear_model_target}'].rolling(window=21).corr(predictions[f'{contract}_{linear_model_target}_pred_{model}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe9be3d",
   "metadata": {},
   "source": [
    "## Looks like we obtained a pretty smooth path. There are many extreme values not being captured -- Thus the motivation for GARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f67b6",
   "metadata": {},
   "source": [
    "# GARCH Volatility Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0889c7c",
   "metadata": {},
   "source": [
    "Set up some GARCH models, check their fit our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904cc1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_data = drift.copy()\n",
    "garch_scale = 100\n",
    "\n",
    "\n",
    "# Get drift residuals:\n",
    "for contract in contracts:\n",
    "    garch_data[f'{contract}_drift_residuals_{linear_drift_model}'] = drift[f'{contract}_{linear_model_target}'] - drift[f'{contract}_{linear_model_target}_pred_{linear_drift_model}'] # Calculate residual from enet\n",
    "    garch_data[f'{contract}_drift_residuals_{linear_drift_model}'] = garch_data[f'{contract}_drift_residuals_{linear_drift_model}'] * garch_scale # Scaling for better convergence\n",
    "\n",
    "# Split data\n",
    "garch_training, garch_testing = split_data(garch_data)\n",
    "\n",
    "# Models, distributions, and paramters to check\n",
    "\n",
    "garch_models = {\n",
    "    'Garch': 'Garch',\n",
    "    'Egarch': 'Egarch',\n",
    "    'GJR_Garch': 'Garch',\n",
    "    'APARCH': 'APARCH'\n",
    "}\n",
    "\n",
    "dists = ['normal', 't', 'skewt']\n",
    "\n",
    "params = {\n",
    "    'Garch': {'p': 1, 'o': 0, 'q': 1},\n",
    "    'Egarch': {'p': 2, 'o': 1, 'q': 1},\n",
    "    'GJR_Garch': {'p': 1, 'o': 1, 'q': 1},\n",
    "    'APARCH': {'p': 1, 'o': 1, 'q': 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cdce77",
   "metadata": {},
   "source": [
    "Grid search for rigorous model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76aeaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlike(y2, fvar):\n",
    "    y2 = np.asarray(y2, float)\n",
    "    fvar = np.asarray(fvar, float)\n",
    "    return np.mean(y2 / fvar + np.log(fvar))\n",
    "\n",
    "def fit_one(y, vol_name, model_name, p, o, q, dist, mean='Constant', lags=0):\n",
    "    kw = dict(mean=mean, lags=lags, vol=vol_name, dist=dist, p=p, q=q)\n",
    "    if model_name != 'Garch':  # only asym metrics use 'o'\n",
    "        kw['o'] = o\n",
    "    am = arch_model(y, **kw)\n",
    "    return am.fit(disp='off')\n",
    "\n",
    "def fit_arx(y, X, vol_name, model_name, p, o, q, dist, mean, lags):\n",
    "    kw = dict(mean=mean, lags=lags, x=X, vol=vol_name, dist=dist, p=p, q=q)\n",
    "    if model_name != 'Garch':  # only asym metrics use 'o'\n",
    "        kw['o'] = o\n",
    "    am = arch_model(y, **kw)\n",
    "    return am.fit(disp='off')\n",
    "\n",
    "def rolling_oos_qlike(y, vol_name, model_name, p, o, q, dist,\n",
    "                      mean='Constant', lags=0, refit_every=21, start_frac=0.6):\n",
    "    \"\"\"1-step-ahead variance forecasts with periodic refits.\"\"\"\n",
    "    y = y.dropna()\n",
    "    n = len(y)\n",
    "    start = int(n * start_frac)\n",
    "    fvars = []\n",
    "    res = None\n",
    "    for t in range(start, n - 1):\n",
    "        if res is None or ((t - start) % refit_every == 0):\n",
    "            res = fit_one(y.iloc[:t + 1], vol_name, model_name, p, o, q, dist, mean, lags)\n",
    "        fvar = res.forecast(horizon=1, reindex=False).variance.values[-1, 0]\n",
    "        fvars.append(fvar)\n",
    "    return qlike((y**2).iloc[start+1:], fvars)\n",
    "\n",
    "def garch_grid_search(\n",
    "    garch_training,             # DataFrame with your series, e.g., f'{contract}_drift_residuals_enet'             # dict like {'Garch':'GARCH','Egarch':'EGARCH','GJR_Garch':'GJR-GARCH','APARCH':'APARCH'}\n",
    "    dists=('t','skewt'),\n",
    "    P=(1,2,3), Q=(1,2,3), O=(0,1),\n",
    "    means=('Constant', 'Zero'), lags=(0,1,2),\n",
    "    contracts=contracts,\n",
    "    garch_models = garch_models,\n",
    "    compute_oos=False, refit_every=21, start_frac=0.6, arx=False, X=None\n",
    "):\n",
    "    rows = []\n",
    "    for contract in contracts:\n",
    "        y = garch_training[f'{contract}_drift_residuals_{linear_drift_model}'].dropna()\n",
    "        for model_name, vol_name in garch_models.items():\n",
    "            # Build parameter grid; for plain GARCH, force o=0 only\n",
    "            grid_O = (0,) if model_name == 'Garch' else O\n",
    "            for p, q, o, dist, lag, mean in product(P, Q, grid_O, dists, lags, means):\n",
    "                try:\n",
    "                    if arx:\n",
    "                        res = fit_arx(y, X, vol_name, model_name, p, o, q, dist, mean, lag)\n",
    "                    else:\n",
    "                        res = fit_one(y, vol_name, model_name, p, o, q, dist, mean, lag)\n",
    "                    aic = res.aic\n",
    "                    bic = res.bic\n",
    "                    llf = res.loglikelihood\n",
    "                    row = {\n",
    "                        'contract': contract,\n",
    "                        'model': model_name,\n",
    "                        'vol': vol_name,\n",
    "                        'dist': dist,\n",
    "                        'p': p, 'o': o, 'q': q,\n",
    "                        'aic': aic, 'bic': bic, 'llf': llf,\n",
    "                        'lags': lag,\n",
    "                        'mean': mean\n",
    "                    }\n",
    "                    if compute_oos:\n",
    "                        try:\n",
    "                            oos = rolling_oos_qlike(y, vol_name, model_name, p, o, q, dist,\n",
    "                                                    mean, lags, refit_every, start_frac)\n",
    "                        except Exception:\n",
    "                            oos = np.nan\n",
    "                        row['oos_qlike'] = oos\n",
    "                    rows.append(row)\n",
    "                except Exception:\n",
    "                    # skip failed fits\n",
    "                    print('Failed fit, skipping')\n",
    "                    continue\n",
    "    out = pd.DataFrame(rows)\n",
    "    if out.empty:\n",
    "        return out\n",
    "    sort_cols = ['oos_qlike','aic'] if compute_oos else ['aic']\n",
    "    ascending = [True] * len(sort_cols)\n",
    "    return out.sort_values(sort_cols, ascending=ascending).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd25add",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = get_or_build_results('model_data/garch_grid_search', garch_grid_search, garch_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results.drop_duplicates(subset=['contract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2344a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_test_models = ['Egarch', 'APARCH']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510be84",
   "metadata": {},
   "source": [
    "## Forecast our testing period with the chosen models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dba090",
   "metadata": {},
   "outputs": [],
   "source": [
    "refit_freq = 21 # Frequency to refit at -- slightly larger helps with signal:noislambda, but not too big\n",
    "\n",
    "def forecast_garch(garch_data, contracts=contracts, models=garch_test_models, linear_drift_model=linear_drift_model):\n",
    "    garch_training, garch_testing = split_data(garch_data)\n",
    "\n",
    "    for contract in contracts:\n",
    "        for model in models:\n",
    "            drift_residuals = garch_data[f'{contract}_drift_residuals_{linear_drift_model}']\n",
    "\n",
    "            scaled_predictions = []\n",
    "            for i in range(len(garch_training), len(garch_training) + len(garch_testing)): # Walk forward, forecast each step and refit model based on new inputs\n",
    "\n",
    "                # Ensure we fit on first day and at our frequency\n",
    "                testing_day = i - len(garch_training)\n",
    "                is_refit_day = testing_day % refit_freq == 0\n",
    "                \n",
    "                if is_refit_day:\n",
    "                    garch = arch_model(drift_residuals.iloc[:i], vol=garch_models[model], **params[model], mean='Zero', dist='skewt')\n",
    "                    garch_fit = garch.fit(disp='off') # fit\n",
    "\n",
    "                forecast = garch_fit.forecast(horizon=1)\n",
    "\n",
    "                pred_variance = forecast.variance.iloc[-1].values[0]\n",
    "                scaled_predictions.append(pred_variance)\n",
    "\n",
    "            # Get exponentiation of std\n",
    "            if model == 'APARCH':\n",
    "                delta = garch_fit.params['delta']\n",
    "            else:\n",
    "                delta = 2\n",
    "            \n",
    "            unscaled_predictions = pd.Series(scaled_predictions, index=garch_testing.index) / (garch_scale ** delta) # Unscale (inputs scaled by 100 for convergence, var is squared, but APARCH may return delta =/= 2)\n",
    "            final_predictions = unscaled_predictions ** (1 / delta)\n",
    "\n",
    "            garch_testing[f'{contract}_std_pred_{model}'] = final_predictions\n",
    "\n",
    "    return garch_testing.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_testing = pd.DataFrame(get_or_build_results('model_data/garch_testing_df.pkl.gz', forecast_garch, garch_data))\n",
    "garch_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4275f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "for contract in contracts:\n",
    "    for model in garch_test_models:\n",
    "        predictions[f'{contract}_std_pred_{model}'] = garch_testing[f'{contract}_std_pred_{model}']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73fe15",
   "metadata": {},
   "source": [
    "# Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b85348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm # For confidence scaling testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c757b4d1",
   "metadata": {},
   "source": [
    "Sizing strategy 1: Using Merton portfolio optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5255f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kelly_fraction(mu, rf, sigma, fraction=.5):\n",
    "    size = fraction * (mu - rf) / (sigma**2 + 1e-8) \n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a684f672",
   "metadata": {},
   "source": [
    "Sizing strategy 2: Using vol target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a58ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vol_target_sizing(target, vol):\n",
    "    size = (target/vol)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894979b4",
   "metadata": {},
   "source": [
    "Returns info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_and_mdd(df, col_equity, col_rf=\"ten_yr\", periods_per_year=252):\n",
    "    eq = df[col_equity]\n",
    "    rf = df[col_rf]\n",
    "\n",
    "    # compute portfolio simple returns\n",
    "    port_rets = eq.pct_change().dropna()\n",
    "    # align rf\n",
    "    rf_aligned = rf.reindex(port_rets.index).astype(float)\n",
    "\n",
    "    # excess returns (assuming rf is already per-period, e.g. daily)\n",
    "    excess = port_rets - ((1+rf_aligned/100)**(1/periods_per_year)-1)\n",
    "    sharpe = (excess.mean() / excess.std()) * np.sqrt(periods_per_year) if excess.std() > 0 else np.nan\n",
    "\n",
    "    # max drawdown\n",
    "    running_max = eq.cummax()\n",
    "    drawdown = eq / running_max - 1.0\n",
    "    max_dd = drawdown.min()\n",
    "\n",
    "    return sharpe, max_dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44378db9",
   "metadata": {},
   "source": [
    "## Trading strategy 1: Trade based on our models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2f6bf",
   "metadata": {},
   "source": [
    "### Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73c1a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_models = ['ridge', 'lasso', 'APARCH', 'Egarch'] # Volatility models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c5260",
   "metadata": {},
   "source": [
    "Get historical margin amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_margins = {}\n",
    "\n",
    "for contract in contracts:\n",
    "    historical_margins[contract] = pd.read_csv(f'{contract}_Historical_Margin.csv')\n",
    "    historical_margins[contract]['Date'] = pd.to_datetime(historical_margins[contract]['Date'])\n",
    "    historical_margins[contract].set_index(historical_margins[contract]['Date'], inplace=True)\n",
    "    historical_margins[contract].drop(columns=['Date'], inplace=True)\n",
    "    historical_margins[contract].rename(columns={'Long Margin': f'{contract}_Long', 'Short Margin': f'{contract}_Short'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b0d18f",
   "metadata": {},
   "source": [
    "### Individual Contract BTs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34a8f4",
   "metadata": {},
   "source": [
    "Backtesting with margin functionality (with rules per CME future challenge)  \n",
    "Note that I am omitting the minimum 10 per day contract trading rule because we can just buy and sell quickly to nullify the effect  \n",
    "I am also omitting the liquidation before expiry rule because there are no expiries during the competition  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d579ebb1",
   "metadata": {},
   "source": [
    "It might be slower, but using iterrows is simpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf84ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial conditions\n",
    "starting_cash = 500_000\n",
    "commission = 2.5\n",
    "drawdown_lock = .2\n",
    "\n",
    "initial_margin = 1.1 # 10% extra assumption on initial entry\n",
    "tick_size = {\n",
    "    'ES': 0.25, \n",
    "    'NQ': 0.25, \n",
    "    'FV': 0.0078125, \n",
    "    'TY': 0.015625, \n",
    "    'US': 0.03125, \n",
    "    'GC': 0.10, \n",
    "    'CL': 0.01, \n",
    "    'JY': 0.0000005,\n",
    "    'BP': 0.0001, \n",
    "    'EU': 0.00005\n",
    "}\n",
    "\n",
    "tick_value = {\n",
    "    'ES': 12.50, \n",
    "    'NQ': 5.00, \n",
    "    'FV': 7.8125, \n",
    "    'TY': 15.625, \n",
    "    'US': 31.25, \n",
    "    'GC': 10.00, \n",
    "    'CL': 10.00, \n",
    "    'JY': 6.25,\n",
    "    'BP': 6.25, \n",
    "    'EU': 6.25\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bfe102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create backtest df copy\n",
    "bt = predictions.copy()\n",
    "bt = bt.join([margin_data for margin_data in historical_margins.values()])\n",
    "bt = bt.replace(0, pd.NA)\n",
    "bt = bt.fillna(method='ffill')\n",
    "bt_train, bt_test = split_data(bt)\n",
    "linear_drift_model = 'enet'\n",
    "\n",
    "bt_data = {}\n",
    "\n",
    "for contract in contracts:\n",
    "    bt_data[contract] = {}\n",
    "\n",
    "    for model in bt_models:\n",
    "        bt_test[f'{contract}_cash_{model}'] = 0.0\n",
    "        bt_test[f'{contract}_margin_{model}'] = 0.0\n",
    "        bt_test.loc[bt_test.index[0], f'{contract}_cash_{model}'] = starting_cash\n",
    "        bt_test[f'{contract}_signal_{model}'] = 0\n",
    "        bt_test[f'{contract}_baseline_wealth'] = np.exp(bt_test[f'{contract}_log_close_ret'].cumsum()) * starting_cash\n",
    "        bt_test[f'{contract}_margin_usage_{model}'] = 0\n",
    "\n",
    "\n",
    "        bt_data[contract][model] = {\n",
    "            'last_margin': 0,\n",
    "            'last_cash': starting_cash,\n",
    "            'position': 0,\n",
    "            'average_vol_weight': 0,\n",
    "            'average_confidence': 0,\n",
    "            'average_vol': 0,\n",
    "            'average_mu': 0,\n",
    "            'average_rf': 0,\n",
    "            'average_ret': 0,\n",
    "            'high_water': 0,\n",
    "        }\n",
    "\n",
    "# Starts at second day\n",
    "for idx, row in bt_test.iloc[1:].iterrows():\n",
    "    for contract in contracts:\n",
    "        vol_data = []\n",
    "        for model in bt_models:\n",
    "            last_margin = bt_data[contract][model]['last_margin']\n",
    "            last_cash = bt_data[contract][model]['last_cash']\n",
    "\n",
    "            multiplier = tick_value[contract] / tick_size[contract]\n",
    "            position_size = bt_data[contract][model]['position']\n",
    "            lock_up = False\n",
    "\n",
    "            \n",
    "            vol = row[f'{contract}_std_pred_{model}']\n",
    "\n",
    "            # Mu needs to be adjusted upwards for sigma, notice no exponentiation is needed since we are using continuous time from log gbm assumption\n",
    "            if model not in garch_models.keys():\n",
    "                mu = np.exp(row[f'{contract}_{linear_model_target}_pred_{linear_drift_model}'] + .5 * vol**2) - 1\n",
    "            else:\n",
    "                # We need a different adjustment for our garch models since they are not based on a normality assumption\n",
    "                garch_drift_adjustment = + (1/2) * vol**2 - 1/6 * vol**3\n",
    "                mu = np.exp(row[f'{contract}_{linear_model_target}_pred_{linear_drift_model}'] + garch_drift_adjustment) - 1\n",
    "                \n",
    "            rf = (1+row['^TNX_close']/100)**(1/252)-1 # Daily\n",
    "            investment = 0\n",
    "            ret = np.exp(row[f'{contract}_log_close_ret']) - 1\n",
    "            delta = ret*np.exp(row[f'{contract}_log_open']) # This calculation is basically assuming we are making our trading decisions at the end of every day\n",
    "            pred = mu*np.exp(row[f'{contract}_log_close'])\n",
    "            \n",
    "\n",
    "            # Update margin value\n",
    "            pnl_today = bt_data[contract][model]['position'] * delta * multiplier\n",
    "\n",
    "            # Calculate total equity\n",
    "            last_margin += pnl_today\n",
    "            total_equity = last_margin + last_cash\n",
    "\n",
    "            # Calculate max portfolio value for drawdown constraints\n",
    "            bt_data[contract][model]['high_water'] = max(total_equity, bt_data[contract][model]['high_water'])\n",
    "\n",
    "            # Check maintenance requirements\n",
    "            if position_size > 0:\n",
    "                direction = 'Long'\n",
    "                side = 1\n",
    "            else:\n",
    "                direction = 'Short'\n",
    "                side = -1\n",
    "\n",
    "            margin_difference = row[f'{contract}_{direction}']*position_size - last_margin\n",
    "            if margin_difference > 0:\n",
    "                # If enough cash to top up, do it automatically\n",
    "                if total_equity >= margin_difference:\n",
    "                    last_cash -= margin_difference\n",
    "                    last_margin += margin_difference\n",
    "                else:\n",
    "                    # Otherwise liquidate\n",
    "                    bt_data[contract][model]['position'] = 0\n",
    "                    last_cash += last_margin # Ignoring commission\n",
    "                    last_margin = 0\n",
    "            else:\n",
    "                last_cash += margin_difference\n",
    "                last_margin -= margin_difference\n",
    "\n",
    "            # Recalculate total equity\n",
    "            total_equity = last_margin + last_cash\n",
    "\n",
    "            # If drawdown criteria met, lock portfolio for day\n",
    "            if ret <= -drawdown_lock:\n",
    "                lock_up = True\n",
    "\n",
    "            if not lock_up:\n",
    "                # Get bet sizing with fractional kelly -- default is .5\n",
    "                max_leverage = 5\n",
    "                target_notional = kelly_fraction(mu, rf, vol, fraction=1) * total_equity # Daily rf\n",
    "                target_notional = max(-total_equity * max_leverage, min(target_notional, total_equity * max_leverage)) # Make sure we don't exceed equity\n",
    "\n",
    "                # Get bet sizing with fixed vol target\n",
    "                #base_weight = get_vol_target_sizing(.15 / np.sqrt(252), vol)\n",
    "                #confidence = (mu - rf) / vol\n",
    "                #k=2\n",
    "                #target_weight = base_weight * norm.cdf(confidence*np.sqrt(252))*k # Annualize\n",
    "                #target_notional = target_weight * total_equity\n",
    "                #target_notional = max(-total_equity, min(target_notional, total_equity)) # Make sure we don't exceed equity / effective equity cap\n",
    "\n",
    "                # Data logging\n",
    "                bt_data[contract][model]['average_rf'] += rf\n",
    "                bt_data[contract][model]['average_ret'] += ret\n",
    "                bt_data[contract][model]['average_mu'] += mu\n",
    "                bt_data[contract][model]['average_vol'] += vol\n",
    "                #bt_data[contract][model]['average_vol_weight'] += base_weight\n",
    "                #bt_data[contract][model]['average_confidence'] += norm.cdf(confidence*np.sqrt(252))*k\n",
    "\n",
    "                # Execute trade / Update investment amount to reach target position\n",
    "                if target_notional > 0:\n",
    "                    direction = 'Long'\n",
    "                else: \n",
    "                    direction = 'Short'\n",
    "\n",
    "                notional_per_contract = np.exp(row[f'{contract}_log_close']) * multiplier\n",
    "                initial_cost = row[f'{contract}_{direction}']*initial_margin # Get current maintenance * initial margin percentage requirement\n",
    "                position = math.floor(target_notional/notional_per_contract) # Get number of contracts to calculate margin investment, ignoring commission\n",
    "                investment = position*initial_cost - last_margin\n",
    "                investment = max(-last_cash, min(investment, last_cash))\n",
    "\n",
    "                bt_data[contract][model]['position'] = position\n",
    "\n",
    "            else:\n",
    "                investment = 0\n",
    "\n",
    "            # Update cash\n",
    "            last_cash = bt_data[contract][model]['last_cash'] = last_cash - investment\n",
    "            bt_test.loc[idx, f'{contract}_cash_{model}'] = last_cash\n",
    "\n",
    "            # Store portfolio value as last value for calculation\n",
    "            bt_data[contract][model]['last_margin'] = last_margin + investment\n",
    "            bt_test.loc[idx, f'{contract}_margin_{model}'] = bt_data[contract][model]['last_margin']\n",
    "            bt_test.loc[idx, f'{contract}_margin_usage_{model}'] = last_margin/total_equity\n",
    "\n",
    "            # Set signal for plotting later\n",
    "            if investment > 0:\n",
    "                bt_test.loc[idx, f'{contract}_signal_{model}'] = 1\n",
    "            elif investment < 0:\n",
    "                bt_test.loc[idx, f'{contract}_signal_{model}'] = -1\n",
    "    \n",
    "for contract in contracts:\n",
    "    print(f'=== {contract} ===')\n",
    "    total_value = f'{contract}_baseline_wealth'\n",
    "    \n",
    "    final_training_value = bt_test[total_value].iloc[-1]\n",
    "    training_cagr = 100 * ((final_training_value/bt_test[total_value].iloc[0])**(1/(bt_test.index[-1].year - bt_test.index[0].year + bt_test.index[-1].month/12))-1)\n",
    "    print(f'Final value for {contract}, baseline: {final_training_value:.2f}')\n",
    "    \n",
    "    test_sharpe, test_mdd = sharpe_and_mdd(bt_test, total_value, col_rf=\"^TNX_close\")\n",
    "    print(f\"Sharpe: {test_sharpe:.2f}, Max Drawdown: {test_mdd:.2%}, CAGR: {training_cagr:.2f}%\\n\")\n",
    "    for model in bt_models:\n",
    "        \n",
    "        total_value = f'{contract}_total_value_{model}'\n",
    "        bt_test[total_value] = bt_test[f'{contract}_margin_{model}'] + bt_test[f'{contract}_cash_{model}']\n",
    "\n",
    "        final_testing_value = bt_test[total_value].iloc[-1]\n",
    "        testing_cagr = 100*((final_testing_value/bt_test[total_value].iloc[0])**(1/(bt_test.index[-1].year - bt_test.index[0].year + bt_test.index[-1].month/12))-1)\n",
    "        print(f'Final value for {contract}, {model}: {final_testing_value:.2f}')\n",
    "        \n",
    "        test_sharpe, test_mdd = sharpe_and_mdd(bt_test, total_value, col_rf=\"^TNX_close\")\n",
    "        print(f\"Sharpe: {test_sharpe:.2f}, Max Drawdown: {test_mdd:.2%}, CAGR: {testing_cagr:.2f}%\")\n",
    "\n",
    "\n",
    "        bt_data[contract][model]['average_mu'] /= len(bt_test)\n",
    "        bt_data[contract][model]['average_vol'] /= len(bt_test)\n",
    "        bt_data[contract][model]['average_rf'] /= len(bt_test)\n",
    "        bt_data[contract][model]['average_ret'] /= len(bt_test)\n",
    "        average_margin_util = bt_test[f'{contract}_margin_usage_{model}'].sum() / len(bt_test)\n",
    "        print(f\"Average Vol: {bt_data[contract][model]['average_vol']:.4f}, Average mu: {bt_data[contract][model]['average_mu']:.4f}, Average ret: {bt_data[contract][model]['average_ret']:.4f}\")\n",
    "        print(f\"Average Margin Utilizization: {average_margin_util*100:.2f}%\")\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ee77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(bt_test, 'baseline_wealth', ['total_value'], 'Backtest Performance', 'Wealth', models=bt_models, contracts=contracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46b4a0",
   "metadata": {},
   "source": [
    "### Let's try making a single portfolio now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e67101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bt(data, baseline, measures, title, subtitles, models=bt_models):\n",
    "    fig = make_subplots(\n",
    "        rows=len(models), cols=1, shared_xaxes=True, vertical_spacing=0.06,\n",
    "        subplot_titles=[model for model in models]\n",
    "    )\n",
    "\n",
    "    for i, (model) in enumerate(models, start=1):\n",
    "        show_leg = (i == 1)\n",
    "\n",
    "        # True next_ret\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=data.index, y=data[f'{baseline}_{model}'], name=f'{baseline}_{model}',\n",
    "                    mode=\"lines\", line=dict(width=1.6),\n",
    "                    showlegend=show_leg, legendgroup=\"true\"),\n",
    "            row=i, col=1\n",
    "        )\n",
    "\n",
    "        # Plot predictions\n",
    "        for measure in measures:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=data.index, y=data[f'{measure}_{model}'], name=f\"{model} prediction\",\n",
    "                        mode=\"lines\", line=dict(width=1.4, dash=\"dot\"),\n",
    "                        showlegend=show_leg, legendgroup=f\"{measure}\"),\n",
    "                row=i, col=1\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        height=1200,\n",
    "        hovermode=\"x unified\",\n",
    "        template=\"plotly_white\",\n",
    "        margin=dict(t=80, r=30, b=80, l=70),\n",
    "        legend=dict(orientation=\"h\", yanchor=\"top\", y=-0.12, xanchor=\"left\", x=0)\n",
    "    )\n",
    "\n",
    "    for r in range(1, len(contracts) + 1):\n",
    "        fig.update_yaxes(title_text=subtitles, row=r, col=1)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b256c",
   "metadata": {},
   "source": [
    "Historical Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c18a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate historical correlation matrix for train returns\n",
    "contract_corr = bt_train[[f'{contract}_log_close_ret' for contract in contracts]].corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68546a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_asset_kelly_fraction(mu, sigma, fraction=.5):\n",
    "    weights = fraction * np.linalg.solve(sigma, mu) \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227efbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b247520c",
   "metadata": {},
   "source": [
    "Backtesting on multiple assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6815d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_asset_backtest(max_leverage=15, kelly_frac=.5, bt_models=bt_models):\n",
    "    # Create backtest df copy\n",
    "    bt = predictions.copy()\n",
    "    bt = bt.join([margin_data for margin_data in historical_margins.values()])\n",
    "    bt = bt.replace(0, pd.NA)\n",
    "    bt = bt.fillna(method='ffill')\n",
    "    bt_train, bt_test = split_data(bt)\n",
    "    linear_drift_model = 'enet'\n",
    "\n",
    "    multipliers = np.array([tick_value[contract]/tick_size[contract] for contract in contracts])\n",
    "\n",
    "    T = len(bt_test)\n",
    "\n",
    "    close_cols  = [f'{contract}_log_close' for contract in contracts]\n",
    "    ret_cols   = [f'{contract}_log_close_ret' for contract in contracts]\n",
    "    long_cols  = [f'{contract}_Long' for contract in contracts]\n",
    "    short_cols = [f'{contract}_Short' for contract in contracts]\n",
    "    mu_cols    = [f'{contract}_{linear_model_target}_pred_{linear_drift_model}' for contract in contracts]\n",
    "\n",
    "    rf_mat = np.power((1+(bt_test['^TNX_close'].to_numpy(float))/100), 1/252)-1  # Daily\n",
    "    raw_mu_mat = bt_test[mu_cols].to_numpy(float)\n",
    "    ret_mat = np.exp(bt_test[ret_cols].to_numpy(float)) - 1\n",
    "    close_mat = np.exp(bt_test[close_cols].to_numpy(float))\n",
    "    maintL_mat = bt_test[long_cols].to_numpy(float)\n",
    "    maintS_mat = bt_test[short_cols].to_numpy(float)\n",
    "\n",
    "    for model in bt_models:\n",
    "        bt_test[f'cash_{model}'] = 0.0\n",
    "        bt_test[f'cash_{model}'].iloc[0] = starting_cash\n",
    "        bt_test[f'margin_{model}'] = 0.0\n",
    "        equal_weighted_returns = np.exp(bt_test[ret_cols].cumsum())\n",
    "        portfolio_values = equal_weighted_returns * (starting_cash / len(contracts))\n",
    "        bt_test[f'baseline_wealth_{model}'] = portfolio_values.sum(axis=1)\n",
    "        bt_test[f'margin_usage_{model}'] = 0\n",
    "        \n",
    "        vol_cols = [f'{contract}_std_pred_{model}' for contract in contracts]\n",
    "        pred_vol_mat = bt_test[vol_cols].to_numpy(float)\n",
    "        pred_var_mat = np.square(pred_vol_mat)\n",
    "\n",
    "        margins = np.full(len(contracts),0.0)\n",
    "        positions = np.full(len(contracts),0)\n",
    "        last_total_equity = 0\n",
    "        total_equity = 0\n",
    "        cash = starting_cash\n",
    "\n",
    "        # Loop\n",
    "        for t in tqdm(range(1, T)):\n",
    "            rf = rf_mat[t]\n",
    "            lock_up = False\n",
    "\n",
    "            # Get predicted variances for next day and create covariance matrix based on historical correlation\n",
    "            pred_vol = pred_vol_mat[t]\n",
    "            D = np.diag(pred_vol)\n",
    "            pred_cov = D @ contract_corr @ D\n",
    "\n",
    "            # mu needs to be adjusted upwards for sigma\n",
    "            raw_mu = raw_mu_mat[t]\n",
    "            if model not in garch_models.keys():\n",
    "                mu = np.exp(raw_mu + (1/2) * pred_var_mat[t]) - 1\n",
    "            else:\n",
    "                # We need a different adjustment for our garch models since they are not based on a normality assumption\n",
    "                garch_drift_adjustment = (1/2) * pred_var_mat[t] - (1/6) * np.power(pred_vol_mat[t], 3)\n",
    "                mu = np.exp(raw_mu + garch_drift_adjustment) - 1\n",
    "\n",
    "            # Update margin value\n",
    "            prevClose = close_mat[t-1]\n",
    "            close = close_mat[t]\n",
    "            ret = ret_mat[t]\n",
    "            delta = ret*prevClose # This calculation is basically assuming we are making our trading decisions at the end of every day\n",
    "            pnl_today = positions * delta * multipliers\n",
    "            margins += pnl_today\n",
    "\n",
    "            # Calculate total equity for lockup criteria\n",
    "            last_total_equity = total_equity\n",
    "            total_equity = margins.sum() + cash\n",
    "\n",
    "            # If drawdown criteria met, lock portfolio for day\n",
    "            lock_up = total_equity/(last_total_equity + 1e-8) <= (1 - drawdown_lock)\n",
    "\n",
    "            # Check maintenance requirements\n",
    "            maint_long = maintL_mat[t]\n",
    "            maint_short = maintS_mat[t]\n",
    "            maint_side = np.where(positions > 0, maint_long, maint_short)\n",
    "\n",
    "            maint_diff = maint_side*positions - margins\n",
    "\n",
    "            if maint_diff.sum() > cash:\n",
    "                # Liquidate -- simple, not greedy (may be less effective)\n",
    "                cash += margins.sum()\n",
    "                margins *= 0\n",
    "                positions *= 0\n",
    "            else:\n",
    "                # Top ups & off load to cash -- notice that 'negative' maint_diff means return to normal maintenance and add to cash\n",
    "                margins += maint_diff\n",
    "                cash -= maint_diff.sum()\n",
    "\n",
    "            # Recalculate total equity\n",
    "            total_equity = margins.sum() + cash\n",
    "\n",
    "            if not lock_up:\n",
    "                # Get bet sizing with fractional kelly -- default is .5\n",
    "                target_notionals = np.clip(multi_asset_kelly_fraction(mu-rf, pred_cov, fraction=kelly_frac), -max_leverage, max_leverage) * total_equity\n",
    "                maint_side = np.where(target_notionals > 0, maint_long, maint_short)\n",
    "\n",
    "                # Portfolio allocation\n",
    "                position_found = False\n",
    "                k = 1 # Scaling constant to reduce portfolio investment if needed\n",
    "                while not position_found and k > 0:\n",
    "                    notional_per_contract = close * multipliers\n",
    "                    initial_costs = maint_side*initial_margin # Get current maintenance * initial margin percentage requirement\n",
    "                    target_positions = target_notionals * k / notional_per_contract # Get number of contracts to calculate margin investment, ignoring commission\n",
    "                    target_positions = np.where(target_positions > 0, np.floor(target_positions), np.ceil(target_positions))\n",
    "                    \n",
    "                    investments = target_positions*initial_costs - margins\n",
    "                    \n",
    "                    required_margin = np.abs(investments).sum()\n",
    "                    total_investment = investments.sum()\n",
    "\n",
    "                    if required_margin.sum() > cash:\n",
    "                        k -= .05\n",
    "                    else:\n",
    "                        position_found = True\n",
    "            else:\n",
    "                investments = np.full(len(contracts),0.0)\n",
    "                total_investment = 0\n",
    "                target_positions = positions\n",
    "\n",
    "            # Update cash\n",
    "            cash -= total_investment\n",
    "            bt_test.iloc[t, bt_test.columns.get_loc(f'cash_{model}')] = cash\n",
    "\n",
    "            # Update positions and margins\n",
    "            positions = target_positions\n",
    "            margins += investments\n",
    "            bt_test.iloc[t, bt_test.columns.get_loc(f'margin_{model}')] = margins.sum()\n",
    "        \n",
    "        # Update models in bt_test\n",
    "        total_value = f'total_value_{model}'\n",
    "        bt_test[total_value] = bt_test[f'margin_{model}'] + bt_test[f'cash_{model}']\n",
    "\n",
    "    return bt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14a61ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_test = multi_asset_backtest(max_leverage=16.5, kelly_frac=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d14e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in bt_models:\n",
    "    print(f'=== {model} ===')\n",
    "    total_value = f'baseline_wealth_{model}'\n",
    "    \n",
    "    final_training_value = bt_test[total_value].iloc[-1]\n",
    "    training_cagr = 100 * ((final_training_value/bt_test[total_value].iloc[0])**(1/(bt_test.index[-1].year - bt_test.index[0].year + bt_test.index[-1].month/12))-1)\n",
    "    print(f'Final value for {contract}, baseline: {final_training_value:.2f}')\n",
    "    \n",
    "    test_sharpe, test_mdd = sharpe_and_mdd(bt_test, total_value, col_rf=\"^TNX_close\")\n",
    "    print(f\"Sharpe: {test_sharpe:.2f}, Max Drawdown: {test_mdd:.2%}, CAGR: {training_cagr:.2f}%\\n\")\n",
    "\n",
    "    total_value = f'total_value_{model}'\n",
    "\n",
    "    final_testing_value = bt_test[total_value].iloc[-1]\n",
    "    testing_cagr = 100*((final_testing_value/bt_test[total_value].iloc[0])**(1/(bt_test.index[-1].year - bt_test.index[0].year + bt_test.index[-1].month/12))-1)\n",
    "    print(f'Final value for {model}: {final_testing_value:.2f}')\n",
    "    \n",
    "    test_sharpe, test_mdd = sharpe_and_mdd(bt_test, total_value, col_rf=\"^TNX_close\")\n",
    "    print(f\"Sharpe: {test_sharpe:.2f}, Max Drawdown: {test_mdd:.2%}, CAGR: {testing_cagr:.2f}%\")\n",
    "\n",
    "    average_margin_util = bt_test[f'margin_usage_{model}'].sum() / len(bt_test)\n",
    "    print(f\"Average Margin Utilizization: {average_margin_util*100:.2f}%\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62767242",
   "metadata": {},
   "source": [
    "### Hyperparameter grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3173a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d15bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter ranges\n",
    "leverages = np.arange(10.0, 20.0, 0.5)  # A more reasonable range than 1-50\n",
    "kelly_fracs = np.arange(0.1, 1.1, 0.1)\n",
    "results = {}\n",
    "testing_model = 'APARCH'\n",
    "\n",
    "# Create the grid of all parameter combinations\n",
    "param_grid = list(itertools.product(leverages, kelly_fracs))\n",
    "print(f\"Starting grid search for {len(param_grid)} parameter combinations...\")\n",
    "\n",
    "for leverage, kelly_frac in tqdm(param_grid):\n",
    "    # The dictionary key is now a tuple of the parameters\n",
    "    param_key = (leverage, kelly_frac)\n",
    "    try:\n",
    "        bt_test = multi_asset_backtest(max_leverage=leverage, kelly_frac=kelly_frac, bt_models=[testing_model])\n",
    "\n",
    "        total_value_col = f'total_value_{testing_model}'\n",
    "        final_value = bt_test[total_value_col].iloc[-1]\n",
    "        sharpe, mdd = sharpe_and_mdd(bt_test, total_value_col, col_rf=\"^TNX_close\")\n",
    "\n",
    "        results[param_key] = (final_value, sharpe)\n",
    "    except Exception as e:\n",
    "        print(f\"Backtest failed for params {param_key}. Reason: {e}\")\n",
    "        results[param_key] = (np.nan, np.nan)\n",
    "\n",
    "# --- Analysis: Find the Best Result ---\n",
    "\n",
    "# Filter out failed runs\n",
    "valid_results = {k: v for k, v in results.items() if isinstance(v, tuple) and not np.isnan(v[1])}\n",
    "\n",
    "if valid_results:\n",
    "    # Find the parameter tuple (key) that corresponds to the maximum Sharpe ratio (the 2nd element of the value)\n",
    "    best_params = max(valid_results, key=lambda k: valid_results[k][1])\n",
    "    \n",
    "    # Unpack the parameters and the results\n",
    "    best_leverage, best_kelly_frac = best_params\n",
    "    best_total_value, best_sharpe_ratio = valid_results[best_params]\n",
    "\n",
    "    print(\"\\n--- Optimization Complete (based on Sharpe Ratio) ---\")\n",
    "    print(f\"Best Sharpe Ratio: {best_sharpe_ratio:.4f}\")\n",
    "    print(f\"Total Value at Best Sharpe: ${best_total_value:,.2f}\")\n",
    "    print(f\"Achieved with Max Leverage: {best_leverage:.2f}\")\n",
    "    print(f\"Achieved with Kelly Fraction: {best_kelly_frac:.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nAll backtests failed.\")\n",
    "\n",
    "# --- Better Analysis with Pandas ---\n",
    "print(\"\\n--- Top 10 Results ---\")\n",
    "# Convert results to a DataFrame for easier analysis\n",
    "records = []\n",
    "for params, metrics in valid_results.items():\n",
    "    records.append({\n",
    "        'Leverage': params[0],\n",
    "        'KellyFrac': params[1],\n",
    "        'FinalValue': metrics[0],\n",
    "        'SharpeRatio': metrics[1]\n",
    "    })\n",
    "results_df = pd.DataFrame(records)\n",
    "print(results_df.sort_values(by='SharpeRatio', ascending=False).head(10).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc7a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bt(bt_test, 'baseline_wealth', ['total_value'], 'Backtest Performance', 'Wealth', models=bt_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
