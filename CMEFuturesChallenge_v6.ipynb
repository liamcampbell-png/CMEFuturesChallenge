{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef69f29",
   "metadata": {},
   "source": [
    "# Notebook for CME Futures Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64da73",
   "metadata": {},
   "source": [
    "# Downloading historical data for indices (S&P, NASDAQ, DJIA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f4420",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f8bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import re\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8196f13",
   "metadata": {},
   "source": [
    "Make get_data function for downloading from yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe = '1000mo' # set timeframe\n",
    "\n",
    "def get_data(tickers: List):\n",
    "    data_dictionary = {}\n",
    "    for ticker in tickers:\n",
    "        data_dictionary[ticker] = yf.download(ticker, period=timeframe, interval='1d')\n",
    "    return data_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fd38e6",
   "metadata": {},
   "source": [
    "# Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e00db",
   "metadata": {},
   "source": [
    "We need continuized data for returns information about futures since they roll over at expiration dates. I bought some continuous data for cheap, it's not back adjusted, which we could do - but I may try doing CV folds on each contract instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_df = pd.read_csv('ES.txt') # E S&P\n",
    "nq_df = pd.read_csv('NQ.txt') # E Nasdaq\n",
    "fv_df = pd.read_csv('FV.txt') # 5yr us treasury note contracts\n",
    "ty_df = pd.read_csv('TY.txt') # 10yr US treasury bond contracts\n",
    "us_df = pd.read_csv('US.txt') # 30yr US treasury bond contracts\n",
    "gc_df = pd.read_csv('GC.txt') # Gold contracts\n",
    "cl_df = pd.read_csv('CL.txt') # Crude oil contracts\n",
    "jy_df = pd.read_csv('JY.txt') # JPY/USD contracts\n",
    "bp_df = pd.read_csv('BP.txt') # BP/USD contracts\n",
    "eu_df = pd.read_csv('EU.txt') # EURO/USD contracts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c16cc89",
   "metadata": {},
   "source": [
    "Clean these up a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97079487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_price_columns(data, item):\n",
    "    # Rename columns\n",
    "    data.rename(columns={'Open': f'{item}_open', 'High': f'{item}_high', 'Low': f'{item}_low', 'Close': f'{item}_close', 'Volume': f'{item}_volume'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d300c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_data = {\n",
    "    'ES': es_df,\n",
    "    'NQ': nq_df,\n",
    "    'FV': fv_df,\n",
    "    'TY': ty_df,\n",
    "    'US': us_df, \n",
    "    'GC': gc_df,\n",
    "    'CL': cl_df,\n",
    "    'JY': jy_df,\n",
    "    'BP': bp_df,\n",
    "    'EU': eu_df\n",
    "}\n",
    "\n",
    "for contract, data in contract_data.items():\n",
    "    # Make sure date is datetime and set as index\n",
    "    try:\n",
    "        data['Date'] = pd.to_datetime(data['Date'])\n",
    "        data.set_index(data['Date'], inplace=True)\n",
    "        data.drop(columns=['Date'], inplace=True)\n",
    "    except:\n",
    "        print('Index already fixed.')\n",
    "\n",
    "    # Rename columns\n",
    "    rename_price_columns(data, contract)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a39f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_data['ES']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c9267",
   "metadata": {},
   "source": [
    "References for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78115057",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = ['S&P', 'NASDAQ', 'DJIA']\n",
    "contracts = contract_data.keys() # These are the ones we are training on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaa46d2",
   "metadata": {},
   "source": [
    "Get yahoo finance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['^GSPC', '^IXIC', '^DJI'] # S&P, NASDAQ, DJIA\n",
    "\n",
    "# Download data\n",
    "data_dictionary = get_data(tickers)\n",
    "\n",
    "# Turn to df\n",
    "s_p = pd.DataFrame(data_dictionary['^GSPC'])\n",
    "nasdaq = pd.DataFrame(data_dictionary['^IXIC'])\n",
    "djia = pd.DataFrame(data_dictionary['^DJI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5f7db",
   "metadata": {},
   "source": [
    "Fix structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten df so ticker doesn't span the top\n",
    "s_p = s_p.droplevel(1, axis=1)\n",
    "nasdaq = nasdaq.droplevel(1, axis=1)\n",
    "djia = djia.droplevel(1, axis=1)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "#s_p.drop(columns=['High', 'Low', 'Open'], inplace=True)\n",
    "#nasdaq.drop(columns=['High', 'Low', 'Open'], inplace=True)\n",
    "#djia.drop(columns=['High', 'Low', 'Open'], inplace=True)\n",
    "\n",
    "index_data = {\n",
    "    'S&P': s_p,\n",
    "    'NASDAQ': nasdaq,\n",
    "    'DJIA': djia,\n",
    "}\n",
    "\n",
    "for index, data in index_data.items():\n",
    "    # Rename columns\n",
    "    rename_price_columns(data, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb7ee6e",
   "metadata": {},
   "source": [
    "# Downloading historical data for our factor model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7547dc9e",
   "metadata": {},
   "source": [
    "We are going to model the index as a geometric brownian motion, with the mu factor being a linear regression model with numerous inputs.  \n",
    "\n",
    "## Factor considerations:  \n",
    "### <u>Term structure</u>\n",
    "###### Term spread (10Y-3M)\n",
    "\n",
    "### <u>Credit conditions</u>\n",
    "###### IG spread (BAA-AAA)\n",
    "\n",
    "### <u>Valuation</u>\n",
    "###### Forward E/P - real 10Y\n",
    "###### Dividend yield\n",
    "\n",
    "### <u>Economic</u>\n",
    "###### Fed funds\n",
    "###### Inflation (CPI)\n",
    "###### DXY change (dollar index)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb92a74",
   "metadata": {},
   "source": [
    "### Some of these we can get from yahoo finance:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13407860",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\n",
    "    # Term structure\n",
    "    '^TNX', # 10yr CBOE\n",
    "    '^IRX', # 3m bill (on discount basis, need to convert to yield)\n",
    "\n",
    "    # Economic\n",
    "    'DX-Y.NYB', # Dollar index\n",
    "\n",
    "    # Volatility\n",
    "    '^VIX',\n",
    "\n",
    "    # Gold could be a measure of macro activity?\n",
    "    \"GLD\", # Gold ETF\n",
    "\n",
    "    # Other metals\n",
    "    \"XME\", # SPDR Metals and mining ETF\n",
    "\n",
    "    # Credit\n",
    "    'FLRN', # Bloomberg IG Credit ETF\n",
    "    'JNK', # Bloomberg HY Credit ETF\n",
    "\n",
    "]\n",
    "\n",
    "data_dictionary = get_data(tickers)\n",
    "\n",
    "for ticker, data in data_dictionary.items():\n",
    "    data_dictionary[ticker] = pd.DataFrame(data_dictionary[ticker])\n",
    "    data_dictionary[ticker] = data_dictionary[ticker].droplevel(1, axis=1)\n",
    "    rename_price_columns(data_dictionary[ticker], ticker)\n",
    "\n",
    "ten_yr = data_dictionary['^TNX']\n",
    "three_m = data_dictionary['^IRX']\n",
    "dollar_index = data_dictionary['DX-Y.NYB']\n",
    "vix = data_dictionary['^VIX']\n",
    "\n",
    "# Need to drop volume column here\n",
    "ten_yr.drop(columns=['^TNX_volume'], inplace=True)\n",
    "three_m.drop(columns=['^IRX_volume'], inplace=True)\n",
    "dollar_index.drop(columns=['DX-Y.NYB_volume'], inplace=True)\n",
    "vix.drop(columns=['^VIX_volume'], inplace=True)\n",
    "\n",
    "\n",
    "#gold = data_dictionary['GLD']\n",
    "#metals = data_dictionary['XME']\n",
    "#igb = pd.DataFrame(data_dictionary['FLRN'])\n",
    "#hyb = data_dictionary['JNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14965ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "yf_macros = [vix, ten_yr, three_m, dollar_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9734f02",
   "metadata": {},
   "source": [
    "We should get dividend yield too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "div_data = {}\n",
    "etfs = ['SPY', 'QQQ', 'DIA']\n",
    "\n",
    "for etf in etfs:\n",
    "    ticker = yf.Ticker(etf)\n",
    "    div = ticker.dividends\n",
    "    price = ticker.history(timeframe)['Close']\n",
    "\n",
    "    # Calculate dividend yield\n",
    "    div_12m = div.rolling(window='365D', min_periods=1).sum()\n",
    "    div_12m = div_12m.reindex(price.index, method='ffill')\n",
    "    div_yield = div_12m / price\n",
    "    div_data[etf] = div_yield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fffc352",
   "metadata": {},
   "source": [
    "Fix index for all 3 and rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac371df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "div_data['SPY'].index = pd.to_datetime(div_data['SPY'].index).normalize().tz_localize(None) # Normalize puts date in format we want\n",
    "div_data['QQQ'].index = pd.to_datetime(div_data['QQQ'].index).normalize().tz_localize(None) # Localize (none) makes sure it doesn't add our timezone\n",
    "div_data['DIA'].index = pd.to_datetime(div_data['DIA'].index).normalize().tz_localize(None)\n",
    "\n",
    "div_data['SPY'].name = 'S&P_div'\n",
    "div_data['QQQ'].name = 'NASDAQ_div'\n",
    "div_data['DIA'].name = 'DJIA_div'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52265c2b",
   "metadata": {},
   "source": [
    "### pandas_datareader lets us download fred data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f617557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(1990,1,1) # Start date for download\n",
    "\n",
    "# Macroeconomic data\n",
    "gdp = pdr.DataReader(\"GDP\", \"fred\", start)\n",
    "cpi = pdr.DataReader(\"CPIAUCSL\", \"fred\", start)\n",
    "fedfunds = pdr.DataReader(\"FEDFUNDS\", \"fred\", start)\n",
    "consumer_sentiment = pdr.DataReader(\"UMCSENT\", \"fred\", start)\n",
    "inflation_expectation = pdr.DataReader(\"MICH\", \"fred\", start)\n",
    "\n",
    "# For some reason this download doesn't have the most recent fed funds rate\n",
    "fedfunds = pd.concat([fedfunds['FEDFUNDS'], pd.Series([4.08], index=[datetime(2025,9,17)])])\n",
    "\n",
    "# Credit risk data\n",
    "ig_spread = pdr.DataReader(\"BAMLC0A4CBBB\", \"fred\", start)   # BofA BBB corp minus Treasuries\n",
    "#hy_spread = pdr.DataReader(\"BAMLH0A0HYM2\", \"fred\", start)   # BofA US High Yield spread\n",
    "#baa_spread = pdr.DataReader(\"BAA10Y\", \"fred\", start)        # Moody’s Baa – 10Y Treasury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b0f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_data = [gdp, cpi, fedfunds, ig_spread, consumer_sentiment, inflation_expectation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def72632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last business day <= today\n",
    "last_bday = pd.bdate_range(end=pd.Timestamp.today().normalize().tz_localize(None), periods=1)[0]\n",
    "\n",
    "for i, df in enumerate(fred_data):\n",
    "    s = df.squeeze() # make it a Series\n",
    "    # Build a business-day index from the series start to last_bday\n",
    "    bidx = pd.bdate_range(start=s.index.min(), end=last_bday)\n",
    "    # Reindex to business days and forward-fill\n",
    "    s = s.reindex(bidx, method='ffill')\n",
    "    # Write back as a 1-col DataFrame with a proper name\n",
    "    name = s.name if s.name else f\"series_{i}\"\n",
    "    fred_data[i] = s.to_frame(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ea15b",
   "metadata": {},
   "source": [
    "Let's build a master dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf2055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "data = s_p.join([nasdaq, djia])\n",
    "data = data.join(contract for contract in contract_data.values())\n",
    "data = data.join(macro for macro in yf_macros)\n",
    "data = data.join(div for div in div_data.values())\n",
    "data = data.join([fd for fd in fred_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae193efe",
   "metadata": {},
   "source": [
    "Rename some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'CPIAUCSL': 'CPI', 'BAMLC0A4CBBB': 'credit_spread', 'UMCSENT': 'consumer_sentiment', 'MICH': 'inflation_expectation', 'series_2': 'fed_funds'}, inplace=True)\n",
    "data[data.index.year > 2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00b0cb",
   "metadata": {},
   "source": [
    "# Linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd38fe",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1333d",
   "metadata": {},
   "source": [
    "Let's check for NaNs (there's a ton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3373e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf740444",
   "metadata": {},
   "source": [
    "Check data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75773430",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a8b04",
   "metadata": {},
   "source": [
    "Fill forward for any gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a84a729",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace(0, np.nan)\n",
    "data = data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f31121",
   "metadata": {},
   "source": [
    "Drop others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c572d98d",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1957e49",
   "metadata": {},
   "source": [
    "We need to be careful to not include things such as raw moving averages that will leak volatility information into our drift prediction  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd9c64",
   "metadata": {},
   "source": [
    "Forward adjusting futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb0c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "roll_rules = { # Rollover occurances: (months, days)\n",
    "    'ES': (0,5),\n",
    "    'NQ': (0,5),\n",
    "    'FV': (1,1),\n",
    "    'TY': (1,1),\n",
    "    'US': (1,1),\n",
    "    'GC': (1,2),\n",
    "    'CL': (0,3),\n",
    "    'JY': (0,2),\n",
    "    'BP': (0,2),\n",
    "    'EU': (0,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01fb87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMM_QTR = {3, 6, 9, 12}\n",
    "GOLD_MONTHS = {2, 4, 6, 8, 10, 12}\n",
    "\n",
    "def _third_fridays_quarterly(start, end):\n",
    "    d = pd.date_range(start, end, freq=\"WOM-3FRI\")\n",
    "    return d[d.month.isin(IMM_QTR)]\n",
    "\n",
    "def _bmonth_end_quarterly(start, end):\n",
    "    # Last business day of IMM months\n",
    "    all_month_ends = pd.date_range(start.normalize(), end.normalize(), freq=\"BM\")\n",
    "    return all_month_ends[all_month_ends.month.isin(IMM_QTR)]\n",
    "\n",
    "def _third_wednesdays_quarterly(start, end):\n",
    "    # 3rd Wednesday of IMM months\n",
    "    weds = pd.date_range(start, end, freq=\"WOM-3WED\")\n",
    "    return weds[weds.month.isin(IMM_QTR)]\n",
    "\n",
    "def _gold_expiries(start, end):\n",
    "    # 3rd-last business day of bi-monthly GC months (anchor only)\n",
    "    mends = pd.date_range(start.normalize(), end.normalize(), freq=\"BM\")\n",
    "    mends = mends[mends.month.isin(GOLD_MONTHS)]\n",
    "    # 3rd last BD in that month = BM - 2 BDays\n",
    "    return (mends - pd.offsets.BusinessDay(2))\n",
    "\n",
    "def _cl_expiries(start, end):\n",
    "    # Crude \"expiration\" day (last trade day): 3 business days before the 25th of the PRIOR month\n",
    "    # We'll generate it for EVERY contract month in range.\n",
    "    cl_expiries = []\n",
    "    # iterate month starts to month ends\n",
    "    month_starts = pd.date_range(start.normalize(), end.normalize(), freq=\"MS\")\n",
    "    for ms in month_starts:\n",
    "        # prior month 25th\n",
    "        prior_month_25th = (ms - pd.offsets.MonthBegin(1)) + pd.offsets.Day(24)  # 25th of prior month\n",
    "        # adjust 25th to previous business day if needed, then subtract 3 business days\n",
    "        # (approximation: we ignore exchange holidays and use business days = weekdays)\n",
    "        # Find the previous business day <= prior_month_25th\n",
    "        prev_bd = prior_month_25th\n",
    "        # step back to weekday if falls on weekend\n",
    "        while prev_bd.weekday() >= 5:\n",
    "            prev_bd -= pd.Timedelta(days=1)\n",
    "        ltd = prev_bd - pd.offsets.BusinessDay(3)\n",
    "        cl_expiries.append(ltd.normalize())\n",
    "    return pd.DatetimeIndex(sorted(set(cl_expiries)))\n",
    "\n",
    "def _expiry_calendar(symbol, start, end):\n",
    "    s = symbol.upper()\n",
    "    if s in {\"ES\", \"NQ\"}:\n",
    "        return _third_fridays_quarterly(start, end)\n",
    "    if s in {\"FV\", \"TY\", \"US\"}:\n",
    "        return _bmonth_end_quarterly(start, end)\n",
    "    if s == \"GC\":\n",
    "        return _gold_expiries(start, end)\n",
    "    if s == \"CL\":\n",
    "        return _cl_expiries(start, end)\n",
    "    if s in {\"JY\", \"BP\", \"EU\"}:\n",
    "        return _third_wednesdays_quarterly(start, end)\n",
    "    # Fallback: quarterly 3rd Fridays (reasonable default)\n",
    "    return _third_fridays_quarterly(start, end)\n",
    "\n",
    "# -----------------------\n",
    "# main: compute rollover flags, episodes, and DTR\n",
    "# -----------------------\n",
    "def get_rollover_data(data: pd.DataFrame, item: str, roll_rules: dict):\n",
    "    months_off, days_off = roll_rules[item]  # (months, days)\n",
    "\n",
    "    # We add a buffer so we can see the next expiry at the end of the dataset\n",
    "    start = pd.Timestamp(data.index.min()).normalize()\n",
    "    end = (pd.Timestamp(data.index.max()).normalize() + pd.DateOffset(months=4))\n",
    "\n",
    "    expiries = _expiry_calendar(item, start, end)\n",
    "\n",
    "    # Compute roll dates per rule\n",
    "    if months_off == 0:\n",
    "        roll_dates = expiries - pd.offsets.BusinessDay(days_off)\n",
    "    else:\n",
    "        # For each expiry, go back N months to that month's LAST business day, then step back 'days_off'\n",
    "        anchor_month_ends = (expiries - pd.offsets.MonthBegin(months_off))  # first of prior month(s)\n",
    "        # move to that month's last business day\n",
    "        anchor_bm_end = pd.to_datetime(anchor_month_ends).to_period('M').to_timestamp('M')\n",
    "        # ensure it's a business day (BM gives last business day; if not, use BMonthEnd)\n",
    "        anchor_bm_end = pd.DatetimeIndex(anchor_bm_end)  # calendar month-end\n",
    "        # Convert to last BUSINESS day of that month\n",
    "        anchor_last_bd = pd.DatetimeIndex([pd.Timestamp(d).to_period('M').to_timestamp('M') for d in anchor_bm_end])\n",
    "        # Replace with BusinessMonthEnd for precision\n",
    "        anchor_last_bd = pd.date_range(anchor_last_bd.min(), anchor_last_bd.max(), freq=\"BM\").intersection(anchor_last_bd)\n",
    "        # The above intersection collapses; safer approach:\n",
    "        # Recompute last BD robustly per expiry:\n",
    "        last_bds = []\n",
    "        for e in expiries:\n",
    "            # month = e - months_off\n",
    "            mstart = (e - pd.offsets.MonthBegin(months_off))\n",
    "            # last business day of that month:\n",
    "            lbd = (mstart + pd.offsets.BMonthEnd(0))\n",
    "            last_bds.append(lbd)\n",
    "        anchor_last_bd = pd.DatetimeIndex(last_bds)\n",
    "\n",
    "        roll_dates = anchor_last_bd - pd.offsets.BusinessDay(days_off)\n",
    "\n",
    "    roll_dates = pd.DatetimeIndex(sorted(set(pd.to_datetime(roll_dates))))\n",
    "    # Keep only roll dates that appear in our data index (typical)\n",
    "    roll_dates = roll_dates.intersection(pd.DatetimeIndex(data.index))\n",
    "\n",
    "    # Flags and episodes\n",
    "    rflag = f\"{item}_Rollover\"\n",
    "    eph = f\"{item}_Episode\"\n",
    "    data[rflag] = 0\n",
    "    if len(roll_dates):\n",
    "        data.loc[roll_dates, rflag] = 1\n",
    "    data[eph] = data[rflag].cumsum()\n",
    "\n",
    "    # Days until next rollover (trading days)\n",
    "    # Forward-fill the next roll date and count business days\n",
    "    next_roll = (\n",
    "        pd.Series(roll_dates, index=roll_dates)\n",
    "        .reindex(data.index, method=\"bfill\")\n",
    "        .to_numpy()\n",
    "    )\n",
    "    days = np.full(len(data), np.nan)\n",
    "    mask = ~pd.isna(next_roll)\n",
    "    # convert to daily (no time) for busday_count\n",
    "    left = data.index.to_numpy(dtype=\"datetime64[D]\")\n",
    "    right = pd.to_datetime(next_roll).to_numpy(dtype=\"datetime64[D]\")\n",
    "    days[mask] = np.busday_count(left[mask], right[mask])  # excludes the right endpoint\n",
    "    data[f\"{item}_Days_Until_Rollover\"] = days\n",
    "\n",
    "# -----------------------\n",
    "# forward adjustment on roll days\n",
    "# -----------------------\n",
    "def rollover_forward_adjustment(data: pd.DataFrame, contract: str):\n",
    "    rflag = f\"{contract}_Rollover\"\n",
    "    close = f\"{contract}_close\"\n",
    "    # Jump only on the roll day = today's close minus yesterday's close\n",
    "    jump = data[rflag].fillna(0).astype(int) * (data[close] - data[close].shift(1).fillna(0.0))\n",
    "    cum_adj = jump.cumsum()\n",
    "    for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "        c = f\"{contract}_{col}\"\n",
    "        if c in data.columns:\n",
    "            data[c] = data[c] - cum_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f756f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for contract in contracts:\n",
    "    # Set info about expirations\n",
    "    get_rollover_data(data, contract, roll_rules)\n",
    "\n",
    "    # Forward adjust rollover\n",
    "    rollover_forward_adjustment(data, contract)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aef311",
   "metadata": {},
   "source": [
    "Function definitions to help out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_mean(data, window):\n",
    "    return data.rolling(window, min_periods=window).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e655271",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c0096",
   "metadata": {},
   "source": [
    "Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(data, items, cfg=None, dropna=True):\n",
    "    if cfg is None:\n",
    "        cfg = {\n",
    "            \"rsi_n\": 14,\n",
    "            \"macd_fast\": 12, \"macd_slow\": 26, \"macd_signal\": 9,\n",
    "            \"bb_n\": 20, \"bb_k\": 2.0,\n",
    "            \"atr_n\": 14\n",
    "        }\n",
    "    out = data.copy()\n",
    "\n",
    "    for item in items:\n",
    "        o = out[f\"{item}_open\"].astype(float)\n",
    "        h = out[f\"{item}_high\"].astype(float)\n",
    "        l = out[f\"{item}_low\"].astype(float)\n",
    "        c = out[f\"{item}_close\"].astype(float)\n",
    "        v = out[f\"{item}_volume\"].astype(float)\n",
    "\n",
    "        # --- 1) RSI (Wilder smoothing) ---\n",
    "        n = cfg[\"rsi_n\"]\n",
    "        delta = c.diff()\n",
    "        gain = delta.clip(lower=0.0)\n",
    "        loss = (-delta).clip(lower=0.0)\n",
    "        avg_gain = gain.ewm(alpha=1/n, adjust=False, min_periods=n).mean()\n",
    "        avg_loss = loss.ewm(alpha=1/n, adjust=False, min_periods=n).mean()\n",
    "        rs = avg_gain / (avg_loss.replace(0, np.nan))\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        out[f\"{item}_rsi_{n}\"] = rsi\n",
    "\n",
    "        # --- 2) MACD (EMA fast/slow, signal, hist) ---\n",
    "        ema_fast = c.ewm(span=cfg[\"macd_fast\"], adjust=False).mean()\n",
    "        ema_slow = c.ewm(span=cfg[\"macd_slow\"], adjust=False).mean()\n",
    "        macd = ema_fast - ema_slow\n",
    "        macd_signal = macd.ewm(span=cfg[\"macd_signal\"], adjust=False).mean()\n",
    "        macd_hist = macd - macd_signal\n",
    "        out[f\"{item}_macd\"] = macd\n",
    "        out[f\"{item}_macd_signal\"] = macd_signal\n",
    "        out[f\"{item}_macd_hist\"] = macd_hist\n",
    "\n",
    "        # --- 3) Bollinger Bands (+ %B, Bandwidth) ---\n",
    "        bb_n = cfg[\"bb_n\"]; bb_k = cfg[\"bb_k\"]\n",
    "        mavg = c.rolling(bb_n, min_periods=bb_n).mean()\n",
    "        mstd = c.rolling(bb_n, min_periods=bb_n).std(ddof=0)\n",
    "        upper = mavg + bb_k * mstd\n",
    "        lower = mavg - bb_k * mstd\n",
    "        pct_b = (c - lower) / (upper - lower)\n",
    "        bandwidth = (upper - lower) / mavg\n",
    "        out[f\"{item}_bb_mean_{bb_n}\"] = mavg\n",
    "        out[f\"{item}_bb_upper_{bb_n}_{int(bb_k)}\"] = upper\n",
    "        out[f\"{item}_bb_lower_{bb_n}_{int(bb_k)}\"] = lower\n",
    "        out[f\"{item}_bb_percent_b\"] = pct_b\n",
    "        out[f\"{item}_bb_bandwidth\"] = bandwidth\n",
    "\n",
    "        # --- 4) ATR (Wilder, uses True Range) ---\n",
    "        prev_c = c.shift(1)\n",
    "        tr = np.maximum(h - l, np.maximum((h - prev_c).abs(), (l - prev_c).abs()))\n",
    "        atr = tr.ewm(alpha=1/cfg[\"atr_n\"], adjust=False, min_periods=cfg[\"atr_n\"]).mean()\n",
    "        out[f\"{item}_atr_{cfg['atr_n']}\"] = atr\n",
    "\n",
    "        # --- 5) OBV (On-Balance Volume) ---\n",
    "        sign = np.sign(c.diff().fillna(0.0))\n",
    "        obv = (sign * v).fillna(0.0).cumsum()\n",
    "        out[f\"{item}_obv\"] = obv\n",
    "\n",
    "    if dropna:\n",
    "        out = out.dropna()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f44c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_technical_indicators(data, contracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac8b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_price(data, items):\n",
    "    data = data.copy()\n",
    "    \n",
    "    ohlcv = ['open', 'high', 'low', 'close', 'volume']\n",
    "    for item in items:\n",
    "        for key in ohlcv:\n",
    "            data[f'{item}_log_{key}'] = np.log(data[f'{item}_{key}'])\n",
    "            data.drop(columns=[f'{item}_{key}'], inplace=True) # Drop originals\n",
    "\n",
    "        data.dropna(inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_returns(data, items):\n",
    "    data = data.copy()\n",
    "    \n",
    "    ohlcv = ['open', 'high', 'low', 'close', 'volume']\n",
    "    for item in items:\n",
    "        for key in ohlcv:\n",
    "            data[f'{item}_log_{key}_ret'] = data[f'{item}_log_{key}'].diff()\n",
    "\n",
    "        data[f'{item}_log_open_close_ret'] = data[f'{item}_log_close'] - data[f'{item}_log_open']\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get log transforms of price data\n",
    "log_price_keys = indexes + list(contract_data.keys()) #+ ['GLD', 'XME', 'JNK']\n",
    "\n",
    "# Log prices\n",
    "data = get_log_price(data, log_price_keys)\n",
    "\n",
    "# Log returns\n",
    "data = get_log_returns(data, log_price_keys)\n",
    "\n",
    "# Log for other price data (GDP)\n",
    "data['log_GDP'] = np.log(data['GDP'])\n",
    "data.drop(columns=['GDP'], inplace=True)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e8b758",
   "metadata": {},
   "source": [
    "Price/Volume Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524eaa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price_volume_features(data, items):\n",
    "    data = data.copy()\n",
    "    \n",
    "    for item in items:\n",
    "        # Price-based\n",
    "        data[f'{item}_mom_1w'] = data[f'{item}_log_close'].diff(5) # Total price change / momentum indicator\n",
    "        data[f'{item}_mom_3m'] = data[f'{item}_log_close'].diff(63)\n",
    "        data[f'{item}_1m_rolling_price'] = rolling_mean(data[f'{item}_log_close'], 21)\n",
    "        data[f'{item}_3m_rolling_price'] = rolling_mean(data[f'{item}_log_close'], 63)\n",
    "        data[f'{item}_6m_rolling_price'] = rolling_mean(data[f'{item}_log_close'], 126)\n",
    "        data[f'{item}_trend_speed_price'] = data[f'{item}_3m_rolling_price'].diff(5)  # How fast the 3m trend is changing on a weekly basis\n",
    "        data[f'{item}_trend_dist_price'] = data[f'{item}_log_close'] - data[f'{item}_3m_rolling_price']\n",
    "\n",
    "        # Volume-based (essentially the same as price for now)\n",
    "        data[f'{item}_vlm_1w'] = data[f'{item}_log_volume'].diff(5) # Total volume change / momentum indicator\n",
    "        data[f'{item}_vlm_1m'] = data[f'{item}_log_volume'].diff(21)\n",
    "        data[f'{item}_vlm_3m'] = data[f'{item}_log_volume'].diff(63)\n",
    "        data[f'{item}_3m_rolling_volume'] = rolling_mean(data[f'{item}_log_volume'], 63)\n",
    "        data[f'{item}_trend_speed_volume'] = data[f'{item}_3m_rolling_volume'].diff(5)  # How fast the 3m trend is changing on a weekly basis\n",
    "        data[f'{item}_trend_dist_volume'] = data[f'{item}_log_volume'] - data[f'{item}_3m_rolling_volume']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a0279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_price_volume_features(data, log_price_keys)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978bc76f",
   "metadata": {},
   "source": [
    "Macro features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a9458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['log_GDP_ret'] = data['log_GDP'].diff()\n",
    "data['log_GDP_ret_1y'] = data['log_GDP'].diff(252)\n",
    "data['inflation_expectation_ret'] = data['inflation_expectation'].pct_change()\n",
    "data['consumer_sentiment_ret'] = data['consumer_sentiment'].pct_change()\n",
    "data['cpi_ret'] = data['CPI'].pct_change()\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4464ef",
   "metadata": {},
   "source": [
    "Spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08509e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Spread_NQ_ES_ret'] = data['ES_log_close_ret'] - data['NQ_log_close_ret']\n",
    "data['Spread_NQ_ES_volume'] = data['ES_3m_rolling_volume'] - data['NQ_3m_rolling_volume']\n",
    "data['Spread_ES_GC_ret'] = data['ES_log_close_ret'] - data['GC_log_close_ret']\n",
    "data['Spread_ES_GC_volume'] = data['ES_3m_rolling_volume'] - data['GC_3m_rolling_volume']\n",
    "data['Spread_FV_US_ret'] = data['FV_log_close_ret'] - data['US_log_close_ret']\n",
    "data['Spread_FV_US_volume'] = data['FV_3m_rolling_volume'] - data['US_3m_rolling_volume']\n",
    "data['Spread_TY_US_ret'] = data['TY_log_close_ret'] - data['US_log_close_ret']\n",
    "data['Spread_TY_US_volume'] = data['TY_3m_rolling_volume'] - data['US_3m_rolling_volume']\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71374fec",
   "metadata": {},
   "source": [
    "Calendar dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    month_dummies = pd.get_dummies(data.index.month, prefix=\"month\", dtype=float)\n",
    "    month_dummies.set_index(data.index, inplace=True)\n",
    "    day_dummies = pd.get_dummies(data.index.dayofweek, prefix=\"weekday\", dtype=float)\n",
    "    day_dummies.set_index(data.index, inplace=True)\n",
    "    presidency_year_dummies = pd.get_dummies(data.index.year % 4, prefix='pres_year', dtype='float')\n",
    "    presidency_year_dummies.set_index(data.index, inplace=True)\n",
    "    \n",
    "    data = data.join([month_dummies, day_dummies, presidency_year_dummies])\n",
    "except:\n",
    "    print('Error: probably already created dummies.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6ccd9c",
   "metadata": {},
   "source": [
    "Volatility / Price Features (From ChatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32fff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_vix_vol = data['^VIX_close'] / (100 * np.sqrt(252))\n",
    "data['vix_implied_var'] = np.square(daily_vix_vol)\n",
    "    # Measures the percentage change in the ^VIX over 1 week and 1 month\n",
    "data['vix_mom_1w'] = data['^VIX_close'].pct_change(periods=5)\n",
    "data['vix_mom_1m'] = data['^VIX_close'].pct_change(periods=21)\n",
    "\n",
    "data[f'vix_trend_1m'] = rolling_mean(data[f'^VIX_close'], 21)\n",
    "data[f'vix_trend_3m'] = rolling_mean(data[f'^VIX_close'], 63)\n",
    "data[f'vix_trend_6m'] = rolling_mean(data[f'^VIX_close'], 126)\n",
    "\n",
    "# Calculate how far the current VIX is from its trend (as a percentage)\n",
    "data['vix_trend_dist'] = (data['^VIX_close'] - data['vix_trend_3m']) / data['vix_trend_3m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad2524",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-12\n",
    "\n",
    "# If you already have a rolling_mean helper, keep it. Otherwise:\n",
    "def rolling_mean(s, w):\n",
    "    return s.rolling(w, min_periods=max(2, int(w*0.6))).mean()\n",
    "\n",
    "def ewma_vol(r, lam=0.94):\n",
    "    # EWMA variance per RiskMetrics: sigma_t^2 = (1-lam)*r_{t-1}^2 + lam*sigma_{t-1}^2\n",
    "    # Use pandas ewm for convenience\n",
    "    return r.pow(2).ewm(alpha=(1-lam), adjust=False).mean().clip(lower=0)\n",
    "\n",
    "def rolling_autocorr(x, lag=1, window=63):\n",
    "    # Rolling autocorrelation of x at a given lag\n",
    "    # For stability, require at least ~60% of window\n",
    "    minp = max(10, int(window*0.6))\n",
    "    x0 = x\n",
    "    x1 = x.shift(lag)\n",
    "    return x0.rolling(window, min_periods=minp).corr(x1)\n",
    "\n",
    "def realized_quarticity(r, window=63):\n",
    "    # 3-month robust quarticity proxy (if daily): sum r^4 * (n / 3) approximation\n",
    "    # Here we simply provide rolling sum of r^4; scaling optional depending on use\n",
    "    minp = max(10, int(window*0.6))\n",
    "    return (r.pow(4)).rolling(window, min_periods=minp).sum()\n",
    "\n",
    "def build_vol_features(data, prefix, day_w=21, qtr_w=63, yr_w=252, ewma_lambda=0.94, target='close'):\n",
    "    \"\"\"\n",
    "    Expects:\n",
    "      data[f'{prefix}_log_price'] (daily log price)\n",
    "      data[f'{prefix}_log_volume'] (daily log volume)\n",
    "    Produces a suite of volatility-centric features for that prefix.\n",
    "    \"\"\"\n",
    "    \n",
    "    lp = data[f\"{prefix}_log_{target}\"]\n",
    "    lv = data.get(f\"{prefix}_log_volume\", None)\n",
    "\n",
    "    # Daily log return\n",
    "    r = lp.diff()  # already log-price, so diff = log-return\n",
    "\n",
    "    # --- Realized volatility proxies ---\n",
    "    data[f\"{prefix}_rv_1m\"]  = r.rolling(day_w, min_periods=int(day_w*0.6)).var().clip(lower=0)          # variance\n",
    "    data[f\"{prefix}_rv_3m\"]  = r.rolling(qtr_w, min_periods=int(qtr_w*0.6)).var().clip(lower=0)\n",
    "    data[f\"{prefix}_rv_1y\"]  = r.rolling(yr_w,  min_periods=int(yr_w*0.6)).var().clip(lower=0)\n",
    "    data[f\"{prefix}_absrv_1m\"] = r.abs().rolling(day_w, min_periods=int(day_w*0.6)).mean()               # mean |r|\n",
    "    data[f\"{prefix}_absrv_3m\"] = r.abs().rolling(qtr_w, min_periods=int(qtr_w*0.6)).mean()\n",
    "\n",
    "    # EWMA volatility (RiskMetrics-style)\n",
    "    data[f\"{prefix}_ewma_var\"] = ewma_vol(r, lam=ewma_lambda)\n",
    "    data[f\"{prefix}_ewma_vol\"] = np.sqrt(data[f\"{prefix}_ewma_var\"])\n",
    "\n",
    "    # Volatility-of-volatility (how fast vol is changing)\n",
    "    data[f\"{prefix}_vol_speed_1w\"] = data[f\"{prefix}_rv_3m\"].diff(5)                                      # weekly change in 3m var\n",
    "    data[f\"{prefix}_vol_mom_1m\"]   = data[f\"{prefix}_rv_3m\"] - data[f\"{prefix}_rv_1m\"]                    # 3m vs 1m\n",
    "    data[f\"{prefix}_vol_mom_1y\"]   = data[f\"{prefix}_rv_1y\"] - data[f\"{prefix}_rv_3m\"]\n",
    "\n",
    "    # Volatility clustering proxies\n",
    "    data[f\"{prefix}_acf_sqret_lag1_3m\"] = rolling_autocorr(r.pow(2), lag=1, window=qtr_w)\n",
    "    data[f\"{prefix}_acf_absret_lag1_3m\"] = rolling_autocorr(r.abs(), lag=1, window=qtr_w)\n",
    "\n",
    "    # Leverage effect proxy (contemporaneous corr between return and next day's vol)\n",
    "    # Negative returns often precede higher vol; we proxy with corr(r_t, |r|_{t+1})\n",
    "    data[f\"{prefix}_lev_proxy_3m\"] = r.shift(1).rolling(qtr_w, min_periods=int(qtr_w*0.6)).corr(r.abs()).shift(1)\n",
    "\n",
    "    # Quarticity (heavy tails proxy)\n",
    "    data[f\"{prefix}_quarticity_3m\"] = realized_quarticity(r, window=qtr_w)\n",
    "\n",
    "    # Ratio features (normalized vol levels)\n",
    "    data[f\"{prefix}_vol_ratio_1m_3m\"] = (data[f\"{prefix}_rv_1m\"] / (data[f\"{prefix}_rv_3m\"] + EPS))\n",
    "    data[f\"{prefix}_vol_ratio_3m_1y\"] = (data[f\"{prefix}_rv_3m\"] / (data[f\"{prefix}_rv_1y\"] + EPS))\n",
    "    data[f\"{prefix}_ewma_over_3m\"]    = (data[f\"{prefix}_ewma_var\"] / (data[f\"{prefix}_rv_3m\"] + EPS))\n",
    "\n",
    "    # Price–volatility relation: distance from trend as a stress proxy\n",
    "    data[f\"{prefix}_price_trend_3m\"]  = rolling_mean(lp, qtr_w)\n",
    "    data[f\"{prefix}_price_trend_dist\"] = lp - data[f\"{prefix}_price_trend_3m\"]\n",
    "    # Volatility when far below trend often spikes; include interaction\n",
    "    data[f\"{prefix}_vol_x_trend_dist\"] = data[f\"{prefix}_rv_1m\"] * data[f\"{prefix}_price_trend_dist\"]\n",
    "\n",
    "    # Volume–volatility links (if volume available)\n",
    "    if lv is not None:\n",
    "        dv = lv.diff()  # log-volume change\n",
    "        data[f\"{prefix}_vlm_var_1m\"] = dv.rolling(day_w, min_periods=int(day_w*0.6)).var().clip(lower=0)\n",
    "        data[f\"{prefix}_vlm_var_3m\"] = dv.rolling(qtr_w, min_periods=int(qtr_w*0.6)).var().clip(lower=0)\n",
    "        # Corr between |r| and volume changes (vol–volume clustering)\n",
    "        data[f\"{prefix}_corr_absr_dlv_3m\"] = r.abs().rolling(qtr_w, min_periods=int(qtr_w*0.6)).corr(dv)\n",
    "        # Volume surprise proxy: current vs 3m trend\n",
    "        data[f\"{prefix}_vlm_trend_3m\"] = rolling_mean(lv, qtr_w)\n",
    "        data[f\"{prefix}_vlm_trend_dist\"] = lv - data[f\"{prefix}_vlm_trend_3m\"]\n",
    "        # Vol reacts to volume surprises\n",
    "        data[f\"{prefix}_vol_x_vlm_surprise\"] = data[f\"{prefix}_rv_1m\"] * data[f\"{prefix}_vlm_trend_dist\"]\n",
    "\n",
    "    # 2. Calculate the spread against a realized variance feature you already have\n",
    "    # We'll use a 1-month realized variance (rv_1m) for the S&P 500 as an example.\n",
    "    # Ensure you have 'S&P_rv_1m' or a similar column from your previous code.\n",
    "    data['{prefix}_vrp_spread'] = data['vix_implied_var'] - data[f'{prefix}_rv_1m']\n",
    "\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "# ---- Apply to all price series ---- #### both close and close_ret\n",
    "for item in log_price_keys:\n",
    "    data = build_vol_features(data, index, day_w=21, qtr_w=63, yr_w=252, ewma_lambda=0.94)\n",
    "    data = build_vol_features(data, item, day_w=21, qtr_w=63, yr_w=252, ewma_lambda=0.94, target='close_ret')\n",
    "\n",
    "# ---- Cross-index spillover features (optional but useful) ----\n",
    "# Differences/spreads in contemporaneous vol across indices capture contagion/regime moves\n",
    "data[\"SPX_minus_NDX_vol_1m\"] = data[\"S&P_rv_1m\"] - data[\"NASDAQ_rv_1m\"]\n",
    "data[\"SPX_minus_DJIA_vol_1m\"] = data[\"S&P_rv_1m\"] - data[\"DJIA_rv_1m\"]\n",
    "data[\"NDX_minus_DJIA_vol_1m\"] = data[\"NASDAQ_rv_1m\"] - data[\"DJIA_rv_1m\"]\n",
    "data['ES_minus_NQ_vol_1m'] = data[\"ES_rv_1m\"] - data[\"NQ_rv_1m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f9a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e57460",
   "metadata": {},
   "source": [
    "### Split data\n",
    "\n",
    "Training/testing 80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b1b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4e2899",
   "metadata": {},
   "source": [
    "### Set target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cced463",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_target = 'next_ret'\n",
    "drift = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94577183",
   "metadata": {},
   "outputs": [],
   "source": [
    "for contract in contracts:\n",
    "    drift[f'{contract}_{linear_model_target}'] = drift[f'{contract}_log_close_ret'].shift(-1)\n",
    "drift.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fdde3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split=0.8, embargo=2):\n",
    "    cutoff = math.floor(len(data)*split)\n",
    "    training_data = data.iloc[:(cutoff - embargo)]\n",
    "    testing_data = data.iloc[(cutoff+embargo):]\n",
    "    return training_data.copy(), testing_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbaea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_training_data, drift_testing_data = split_data(drift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c33903",
   "metadata": {},
   "source": [
    "### Determine feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459bd4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa8ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_targets_features(target, training_data, contracts=contracts, additional_exclusions=[], additional_unscalables=[]):\n",
    "    # Make sure we only fit on training_data and explanatory variables\n",
    "    targets = [f'{contract}_{target}' for contract in contracts]\n",
    "    month_dummies = [f'month_{month}' for month in range(1,13)]\n",
    "    day_dummies = [f'weekday_{day}' for day in range(5)]\n",
    "    presidency_year_dummies = day_dummies = [f'pres_year_{year}' for year in range(4)]\n",
    "    rollover_dummy = [f'{contract}_Rollover' for contract in contracts]\n",
    "    dummies = month_dummies + day_dummies + presidency_year_dummies + rollover_dummy\n",
    "    additional_exclusions.extend([f'{contract}_Episode'])\n",
    "\n",
    "\n",
    "    columns_to_exclude = [] + additional_exclusions\n",
    "\n",
    "    unscalable_features = dummies + additional_unscalables\n",
    "    scalable_features = [column for column in training_data.columns if column not in targets and column not in unscalable_features and column not in columns_to_exclude]\n",
    "    all_features = unscalable_features + scalable_features\n",
    "\n",
    "    return targets, all_features, unscalable_features, scalable_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_targets, drift_all_features, drift_unscale_features, drift_scale_features = set_targets_features(linear_model_target, drift_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cae89f",
   "metadata": {},
   "source": [
    "Let's try using exp weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4350b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_feature_importance(training_data, testing_data, features, target, contracts=contracts, mode='permutation', permutation_threshold = 0, selector_threshold='median'):\n",
    "    pruned_features = {}\n",
    "\n",
    "    for contract in contracts:\n",
    "        y_train = training_data[f'{contract}_{target}']\n",
    "        y_test = testing_data[f'{contract}_{target}']\n",
    "        X_train = training_data[features]\n",
    "        X_test = testing_data[features]\n",
    "\n",
    "        # Fit random forest\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators = 500,\n",
    "            max_depth=None,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        if mode == 'permutation':\n",
    "            result = permutation_importance(\n",
    "                rf, X_test, y_test, # Make sure on test data\n",
    "                n_repeats = 30,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "\n",
    "            importances = pd.Series(result.importances_mean, index=X_train.columns).sort_values(ascending=False)\n",
    "            selected_features = importances[importances > permutation_threshold].index.tolist()\n",
    "\n",
    "            pruned_features[contract] = {\n",
    "                'train_reduced': X_train[selected_features],\n",
    "                'test_reduced': X_test[selected_features],\n",
    "                'selected_features': selected_features\n",
    "            }\n",
    "\n",
    "\n",
    "        if mode == 'random_forest':\n",
    "            selector = SelectFromModel(rf, threshold=selector_threshold, prefit=True)\n",
    "            selected_features = training_data[features].columns[selector.get_support()]\n",
    "\n",
    "            pruned_features[contract] = {\n",
    "                'train_reduced': selector.transform(X_train),\n",
    "                'test_reduced': selector.transform(X_test),\n",
    "                'selected_features': selected_features\n",
    "            }\n",
    "\n",
    "    return pruned_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab256b",
   "metadata": {},
   "source": [
    "Pickling for saving compute expensive dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8807a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, gzip, bz2, lzma\n",
    "from pathlib import Path\n",
    "from typing import Callable, Any, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeeed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _opener(path: Path, mode: str):\n",
    "    ext = path.suffix.lower()\n",
    "    if ext in ('.pkl', '.pickle'):\n",
    "        return path.open(mode)\n",
    "    if ext == '.gz':\n",
    "        return gzip.open(path, mode)\n",
    "    if ext == '.bz2':\n",
    "        return bz2.open(path, mode)\n",
    "    if ext in ('.xz', '.lzma'):\n",
    "        return lzma.open(path, mode)\n",
    "    raise ValueError(f\"Unsupported extension: {ext}\")\n",
    "\n",
    "def get_or_build_results(\n",
    "    path: str,\n",
    "    build_fn: Callable[..., dict],\n",
    "    *args, **kwargs\n",
    ") -> Tuple[dict, str]:\n",
    "\n",
    "    p = Path(path)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Try load\n",
    "    try:\n",
    "        with _opener(p, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "\n",
    "        try:\n",
    "            # 2) Build\n",
    "            results = build_fn(*args, **kwargs)\n",
    "\n",
    "            # 3) Save\n",
    "            with _opener(p, 'wb') as f:\n",
    "                pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print(f'Exception occured: {e}')\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest for now because it's faster\n",
    "pruned_features = get_or_build_results('model_data/drift_features_rf.pkl.gz', determine_feature_importance, drift_training_data, drift_testing_data, drift_all_features, 'next_ret', mode='random_forest')\n",
    "\n",
    "for contract in contracts:\n",
    "    print(pruned_features[contract]['selected_features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f599689",
   "metadata": {},
   "source": [
    "### Normalize inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4039bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(training_data, testing_data, features):\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(training_data[features]) # Fitting on training data\n",
    "\n",
    "    train_scaled = training_data.copy()\n",
    "    test_scaled = testing_data.copy()\n",
    "\n",
    "    # Scale\n",
    "    train_scaled[features] = scaler.transform(training_data[features])\n",
    "    test_scaled[features] = scaler.transform(testing_data[features])\n",
    "\n",
    "    # Save info on standardization for later\n",
    "    #scaler_mu = pd.Series(scaler.mean_, index=features)\n",
    "    #scaler_std = pd.Series(scaler.scale_, index=features)\n",
    "    return train_scaled, test_scaled #, scaler_mu, scaler_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781255bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_train_scaled, drift_test_scaled = scale_features(drift_training_data, drift_testing_data, drift_scale_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678fe365",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df85bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from mlxtend.evaluate import GroupTimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ac9fb",
   "metadata": {},
   "source": [
    "We are going to test with multiple different linear models to account for collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which models we want to use\n",
    "linear_models = ['ridge', 'lasso', 'enet', 'pca'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8090b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get print results from the models\n",
    "def eval_and_report(y_true, y_pred, model_name):\n",
    "    print(f\"{model_name:18s} | R^2: {r2_score(y_true, y_pred):.4f} | RMSE: {root_mean_squared_error(y_true, y_pred):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf60305",
   "metadata": {},
   "source": [
    "Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train_scaled, test_scaled, target, contracts=contracts, pruned_features=pruned_features, models=linear_models):\n",
    "    # Suppress all convergence warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for contract in contracts:\n",
    "        results[contract] = {}\n",
    "        features = pruned_features[contract]['selected_features']\n",
    "\n",
    "        print(f\"\\n=== Target: {f'{contract}_{target}'} ===\")\n",
    "        X_train = train_scaled[features].copy()\n",
    "        y_train = train_scaled[f'{contract}_{target}'].copy()\n",
    "        X_test = test_scaled[features].copy()\n",
    "        y_test = test_scaled[f'{contract}_{target}'].copy()\n",
    "\n",
    "        # Create time series splits that isolate each contract\n",
    "        ep_train = train_scaled[f'{contract}_Episode']\n",
    "        gts = GroupTimeSeriesSplit(n_splits = ep_train.nunique()-2, test_size=1, window_type='expanding', gap_size=1)\n",
    "        \n",
    "        tscv = list(gts.split(\n",
    "            X = X_train,\n",
    "            y = y_train,\n",
    "            groups=ep_train.values\n",
    "        ))\n",
    "\n",
    "        # 1. Ordinary Least Squares (OLS)\n",
    "        if 'ols' in models:\n",
    "            ols = LinearRegression()\n",
    "            ols.fit(X_train, y_train)\n",
    "            yhat_ols = ols.predict(X_test)\n",
    "            eval_and_report(y_test, yhat_ols, \"OLS\")\n",
    "\n",
    "            # Print top coefficients\n",
    "            ols_coef = pd.Series(ols.coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "            print(\"Top OLS coeffs:\\n\", ols_coef.head(10))\n",
    "\n",
    "            results[contract].update({\n",
    "                \"ols_model\": ols,\n",
    "                \"ols_coefs\": ols_coef,\n",
    "                \"train_data_ols\": pd.Series(ols.predict(X_train), index=y_train.index, name=f\"ols_train\"),\n",
    "                \"yhat_ols\": pd.Series(yhat_ols, index=y_test.index, name=f\"{contract}_{target}_ols_pred\"),\n",
    "            })\n",
    "\n",
    "        # 2. Ridge with CV over alphas (time-series CV)\n",
    "        if 'ridge' in models:\n",
    "            alphas = np.logspace(-4, 6, 100)\n",
    "\n",
    "            ridge = RidgeCV(alphas=alphas, cv=tscv, fit_intercept=True)\n",
    "            ridge.fit(X_train, y_train)\n",
    "            yhat_ridge = ridge.predict(X_test)\n",
    "            eval_and_report(y_test, yhat_ridge, f\"Ridge (alpha={ridge.alpha_:.4g})\")\n",
    "\n",
    "            # Print top coefficients\n",
    "            ridge_coef = pd.Series(ridge.coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "            print(\"Top Ridge coeffs:\\n\", ridge_coef.head(10))\n",
    "            \n",
    "            results[contract].update({\n",
    "                \"ridge_model\": ridge,\n",
    "                \"ridge_coefs\": ridge_coef,\n",
    "                \"train_data_ridge\": pd.Series(ridge.predict(X_train), index=y_train.index, name=f\"ridge_train\"),\n",
    "                \"yhat_ridge\": pd.Series(yhat_ridge, index=y_test.index, name=f\"{contract}_{target}_ridge_pred\"),\n",
    "            })\n",
    "\n",
    "        # 3. Lasso with CV over alphas (time-series CV)\n",
    "        if 'lasso' in models and not 'lasso_ridge' in models:\n",
    "            alphas = np.logspace(-4, 6, 100)\n",
    "\n",
    "            lasso = LassoCV(alphas=alphas, cv=tscv, fit_intercept=True, n_jobs=-1)\n",
    "            lasso.fit(X_train, y_train)\n",
    "            yhat_lasso = lasso.predict(X_test)\n",
    "            eval_and_report(y_test, yhat_lasso, f\"Lasso (alpha={lasso.alpha_:.4g})\")\n",
    "\n",
    "            # Print top coefficients\n",
    "            lasso_coef = pd.Series(lasso.coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "            print(\"Top Lasso coeffs:\\n\", lasso_coef.head(10))\n",
    "\n",
    "            results[contract].update({\n",
    "                \"lasso_model\": lasso,\n",
    "                \"lasso_coefs\": lasso_coef,\n",
    "                \"train_data_lasso\": pd.Series(lasso.predict(X_train), index=y_train.index, name=f\"lasso_train\"),\n",
    "                \"yhat_lasso\": pd.Series(yhat_lasso, index=y_test.index, name=f\"{contract}_{target}_lasso_pred\"),\n",
    "            })\n",
    "\n",
    "        # 4. ElasticNet with CV over alphas and l1_ratios (time-series CV)\n",
    "        if 'enet' in models:\n",
    "            alphas = np.logspace(-4, 6, 100)\n",
    "            l1_ratios = np.arange(.1, 1, .1)   # 1.0 == Lasso, 0.0 == Ridge\n",
    "\n",
    "            enet = ElasticNetCV(\n",
    "                alphas=alphas,\n",
    "                l1_ratio=l1_ratios,\n",
    "                cv=tscv,\n",
    "                fit_intercept=True,\n",
    "                max_iter=20000,\n",
    "                n_jobs = -1\n",
    "            )\n",
    "            enet.fit(X_train, y_train)\n",
    "\n",
    "            yhat_enet = enet.predict(X_test)\n",
    "            eval_and_report(y_test, yhat_enet, f\"ElasticNet (alpha={enet.alpha_:.4g}, l1_ratio={enet.l1_ratio_})\")\n",
    "\n",
    "            # Print top coefficients\n",
    "            enet_coef = pd.Series(enet.coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "            print(\"Top ElasticNet coeffs:\\n\", enet_coef.head(10))\n",
    "\n",
    "            results[contract].update({\n",
    "                \"enet_model\": enet,\n",
    "                \"enet_coegs\": enet_coef,\n",
    "                \"train_data_enet\": pd.Series(enet.predict(X_train), index=y_train.index, name=f\"enet_train\"),\n",
    "                \"yhat_enet\": pd.Series(yhat_enet, index=y_test.index, name=f\"{contract}_{target}_enet_pred\"),\n",
    "            })\n",
    "\n",
    "        # 5. PCA on OLS\n",
    "        if 'pca' in models:\n",
    "            pca = PCA(n_components=.9).fit(X_train) # keep x% of variance and fit to training set\n",
    "            train_pca = pca.transform(X_train)\n",
    "            test_pca = pca.transform(X_test)\n",
    "\n",
    "            ols_pca = LinearRegression()\n",
    "            ols_pca.fit(train_pca, y_train)\n",
    "            yhat_pca = ols_pca.predict(test_pca)\n",
    "            eval_and_report(y_test, yhat_pca, \"OLS+PCA\")\n",
    "\n",
    "            results[contract].update({\n",
    "                \"pca_model\": ols_pca,\n",
    "                \"train_data_pca\": pd.Series(ols_pca.predict(train_pca), index=y_train.index, name=f\"pca_train\"),\n",
    "                \"yhat_pca\": pd.Series(yhat_pca, index=y_test.index, name=f\"{contract}_{target}_pca_pred\")\n",
    "            })\n",
    "\n",
    "        # 6. Use lasso into ridge\n",
    "        if 'lasso_ridge' in models:\n",
    "            alphas = np.logspace(-4, 6, 100)\n",
    "\n",
    "            lasso = LassoCV(alphas=alphas, cv=tscv, fit_intercept=True)\n",
    "            lasso.fit(X_train, y_train)\n",
    "            yhat_lasso = lasso.predict(X_test)\n",
    "            eval_and_report(y_test, yhat_lasso, f\"Lasso (alpha={lasso.alpha_:.4g})\")\n",
    "\n",
    "            # Print top coefficients\n",
    "            lasso_coef = pd.Series(lasso.coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "            print(\"Top Lasso coeffs:\\n\", lasso_coef.head(10))\n",
    "\n",
    "            results[contract].update({\n",
    "                \"lasso_model\": lasso,\n",
    "                \"lasso_coefs\": lasso_coef,\n",
    "                \"train_data_lasso\": pd.Series(lasso.predict(X_train), index=y_train.index, name=f\"lasso_train\"),\n",
    "                \"yhat_lasso\": pd.Series(yhat_lasso, index=y_test.index, name=f\"{contract}_{target}_lasso_pred\"),\n",
    "            })\n",
    "        \n",
    "            # Get columns where coefficients are nonzero\n",
    "            important_features = X_train.columns[np.abs(lasso.coef_) > 0]\n",
    "            X_train_reduced = X_train[important_features]\n",
    "            X_test_reduced = X_test[important_features]\n",
    "\n",
    "            alphas = np.logspace(-4, 3, 30)\n",
    "\n",
    "            lasso_ridge = RidgeCV(alphas=alphas, cv=tscv, fit_intercept=True)\n",
    "            lasso_ridge.fit(X_train_reduced, y_train)\n",
    "            yhat_lasso_ridge = lasso_ridge.predict(X_test_reduced)\n",
    "            eval_and_report(y_test, yhat_lasso_ridge, f\"Lasso_Ridge (alpha={lasso_ridge.alpha_:.4g})\")\n",
    "\n",
    "            # Print top coefficients\n",
    "            lasso_ridge_coef = pd.Series(lasso_ridge.coef_, index=important_features).sort_values(key=np.abs, ascending=False)\n",
    "            print(\"Top Lasso_Ridge coeffs:\\n\", lasso_ridge_coef.head(10))\n",
    "\n",
    "            results[contract].update({\n",
    "                \"lasso_ridge_model\": lasso_ridge,\n",
    "                \"lasso_ridge_coefs\": lasso_ridge_coef,\n",
    "                \"train_data_lasso_ridge\": pd.Series(lasso_ridge.predict(X_train_reduced), index=y_train.index, name=f\"lasso_ridge_train\"),\n",
    "                \"yhat_lasso_ridge\": pd.Series(yhat_lasso_ridge, index=y_test.index, name=f\"{contract}_{target}_lasso_ridge_pred\"),\n",
    "            })\n",
    "\n",
    "        # 7. Partial Least Squares (PLS) Regression\n",
    "        if 'pls' in models:\n",
    "            # First, find the best number of components using cross-validation\n",
    "            best_n = -1\n",
    "            best_score = -np.inf\n",
    "            \n",
    "            # Test a range of components (e.g., from 1 to 20)\n",
    "            for n in range(1, 8):\n",
    "                pls_cv = PLSRegression(n_components=n)\n",
    "                # Use a time-series friendly CV\n",
    "                scores = cross_val_score(pls_cv, X_train, y_train, cv=tscv, scoring='r2')\n",
    "                \n",
    "                if np.mean(scores) > best_score:\n",
    "                    best_score = np.mean(scores)\n",
    "                    best_n = n\n",
    "\n",
    "            print(f\"PLS best n_components: {best_n}\")\n",
    "\n",
    "            # Now, fit the final PLS model with the optimal number of components\n",
    "            pls = PLSRegression(n_components=best_n)\n",
    "            pls.fit(X_train, y_train)\n",
    "            yhat_pls = pls.predict(X_test)\n",
    "            eval_and_report(y_test, yhat_pls, f\"PLS (n={best_n})\")\n",
    "\n",
    "            # can get coefficients if needed, but they are in PLS component space\n",
    "            # pls_coef = pd.Series(pls.coef_, index=features)\n",
    "\n",
    "            # Store results\n",
    "            results[contract].update({\n",
    "                \"pls_model\": pls,\n",
    "                # \"pls_coefs\": pls_coef,\n",
    "                \"train_data_pls\": pd.Series(pls.predict(X_train).ravel(), index=y_train.index, name=f\"pls_train\"),\n",
    "                \"yhat_pls\": pd.Series(yhat_pls.ravel(), index=y_test.index, name=f\"{contract}_{target}_pls_pred\"),\n",
    "            })\n",
    "            \n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d9f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_results = get_or_build_results('model_data/drift_training_results.pkl.gz', train_models, drift_train_scaled, drift_test_scaled, linear_model_target, models=linear_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5724d19",
   "metadata": {},
   "source": [
    "Make into new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85911f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_predictions(data, target, results, contracts=contracts, models=linear_models):\n",
    "    df = data.copy()\n",
    "\n",
    "    for contract in contracts:\n",
    "        for model in models:\n",
    "            df[f'{contract}_{target}_pred_{model}'] = pd.concat([results[contract][f'train_data_{model}'],results[contract][f'yhat_{model}']])\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120748fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift = df_from_predictions(drift, linear_model_target, drift_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595774e",
   "metadata": {},
   "source": [
    "Plot these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9644fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(data, baseline, measures, title, subtitles, contracts=contracts, models=linear_models):\n",
    "    fig = make_subplots(\n",
    "        rows=len(contracts), cols=1, shared_xaxes=True, vertical_spacing=0.06,\n",
    "        subplot_titles=[contract for contract in contracts]\n",
    "    )\n",
    "\n",
    "    for i, (contract) in enumerate(contracts, start=1):\n",
    "        show_leg = (i == 1)\n",
    "\n",
    "        # True next_ret\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=data.index, y=data[f'{contract}_{baseline}'], name=f'{contract}_{baseline}',\n",
    "                    mode=\"lines\", line=dict(width=1.6),\n",
    "                    showlegend=show_leg, legendgroup=\"true\"),\n",
    "            row=i, col=1\n",
    "        )\n",
    "\n",
    "        # Plot predictions\n",
    "        for model in models:\n",
    "            for measure in measures:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=data.index, y=data[f'{contract}_{measure}_{model}'], name=f\"{model} prediction\",\n",
    "                            mode=\"lines\", line=dict(width=1.4, dash=\"dot\"),\n",
    "                            showlegend=show_leg, legendgroup=\"ridge\"),\n",
    "                    row=i, col=1\n",
    "                )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        height=2500,\n",
    "        hovermode=\"x unified\",\n",
    "        template=\"plotly_white\",\n",
    "        margin=dict(t=80, r=30, b=80, l=70),\n",
    "        legend=dict(orientation=\"h\", yanchor=\"top\", y=-0.12, xanchor=\"left\", x=0)\n",
    "    )\n",
    "\n",
    "    for r in range(1, len(contracts) + 1):\n",
    "        fig.update_yaxes(title_text=subtitles, row=r, col=1)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7117fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(drift, 'log_close_ret', [f'{linear_model_target}_pred'], 'Linear Regression on Drift', 'Log Return')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9793f600",
   "metadata": {},
   "source": [
    "Overall returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67800fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_price_path(df, pred, linear_model_target=linear_model_target, contracts=contracts, models=linear_models):\n",
    "    df = df.copy()\n",
    "\n",
    "    for contract in contracts:\n",
    "        for model in models:\n",
    "            df[f'{contract}_{pred}_pred_{model}'] = np.exp(df[f'{contract}_log_close'].iloc[0]) * np.exp(df[f'{contract}_{linear_model_target}_pred_{model}'].cumsum())\n",
    "\n",
    "            df[f'{contract}_close'] = np.exp(df[f'{contract}_log_close'].iloc[0]) * np.exp(df[f'{contract}_log_close_ret'].cumsum())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_drift = build_price_path(drift, 'price')\n",
    "plot_results(plot_drift, 'close', ['price_pred'], 'Price Path Evolution of Drift', 'Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6491cf0d",
   "metadata": {},
   "source": [
    "# Volatility Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c40168",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_data = data.copy()\n",
    "vol_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared residuals from previous -- using ElasticNet\n",
    "# Turns out we actually need to also take the log because they are so tiny\n",
    "\n",
    "linear_drift_model = 'enet'\n",
    "volatility_model_target = 'log_squared_residual'\n",
    "\n",
    "for contract in contracts:\n",
    "    vol_data[f'{contract}_{volatility_model_target}'] = np.log(np.square(drift[f'{contract}_{linear_model_target}'] - drift[f'{contract}_{linear_model_target}_pred_{linear_drift_model}']))\n",
    "vol_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c11cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_training_data, vol_testing_data = split_data(vol_data)\n",
    "vol_targets, vol_all_features, vol_unscale_features, vol_scale_features = set_targets_features(volatility_model_target, vol_training_data)\n",
    "\n",
    "\n",
    "# Random forest for now because it's faster\n",
    "vol_pruned_features = get_or_build_results('model_data/vol_features.pkl.gz', determine_feature_importance, vol_training_data, vol_testing_data, vol_all_features, volatility_model_target, mode='random_forest')\n",
    "\n",
    "for contract in contracts:\n",
    "    print(vol_pruned_features[contract]['selected_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd4472",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_train_scaled, vol_test_scaled = scale_features(vol_training_data, vol_testing_data, vol_scale_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af70788",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_test_scaled.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d247f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "volatility_models = ['ridge', 'lasso', 'pca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e386364",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_results = get_or_build_results('model_data/vol_results.pkl.gz', train_models, vol_train_scaled, vol_test_scaled, volatility_model_target, pruned_features=vol_pruned_features, models=volatility_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdeb113",
   "metadata": {},
   "source": [
    "Make into new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = df_from_predictions(vol_data, volatility_model_target, vol_results, models=volatility_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3602b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(diffusion, volatility_model_target, [f'{volatility_model_target}_pred'], 'Linear Regression on Diffusion', 'Log Residuals', models=volatility_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2080f5be",
   "metadata": {},
   "source": [
    "Convert to variance / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for contract in contracts:\n",
    "    for model in volatility_models:\n",
    "        diffusion[f'{contract}_variance_pred_{model}'] = np.exp(diffusion[f'{contract}_{volatility_model_target}_pred_{model}'])\n",
    "        diffusion[f'{contract}_std_pred_{model}'] = np.sqrt(diffusion[f'{contract}_variance_pred_{model}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.merge(drift, diffusion, left_index=True, right_index=True, how='inner', suffixes=('', '_to_drop'))\n",
    "predictions = predictions.drop(columns=[col for col in predictions.columns if col.endswith('_to_drop')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b6834f",
   "metadata": {},
   "source": [
    "Let's add a rolling IC column to test with damping mu projections later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for contract in contracts:\n",
    "    predictions[f'{contract}_{linear_model_target}'] = predictions[f'{contract}_log_close'].diff().shift(-1).dropna(inplace=True)\n",
    "    for model in linear_models:\n",
    "        predictions[f'{contract}_IC_{model}'] = predictions[f'{contract}_{linear_model_target}'].rolling(window=21).corr(predictions[f'{contract}_{linear_model_target}_pred_{model}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe9be3d",
   "metadata": {},
   "source": [
    "## Looks like we obtained a pretty smooth path. There are many extreme values not being captured -- Thus the motivation for GARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f67b6",
   "metadata": {},
   "source": [
    "# GARCH Volatility Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0889c7c",
   "metadata": {},
   "source": [
    "Set up some GARCH models, check their fit our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904cc1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_data = drift.copy()\n",
    "garch_scale = 10\n",
    "\n",
    "\n",
    "# Get drift residuals:\n",
    "for contract in contracts:\n",
    "    garch_data[f'{contract}_drift_residuals_{linear_drift_model}'] = drift[f'{contract}_{linear_model_target}'] - drift[f'{contract}_{linear_model_target}_pred_{linear_drift_model}'] # Calculate residual from enet\n",
    "    garch_data[f'{contract}_drift_residuals_{linear_drift_model}'] = garch_data[f'{contract}_drift_residuals_{linear_drift_model}'] * garch_scale # Scaling for better convergence\n",
    "\n",
    "# Split data\n",
    "garch_training, garch_testing = split_data(garch_data)\n",
    "\n",
    "# Models, distributions, and paramters to check\n",
    "\n",
    "garch_models = {\n",
    "    'Garch': 'Garch',\n",
    "    'Egarch': 'Egarch',\n",
    "    'GJR_Garch': 'Garch',\n",
    "    'APARCH': 'APARCH'\n",
    "}\n",
    "\n",
    "dists = ['normal', 't', 'skewt']\n",
    "\n",
    "params = {\n",
    "    'Garch': {'p': 1, 'o': 0, 'q': 1},\n",
    "    'Egarch': {'p': 2, 'o': 1, 'q': 1},\n",
    "    'GJR_Garch': {'p': 1, 'o': 1, 'q': 1},\n",
    "    'APARCH': {'p': 1, 'o': 1, 'q': 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a78ffa",
   "metadata": {},
   "source": [
    "Check the fit to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c39c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for contract in contracts: # Test against normal / t\n",
    "    print(f'=== {contract} ===')\n",
    "    for model in garch_models:\n",
    "        for dist in dists: \n",
    "            \n",
    "            try:\n",
    "                garch = arch_model(garch_training[f'{contract}_drift_residuals_{linear_drift_model}'], vol=garch_models[model], **params[model], dist=dist) # Set model type\n",
    "                garch_fit = garch.fit(disp='off') # Fit model, disp = off turn off output\n",
    "\n",
    "                print(garch_fit.summary()) # Print results       \n",
    "            except:\n",
    "                print('\\n')\n",
    "                print(f'--- {model} with {dist} failed to fit for {contract} ---')\n",
    "            print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e1f68",
   "metadata": {},
   "source": [
    "Results: skew-t always prevails trailed closely by t, Egarch and GJR-Garch are very similar, looks like APARCH wins overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cdce77",
   "metadata": {},
   "source": [
    "Let's try a slightly more rigorous approach by grid searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76aeaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlike(y2, fvar):\n",
    "    y2 = np.asarray(y2, float)\n",
    "    fvar = np.asarray(fvar, float)\n",
    "    return np.mean(y2 / fvar + np.log(fvar))\n",
    "\n",
    "def fit_one(y, vol_name, model_name, p, o, q, dist, mean='Constant', lags=0):\n",
    "    kw = dict(mean=mean, lags=lags, vol=vol_name, dist=dist, p=p, q=q)\n",
    "    if model_name != 'Garch':  # only asym metrics use 'o'\n",
    "        kw['o'] = o\n",
    "    am = arch_model(y, **kw)\n",
    "    return am.fit(disp='off')\n",
    "\n",
    "def fit_arx(y, X, vol_name, model_name, p, o, q, dist, mean, lags):\n",
    "    kw = dict(mean=mean, lags=lags, x=X, vol=vol_name, dist=dist, p=p, q=q)\n",
    "    if model_name != 'Garch':  # only asym metrics use 'o'\n",
    "        kw['o'] = o\n",
    "    am = arch_model(y, **kw)\n",
    "    return am.fit(disp='off')\n",
    "\n",
    "def rolling_oos_qlike(y, vol_name, model_name, p, o, q, dist,\n",
    "                      mean='Constant', lags=0, refit_every=21, start_frac=0.6):\n",
    "    \"\"\"1-step-ahead variance forecasts with periodic refits.\"\"\"\n",
    "    y = y.dropna()\n",
    "    n = len(y)\n",
    "    start = int(n * start_frac)\n",
    "    fvars = []\n",
    "    res = None\n",
    "    for t in range(start, n - 1):\n",
    "        if res is None or ((t - start) % refit_every == 0):\n",
    "            res = fit_one(y.iloc[:t + 1], vol_name, model_name, p, o, q, dist, mean, lags)\n",
    "        fvar = res.forecast(horizon=1, reindex=False).variance.values[-1, 0]\n",
    "        fvars.append(fvar)\n",
    "    return qlike((y**2).iloc[start+1:], fvars)\n",
    "\n",
    "def garch_grid_search(\n",
    "    garch_training,             # DataFrame with your series, e.g., f'{contract}_drift_residuals_enet'             # dict like {'Garch':'GARCH','Egarch':'EGARCH','GJR_Garch':'GJR-GARCH','APARCH':'APARCH'}\n",
    "    dists=('t','skewt'),\n",
    "    P=(1,2,3), Q=(1,2,3), O=(0,1),\n",
    "    means=('Constant', 'Zero'), lags=(0,1,2),\n",
    "    contracts=contracts,\n",
    "    garch_models = garch_models,\n",
    "    compute_oos=False, refit_every=21, start_frac=0.6, arx=False, X=None\n",
    "):\n",
    "    rows = []\n",
    "    for contract in contracts:\n",
    "        y = garch_training[f'{contract}_drift_residuals_{linear_drift_model}'].dropna()\n",
    "        for model_name, vol_name in garch_models.items():\n",
    "            # Build parameter grid; for plain GARCH, force o=0 only\n",
    "            grid_O = (0,) if model_name == 'Garch' else O\n",
    "            for p, q, o, dist, lag, mean in product(P, Q, grid_O, dists, lags, means):\n",
    "                try:\n",
    "                    if arx:\n",
    "                        res = fit_arx(y, X, vol_name, model_name, p, o, q, dist, mean, lag)\n",
    "                    else:\n",
    "                        res = fit_one(y, vol_name, model_name, p, o, q, dist, mean, lag)\n",
    "                    aic = res.aic\n",
    "                    bic = res.bic\n",
    "                    llf = res.loglikelihood\n",
    "                    row = {\n",
    "                        'contract': contract,\n",
    "                        'model': model_name,\n",
    "                        'vol': vol_name,\n",
    "                        'dist': dist,\n",
    "                        'p': p, 'o': o, 'q': q,\n",
    "                        'aic': aic, 'bic': bic, 'llf': llf,\n",
    "                        'lags': lag,\n",
    "                        'mean': mean\n",
    "                    }\n",
    "                    if compute_oos:\n",
    "                        try:\n",
    "                            oos = rolling_oos_qlike(y, vol_name, model_name, p, o, q, dist,\n",
    "                                                    mean, lags, refit_every, start_frac)\n",
    "                        except Exception:\n",
    "                            oos = np.nan\n",
    "                        row['oos_qlike'] = oos\n",
    "                    rows.append(row)\n",
    "                except Exception:\n",
    "                    # skip failed fits\n",
    "                    print('Failed fit, skipping')\n",
    "                    continue\n",
    "    out = pd.DataFrame(rows)\n",
    "    if out.empty:\n",
    "        return out\n",
    "    sort_cols = ['oos_qlike','aic'] if compute_oos else ['aic']\n",
    "    ascending = [True] * len(sort_cols)\n",
    "    return out.sort_values(sort_cols, ascending=ascending).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd25add",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = get_or_build_results('model_data/garch_grid_search', garch_grid_search, garch_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results.drop_duplicates(subset=['contract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2344a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_test_models = ['Egarch', 'APARCH']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510be84",
   "metadata": {},
   "source": [
    "## Forecast our testing period with the chosen models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dba090",
   "metadata": {},
   "outputs": [],
   "source": [
    "refit_freq = 21 # Frequency to refit at -- slightly larger helps with signal:noise, but not too big\n",
    "\n",
    "for contract in contracts:\n",
    "    for model in garch_test_models:\n",
    "        drift_residuals = garch_data[f'{contract}_drift_residuals_{linear_drift_model}']\n",
    "\n",
    "        scaled_predictions = []\n",
    "        for i in range(len(garch_training), len(garch_training) + len(garch_testing)): # Walk forward, forecast each step and refit model based on new inputs\n",
    "\n",
    "            # Ensure we fit on first day and at our frequency\n",
    "            is_refit_day = (i - len(garch_training)) % refit_freq == 0\n",
    "            \n",
    "            if is_refit_day:\n",
    "                garch = arch_model(drift_residuals.iloc[:i], vol=garch_models[model], **params[model], mean='Zero', dist='skewt')\n",
    "                garch_fit = garch.fit(disp='off') # fit\n",
    "\n",
    "            forecast = garch_fit.forecast(horizon=1)\n",
    "\n",
    "            pred_variance = forecast.variance.iloc[-1].values[0]\n",
    "            scaled_predictions.append(pred_variance)\n",
    "\n",
    "        # Get exponentiation of std\n",
    "        if model == 'APARCH':\n",
    "            delta = garch_fit.params['delta']\n",
    "        else:\n",
    "            delta = 2\n",
    "        \n",
    "        unscaled_predictions = pd.Series(scaled_predictions, index=garch_testing.index) / (garch_scale ** delta) # Unscale (inputs scaled by 100 for convergence, var is squared, but APARCH may return delta =/= 2)\n",
    "        final_predictions = unscaled_predictions ** (1 / delta)\n",
    "\n",
    "        garch_testing[f'{contract}_std_pred_{model}'] = final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4275f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "for contract in contracts:\n",
    "    for model in garch_test_models:\n",
    "        predictions[f'{contract}_std_pred_{model}'] = garch_testing[f'{contract}_std_pred_{model}']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73fe15",
   "metadata": {},
   "source": [
    "# Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b85348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm # For confidence scaling testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c757b4d1",
   "metadata": {},
   "source": [
    "Sizing strategy 1: Using Merton portfolio optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5255f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kelly_fraction(mu, rf, sigma, fraction=.5, alpha=0.0, D=1.0):\n",
    "    size = fraction * (mu - rf) / sigma**2 \n",
    "    scale = (1 - alpha * D) / (1 - alpha) # Optional drawdown floor, alpha is min portfolio value, D is high / current wealth\n",
    "    return size * scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a684f672",
   "metadata": {},
   "source": [
    "Sizing strategy 2: Using vol target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a58ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vol_target_sizing(target, vol):\n",
    "    size = (target/vol)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894979b4",
   "metadata": {},
   "source": [
    "Returns info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_and_mdd(df, col_equity, col_rf=\"ten_yr\", periods_per_year=252):\n",
    "    eq = df[col_equity]\n",
    "    rf = df[col_rf]\n",
    "\n",
    "    # compute portfolio simple returns\n",
    "    port_rets = eq.pct_change().dropna()\n",
    "    # align rf\n",
    "    rf_aligned = rf.reindex(port_rets.index).astype(float)\n",
    "\n",
    "    # excess returns (assuming rf is already per-period, e.g. daily)\n",
    "    excess = port_rets - ((1+rf_aligned/100)**(1/periods_per_year)-1)\n",
    "    sharpe = (excess.mean() / excess.std()) * np.sqrt(periods_per_year) if excess.std() > 0 else np.nan\n",
    "\n",
    "    # max drawdown\n",
    "    running_max = eq.cummax()\n",
    "    drawdown = eq / running_max - 1.0\n",
    "    max_dd = drawdown.min()\n",
    "\n",
    "    return sharpe, max_dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44378db9",
   "metadata": {},
   "source": [
    "## Trading strategy 1: Trade based on our models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2f6bf",
   "metadata": {},
   "source": [
    "### Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73c1a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_models = ['ridge', 'lasso', 'APARCH', 'Egarch'] # Volatility models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c5260",
   "metadata": {},
   "source": [
    "Get historical margin amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_margins = {}\n",
    "\n",
    "for contract in contracts:\n",
    "    historical_margins[contract] = pd.read_csv(f'{contract}_Historical_Margin.csv')\n",
    "    historical_margins[contract]['Date'] = pd.to_datetime(historical_margins[contract]['Date'])\n",
    "    historical_margins[contract].set_index(historical_margins[contract]['Date'], inplace=True)\n",
    "    historical_margins[contract].drop(columns=['Date'], inplace=True)\n",
    "    historical_margins[contract].rename(columns={'Long Margin': f'{contract}_Long', 'Short Margin': f'{contract}_Short'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b0d18f",
   "metadata": {},
   "source": [
    "### Individual Contract BTs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34a8f4",
   "metadata": {},
   "source": [
    "Backtesting with margin functionality (with rules per CME future challenge)  \n",
    "Note that I am omitting the minimum 10 per day contract trading rule because we can just buy and sell quickly to nullify the effect  \n",
    "I am also omitting the liquidation before expiry rule because there are no expiries during the competition  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bfe102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create backtest df copy\n",
    "bt = predictions.copy()\n",
    "bt = bt.join([margin_data for margin_data in historical_margins.values()])\n",
    "bt = bt.replace(0, pd.NA)\n",
    "bt = bt.fillna(method='ffill')\n",
    "bt_train, bt_test = split_data(bt)\n",
    "linear_drift_model = 'enet'\n",
    "\n",
    "# Set initial conditions\n",
    "starting_cash = 500_000\n",
    "commission = 2.5\n",
    "drawdown_lock = -.2\n",
    "\n",
    "initial_margin = 1.1 # 10% extra assumption on initial entry\n",
    "tick_size = {\n",
    "    'ES': 0.25, \n",
    "    'NQ': 0.25, \n",
    "    'FV': 0.0078125, \n",
    "    'TY': 0.015625, \n",
    "    'US': 0.03125, \n",
    "    'GC': 0.10, \n",
    "    'CL': 0.01, \n",
    "    'JY': 0.0000005,\n",
    "    'BP': 0.0001, \n",
    "    'EU': 0.00005\n",
    "}\n",
    "\n",
    "tick_value = {\n",
    "    'ES': 12.50, \n",
    "    'NQ': 5.00, \n",
    "    'FV': 7.8125, \n",
    "    'TY': 15.625, \n",
    "    'US': 31.25, \n",
    "    'GC': 10.00, \n",
    "    'CL': 10.00, \n",
    "    'JY': 6.25,\n",
    "    'BP': 6.25, \n",
    "    'EU': 6.25\n",
    "}\n",
    "\n",
    "bt_data = {}\n",
    "\n",
    "for contract in contracts:\n",
    "    bt_data[contract] = {}\n",
    "\n",
    "    for model in bt_models:\n",
    "        bt_test[f'{contract}_cash_{model}'] = 0.0\n",
    "        bt_test[f'{contract}_margin_{model}'] = 0.0\n",
    "        bt_test.loc[bt_test.index[0], f'{contract}_cash_{model}'] = starting_cash\n",
    "        bt_test[f'{contract}_signal_{model}'] = 0\n",
    "        bt_test[f'{contract}_baseline_wealth'] = np.exp(bt_test[f'{contract}_log_close_ret'].cumsum()) * starting_cash\n",
    "        bt_test[f'{contract}_margin_usage_{model}'] = 0\n",
    "\n",
    "\n",
    "        bt_data[contract][model] = {\n",
    "            'last_margin': 0,\n",
    "            'last_cash': starting_cash,\n",
    "            'position': 0,\n",
    "            'average_vol_weight': 0,\n",
    "            'average_confidence': 0,\n",
    "            'average_vol': 0,\n",
    "            'average_mu': 0,\n",
    "            'average_rf': 0,\n",
    "            'average_ret': 0,\n",
    "            'high_water': 0,\n",
    "        }\n",
    "\n",
    "# Starts at second day\n",
    "for idx, row in bt_test.iloc[1:].iterrows():\n",
    "    for contract in contracts:\n",
    "        vol_data = []\n",
    "        for model in bt_models:\n",
    "            last_margin = bt_data[contract][model]['last_margin']\n",
    "            last_cash = bt_data[contract][model]['last_cash']\n",
    "\n",
    "            multiplier = tick_value[contract] / tick_size[contract]\n",
    "            position_size = bt_data[contract][model]['position']\n",
    "            lock_up = False\n",
    "\n",
    "            \n",
    "            vol = row[f'{contract}_std_pred_{model}']\n",
    "            vol_data.append(vol)\n",
    "            vol_data = vol_data[-21:]\n",
    "\n",
    "            if model not in garch_models.keys():\n",
    "                mu = np.exp(row[f'{contract}_{linear_model_target}_pred_{linear_drift_model}'] - .5*vol**2) - 1 # I believe we need the vol adjustment for drift based on the way we did it for the regular models\n",
    "            else:\n",
    "                mu = np.exp(row[f'{contract}_{linear_model_target}_pred_{linear_drift_model}']) - 1\n",
    "                \n",
    "            rf = (1+row['^TNX_close']/100)**(1/252)-1 # Daily\n",
    "            investment = 0\n",
    "            ret = np.exp(row[f'{contract}_log_close_ret']) - 1\n",
    "            delta = ret*np.exp(row[f'{contract}_log_open']) # This calculation is basically assuming we are making our trading decisions at the end of every day\n",
    "            pred = mu*np.exp(row[f'{contract}_log_close'])\n",
    "            \n",
    "\n",
    "            # Update margin value\n",
    "            pnl_today = bt_data[contract][model]['position'] * delta * multiplier\n",
    "\n",
    "            # Calculate total equity\n",
    "            last_margin += pnl_today\n",
    "            total_equity = last_margin + last_cash\n",
    "\n",
    "            # Calculate max portfolio value for drawdown constraints\n",
    "            bt_data[contract][model]['high_water'] = max(total_equity, bt_data[contract][model]['high_water'])\n",
    "\n",
    "            # Check maintenance requirements\n",
    "            if position_size > 0:\n",
    "                direction = 'Long'\n",
    "                side = 1\n",
    "            else:\n",
    "                direction = 'Short'\n",
    "                side = -1\n",
    "\n",
    "            margin_difference = row[f'{contract}_{direction}']*position_size - last_margin\n",
    "            if margin_difference > 0 and position_size != 0 :\n",
    "                # If enough cash to top up, do it automatically\n",
    "                if total_equity >= margin_difference:\n",
    "                    last_cash -= margin_difference\n",
    "                    last_margin += margin_difference\n",
    "                else:\n",
    "                    # Otherwise liquidate\n",
    "                    bt_data[contract][model]['position'] = 0\n",
    "                    last_cash += last_margin # Ignoring commission\n",
    "                    last_margin = 0\n",
    "\n",
    "            # Recalculate total equity\n",
    "            total_equity = last_margin + last_cash\n",
    "\n",
    "            # If drawdown criteria met, lock portfolio for day\n",
    "            if ret <= drawdown_lock:\n",
    "                lock_up = True\n",
    "\n",
    "            if not lock_up:\n",
    "                # Get bet sizing with fractional kelly -- default is .5\n",
    "                leverage = 1 # May want to look into making this dynamic... tried a little bit with mixed results\n",
    "                target_notional = kelly_fraction(mu, rf, vol, fraction=.5) * total_equity * leverage # Daily rf\n",
    "                target_notional = max(-total_equity * leverage, min(target_notional, total_equity * leverage)) # Make sure we don't exceed equity\n",
    "\n",
    "                # Get bet sizing with fixed vol target\n",
    "                #base_weight = get_vol_target_sizing(.15 / np.sqrt(252), vol)\n",
    "                #confidence = (mu - rf) / vol\n",
    "                #k=2\n",
    "                #target_weight = base_weight * norm.cdf(confidence*np.sqrt(252))*k # Annualize\n",
    "                #target_notional = target_weight * total_equity\n",
    "                #target_notional = max(-total_equity, min(target_notional, total_equity)) # Make sure we don't exceed equity / effective equity cap\n",
    "\n",
    "                # Data logging\n",
    "                bt_data[contract][model]['average_rf'] += rf\n",
    "                bt_data[contract][model]['average_ret'] += ret\n",
    "                bt_data[contract][model]['average_mu'] += mu\n",
    "                bt_data[contract][model]['average_vol'] += vol\n",
    "                #bt_data[contract][model]['average_vol_weight'] += base_weight\n",
    "                #bt_data[contract][model]['average_confidence'] += norm.cdf(confidence*np.sqrt(252))*k\n",
    "\n",
    "                # Execute trade / Update investment amount to reach target position\n",
    "                if target_notional > 0:\n",
    "                    direction = 'Long'\n",
    "                else: \n",
    "                    direction = 'Short'\n",
    "\n",
    "                notional_per_contract = np.exp(row[f'{contract}_log_close']) * multiplier\n",
    "                initial_cost = row[f'{contract}_{direction}']*initial_margin # Get current maintenance * initial margin percentage requirement\n",
    "                position = math.floor(target_notional/initial_cost) * .15 # Get number of contracts to calculate margin investment, ignoring commission\n",
    "                investment = position*initial_cost - last_margin\n",
    "                investment = max(-last_cash, min(investment, last_cash))\n",
    "\n",
    "                bt_data[contract][model]['position'] = position\n",
    "\n",
    "            else:\n",
    "                investment = 0\n",
    "\n",
    "            # Update cash\n",
    "            last_cash = bt_data[contract][model]['last_cash'] = last_cash - investment\n",
    "            bt_test.loc[idx, f'{contract}_cash_{model}'] = last_cash\n",
    "\n",
    "            # Store portfolio value as last value for calculation\n",
    "            bt_data[contract][model]['last_margin'] = last_margin + investment\n",
    "            bt_test.loc[idx, f'{contract}_margin_{model}'] = bt_data[contract][model]['last_margin']\n",
    "            bt_test.loc[idx, f'{contract}_margin_usage_{model}'] = last_margin/total_equity\n",
    "\n",
    "            # Set signal for plotting later\n",
    "            if investment > 0:\n",
    "                bt_test.loc[idx, f'{contract}_signal_{model}'] = 1\n",
    "            elif investment < 0:\n",
    "                bt_test.loc[idx, f'{contract}_signal_{model}'] = -1\n",
    "    \n",
    "for contract in contracts:\n",
    "    print(f'=== {contract} ===')\n",
    "    total_value = f'{contract}_baseline_wealth'\n",
    "    \n",
    "    final_training_value = bt_test[total_value].iloc[-1]\n",
    "    training_cagr = 100 * ((final_training_value/bt_test[total_value].iloc[0])**(1/(bt_test.index[-1].year - bt_test.index[0].year + bt_test.index[-1].month/12))-1)\n",
    "    print(f'Final value for {contract}, baseline: {final_training_value:.2f}')\n",
    "    \n",
    "    test_sharpe, test_mdd = sharpe_and_mdd(bt_test, total_value, col_rf=\"^TNX_close\")\n",
    "    print(f\"Sharpe: {test_sharpe:.2f}, Max Drawdown: {test_mdd:.2%}, CAGR: {training_cagr:.2f}%\\n\")\n",
    "    for model in bt_models:\n",
    "        \n",
    "        total_value = f'{contract}_total_value_{model}'\n",
    "        bt_test[total_value] = bt_test[f'{contract}_margin_{model}'] + bt_test[f'{contract}_cash_{model}']\n",
    "\n",
    "        final_testing_value = bt_test[total_value].iloc[-1]\n",
    "        testing_cagr = 100*((final_testing_value/bt_test[total_value].iloc[0])**(1/(bt_test.index[-1].year - bt_test.index[0].year + bt_test.index[-1].month/12))-1)\n",
    "        print(f'Final value for {contract}, {model}: {final_testing_value:.2f}')\n",
    "        \n",
    "        test_sharpe, test_mdd = sharpe_and_mdd(bt_test, total_value, col_rf=\"^TNX_close\")\n",
    "        print(f\"Sharpe: {test_sharpe:.2f}, Max Drawdown: {test_mdd:.2%}, CAGR: {testing_cagr:.2f}%\")\n",
    "\n",
    "\n",
    "        bt_data[contract][model]['average_mu'] /= len(bt_test)\n",
    "        bt_data[contract][model]['average_vol'] /= len(bt_test)\n",
    "        bt_data[contract][model]['average_rf'] /= len(bt_test)\n",
    "        bt_data[contract][model]['average_ret'] /= len(bt_test)\n",
    "        average_margin_util = bt_test[f'{contract}_margin_usage_{model}'].sum() / len(bt_test)\n",
    "        print(f\"Average Vol: {bt_data[contract][model]['average_vol']:.4f}, Average mu: {bt_data[contract][model]['average_mu']:.4f}, Average ret: {bt_data[contract][model]['average_ret']:.4f}\")\n",
    "        print(f\"Average Margin Utilizization: {average_margin_util*100:.2f}%\")\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2696ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(bt_test, 'baseline_wealth', ['total_value'], 'Backtest Performance', 'Wealth', models=bt_models, contracts=contracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9001c47",
   "metadata": {},
   "source": [
    "Might be slower, but much easier to implement strategy using iterative process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e22b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create backtest df copy\n",
    "bt = predictions.copy()\n",
    "bt_training, bt_test = split_data(bt)\n",
    "\n",
    "# Set initial conditions\n",
    "starting_cash = 10000\n",
    "bt_data = {}\n",
    "\n",
    "for contract in contracts:\n",
    "    bt_data[contract] = {}\n",
    "\n",
    "    for model in bt_models:\n",
    "        bt_test[f'{contract}_portfolio_value_{model}'] = 0.0\n",
    "        bt_test[f'{contract}_cash_{model}'] = 0.0\n",
    "        bt_test.loc[bt_test.index[0], f'{contract}_cash_{model}'] = starting_cash\n",
    "        bt_test[f'{contract}_signal_{model}'] = 0\n",
    "        bt_test[f'{contract}_baseline_wealth'] = np.exp(bt_test[f'{contract}_log_close_ret'].cumsum()) * 10000\n",
    "\n",
    "\n",
    "        bt_data[contract][model] = {\n",
    "            'last_value': 0,\n",
    "            'last_cash': starting_cash,\n",
    "            'average_vol_weight': 0,\n",
    "            'average_confidence': 0,\n",
    "            'average_vol': 0,\n",
    "            'average_mu': 0,\n",
    "            'average_rf': 0,\n",
    "            'average_ret': 0\n",
    "        }\n",
    "\n",
    "# Starts at second day\n",
    "for idx, row in bt_test.iloc[1:].iterrows():\n",
    "    for contract in contracts:\n",
    "        for model in bt_models:\n",
    "            last_value = bt_data[contract][model]['last_value']\n",
    "            last_cash = bt_data[contract][model]['last_cash']\n",
    "\n",
    "            mu = np.exp(row[f'{contract}_{linear_model_target}_pred_{linear_drift_model}']) - 1\n",
    "            vol = row[f'{contract}_std_pred_{model}']\n",
    "            rf = (1+row['^TNX_close']/100)**(1/252)-1 # Daily\n",
    "            investment = 0\n",
    "            ret = np.exp(row[f'{contract}_log_close_ret'])\n",
    "\n",
    "            # Update portfolio value\n",
    "            current_value = last_value * ret\n",
    "\n",
    "            # Calculate total equity\n",
    "            total_equity = current_value + last_cash \n",
    "\n",
    "            # Get bet sizing with Merton\n",
    "            #target_pos = get_merton_size(mu, rf, vol) * total_equity # Daily rf\n",
    "            #target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "\n",
    "            # Get bet sizing with vol target\n",
    "            #target_pos = get_vol_target_sizing(.1/np.sqrt(252), vol) * total_equity\n",
    "            #target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "            \n",
    "            # Hybrid - Scale it with signed confidence from mu\n",
    "            base_weight = get_vol_target_sizing(.15 / np.sqrt(252), vol)\n",
    "            confidence = (mu - rf) / vol # Essentially a sharpe ratio\n",
    "            k=2\n",
    "            target_weight = base_weight * norm.cdf(confidence*np.sqrt(252))*k # Annualize\n",
    "            target_pos = target_weight * total_equity\n",
    "            target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "\n",
    "            # Data logging\n",
    "            bt_data[contract][model]['average_rf'] += rf\n",
    "            bt_data[contract][model]['average_ret'] += ret - 1\n",
    "            bt_data[contract][model]['average_mu'] += mu\n",
    "            bt_data[contract][model]['average_vol'] += vol\n",
    "            bt_data[contract][model]['average_vol_weight'] += base_weight\n",
    "            bt_data[contract][model]['average_confidence'] += norm.cdf(confidence*np.sqrt(252))*k\n",
    "\n",
    "            # Bet sizing based on size of potential increase/decrease, signed\n",
    "            #target_pos = (mu - rf)/abs(mu - rf) * total_equity\n",
    "            #target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "\n",
    "            # Execute trade / Update investment amount to reach target position\n",
    "            investment = target_pos - current_value\n",
    "\n",
    "            # Update cash\n",
    "            last_cash = bt_data[contract][model]['last_cash'] = last_cash - investment\n",
    "            bt_test.loc[idx, f'{contract}_cash_{model}'] = last_cash\n",
    "\n",
    "            # Store portfolio value as last value for calculation\n",
    "            bt_data[contract][model]['last_value'] = current_value + investment\n",
    "            bt_test.loc[idx, f'{contract}_portfolio_value_{model}'] = bt_data[contract][model]['last_value']\n",
    "\n",
    "            # Set signal for plotting later\n",
    "            if investment > 0:\n",
    "                bt_test.loc[idx, f'{contract}_signal_{model}'] = 1\n",
    "            elif investment < 0:\n",
    "                bt_test.loc[idx, f'{contract}_signal_{model}'] = -1\n",
    "    \n",
    "for contract in contracts:\n",
    "    print(f'=== {contract} ===')\n",
    "    total_value = f'{contract}_baseline_wealth'\n",
    "    \n",
    "    final_training_value = bt_test[total_value].iloc[-1]\n",
    "    training_cagr = 100 * ((final_training_value/bt_test[total_value].iloc[0])**(1/(bt_test.index[-1].year - bt_test.index[0].year + bt_test.index[-1].month/12))-1)\n",
    "    print(f'Final value for {contract}, baseline: {final_training_value:.2f}')\n",
    "    \n",
    "    test_sharpe, test_mdd = sharpe_and_mdd(bt_test, total_value, col_rf=\"^TNX_close\")\n",
    "    print(f\"Sharpe: {test_sharpe:.2f}, Max Drawdown: {test_mdd:.2%}, CAGR: {training_cagr:.2f}%\\n\")\n",
    "    for model in bt_models:\n",
    "        \n",
    "        total_value = f'{contract}_total_value_{model}'\n",
    "        bt_test[total_value] = bt_test[f'{contract}_portfolio_value_{model}'] + bt_test[f'{contract}_cash_{model}']\n",
    "\n",
    "        final_testing_value = bt_test[total_value].iloc[-1]\n",
    "        testing_cagr = 100*((final_testing_value/bt_test[total_value].iloc[0])**(1/(bt_test.index[-1].year - bt_test.index[0].year + bt_test.index[-1].month/12))-1)\n",
    "        print(f'Final value for {contract}, {model}: {final_testing_value:.2f}')\n",
    "        \n",
    "        test_sharpe, test_mdd = sharpe_and_mdd(bt_test, total_value, col_rf=\"^TNX_close\")\n",
    "        print(f\"Sharpe: {test_sharpe:.2f}, Max Drawdown: {test_mdd:.2%}, CAGR: {testing_cagr:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "        bt_data[contract][model]['average_vol_weight'] /= len(bt_test)\n",
    "        bt_data[contract][model]['average_confidence'] /= len(bt_test)\n",
    "        bt_data[contract][model]['average_mu'] /= len(bt_test)\n",
    "        bt_data[contract][model]['average_vol'] /= len(bt_test)\n",
    "        bt_data[contract][model]['average_rf'] /= len(bt_test)\n",
    "        bt_data[contract][model]['average_ret'] /= len(bt_test)\n",
    "        print(f\"Average Vol: {bt_data[contract][model]['average_vol']:.4f}, Average mu: {bt_data[contract][model]['average_mu']:.4f}, Average ret: {bt_data[contract][model]['average_ret']:.4f}, Average rf: {bt_data[contract][model]['average_rf']:.4f}\")\n",
    "        print(f\"Average Vol Weight: {bt_data[contract][model]['average_vol_weight']:.4f}, Average Confidence Scaling: {bt_data[contract][model]['average_confidence']:.4f}\\n\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63437aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(bt_test, 'baseline_wealth', ['total_value'], 'Backtest Performance', 'Wealth', models=bt_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a38e46",
   "metadata": {},
   "source": [
    "Trading strategy 2: Trading using bollinger bands and moving average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccacb45",
   "metadata": {},
   "source": [
    "# Boll band construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boll bands with enet\n",
    "def create_boll_bands(measure='std_pred', std=2, window=20):\n",
    "    for index in indexes:\n",
    "        for model in models:\n",
    "            vol = predictions[f'{index}_{measure}_{model}']\n",
    "            predictions[f'{index}_sma'] = predictions[f'{index}_Close'].rolling(window=window, min_periods=window).mean()\n",
    "            predictions[f'{index}_upper_boll_{model}'] = std * vol * predictions[f'{index}_sma'] + predictions[f'{index}_sma']\n",
    "            predictions[f'{index}_lower_boll_{model}'] = -std * vol * predictions[f'{index}_sma'] + predictions[f'{index}_sma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e05a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in indexes:\n",
    "    for model in models:\n",
    "        predictions[f'{index}_realized_vol_{model}'] = predictions[f'{index}_price_pred_{model}'].rolling(window=5,min_periods=5).std()\n",
    "\n",
    "create_boll_bands()\n",
    "plot_results(predictions, 'Close', ['upper_boll', 'lower_boll', 'price_pred'], 'Bollinger Bands', 'Price', indexes, ['enet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a036af",
   "metadata": {},
   "source": [
    "Let's see some stats on boll bands here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670aaab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_boll_counts():\n",
    "    for index in indexes:\n",
    "        px_col  = f'{index}_Close'\n",
    "        sma_col = f'{index}_sma'\n",
    "\n",
    "        for model in models:\n",
    "            std_col   = f'{index}_std_pred_{model}'\n",
    "            up_col    = f'{index}_upper_boll_{model}'\n",
    "            low_col   = f'{index}_lower_boll_{model}'\n",
    "            sig_col   = f'{index}_boll_signal_{model}'            # -1 lower break, +1 upper break, 0 inside\n",
    "            touchU    = f'{index}_touch_upper_{model}'            # boolean\n",
    "            touchL    = f'{index}_touch_lower_{model}'\n",
    "            crossU    = f'{index}_cross_above_upper_{model}'      # boolean: crossed today\n",
    "            crossL    = f'{index}_cross_below_lower_{model}'\n",
    "            countAny  = f'{index}_cum_band_breaks_{model}'        # cumulative count\n",
    "            countU    = f'{index}_cum_upper_breaks_{model}'\n",
    "            countL    = f'{index}_cum_lower_breaks_{model}'\n",
    "            inBand    = f'{index}_inside_band_{model}'\n",
    "            widthCol  = f'{index}_band_width_{model}'             # relative width\n",
    "\n",
    "            # 3) Booleans: touches (price outside band)\n",
    "            price = predictions[px_col]\n",
    "            upper = predictions[up_col]\n",
    "            lower = predictions[low_col]\n",
    "\n",
    "            predictions[touchU] = (price >= upper)\n",
    "            predictions[touchL] = (price <= lower)\n",
    "            predictions[inBand] = (~predictions[touchU] & ~predictions[touchL])\n",
    "\n",
    "            # 4) True \"cross\" events (crossed today vs yesterday)\n",
    "            prev_price = price.shift(1)\n",
    "            prev_up    = upper.shift(1)\n",
    "            prev_low   = lower.shift(1)\n",
    "\n",
    "            # Cross above upper: was <= upper yesterday and > upper today\n",
    "            predictions[crossU] = (prev_price <= prev_up) & (price > upper)\n",
    "            # Cross below lower: was >= lower yesterday and < lower today\n",
    "            predictions[crossL] = (prev_price >= prev_low) & (price < lower)\n",
    "\n",
    "            # 5) Compact signal: +1 if price above upper, -1 if below lower, else 0\n",
    "            predictions[sig_col] = np.select(\n",
    "                [predictions[touchU], predictions[touchL]],\n",
    "                [1, -1],\n",
    "                default=0\n",
    "            ).astype(int)\n",
    "\n",
    "            # 6) Cumulative counts you can tally quickly\n",
    "            predictions[countU]   = predictions[crossU].cumsum()\n",
    "            predictions[countL]   = predictions[crossL].cumsum()\n",
    "            predictions[countAny] = (predictions[crossU] | predictions[crossL]).cumsum()\n",
    "\n",
    "            # 7) Optional: rolling 20-day counts if you want “recent frequency”\n",
    "            predictions[f'{index}_roll20_breaks_{model}'] = (\n",
    "                (predictions[crossU] | predictions[crossL]).rolling(20, min_periods=1).sum()\n",
    "            )\n",
    "\n",
    "            # 8) Band width (relative, helpful for diagnostics/screening)\n",
    "            predictions[widthCol] = (upper - lower) / predictions[sma_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a58e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def forward_returns(price, horizons=(1,3,5,10)):\n",
    "    \"\"\"Calculates forward log returns for given horizons.\"\"\"\n",
    "    out = {}\n",
    "    lp = np.log(price.astype(float))\n",
    "    for h in horizons:\n",
    "        out[h] = (lp.shift(-h) - lp)      # r_{t→t+h}, aligned at t\n",
    "    return pd.DataFrame(out, index=price.index)\n",
    "\n",
    "def reenter_band_within(pred_df, idx, mdl, horizon=5):\n",
    "    \"\"\"Boolean: did price re-enter band within 'horizon' days after being outside?\"\"\"\n",
    "    px     = pred_df[f\"{idx}_Close\"]\n",
    "    up     = pred_df[f\"{idx}_upper_boll_{mdl}\"]\n",
    "    low    = pred_df[f\"{idx}_lower_boll_{mdl}\"]\n",
    "    outside = (px > up) | (px < low)\n",
    "    inside  = ~(outside)\n",
    "    # rolling forward \"any inside\" within next N days (including next day)\n",
    "    # Build a forward-looking window using shift(-k). We’ll OR across 1..N.\n",
    "    any_inside_nextN = pd.Series(False, index=pred_df.index)\n",
    "    for k in range(1, horizon+1):\n",
    "        any_inside_nextN = any_inside_nextN | inside.shift(-k)\n",
    "    return outside & any_inside_nextN\n",
    "\n",
    "def summarize_boll_stats(predictions, indexes, models, horizons=(1,3,5,10), reenter_N=5):\n",
    "    \"\"\"\n",
    "    Calculates various statistics for Bollinger Band signals, including hit rates,\n",
    "    mean returns, and average win/loss amounts.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for idx in indexes:\n",
    "        px = predictions[f\"{idx}_Close\"]\n",
    "        fwd = forward_returns(px, horizons)\n",
    "        for mdl in models:\n",
    "            # Columns built earlier\n",
    "            up     = predictions[f\"{idx}_upper_boll_{mdl}\"]\n",
    "            low    = predictions[f\"{idx}_lower_boll_{mdl}\"]\n",
    "            touchU = predictions[f\"{idx}_touch_upper_{mdl}\"].astype(bool)\n",
    "            touchL = predictions[f\"{idx}_touch_lower_{mdl}\"].astype(bool)\n",
    "            crossU = predictions[f\"{idx}_cross_above_upper_{mdl}\"].astype(bool)\n",
    "            crossL = predictions[f\"{idx}_cross_below_lower_{mdl}\"].astype(bool)\n",
    "            inside = predictions[f\"{idx}_inside_band_{mdl}\"].astype(bool)\n",
    "            width  = (up - low) / predictions[f\"{idx}_sma\"]\n",
    "\n",
    "            n = len(px.dropna())\n",
    "            # base rates\n",
    "            pct_inside = inside.mean()\n",
    "            pct_touchU = touchU.mean()\n",
    "            pct_touchL = touchL.mean()\n",
    "            pct_crossU = crossU.mean()\n",
    "            pct_crossL = crossL.mean()\n",
    "\n",
    "            # --- NEW: Define more complex signals ---\n",
    "            # Bearish: Touched upper band yesterday, and today it's back inside.\n",
    "            mean_revert_U = touchU.shift(1) & inside\n",
    "            # Bullish: Touched lower band yesterday, and today it's back inside.\n",
    "            mean_revert_L = touchL.shift(1) & inside\n",
    "            # --- END NEW ---\n",
    "\n",
    "            # forward return frames aligned to signal dates\n",
    "            sigs = {\n",
    "                \"touchU\": touchU,\n",
    "                \"touchL\": touchL,\n",
    "                \"crossU\": crossU,\n",
    "                \"crossL\": crossL,\n",
    "                \"mean_revert_U\": mean_revert_U, # NEW\n",
    "                \"mean_revert_L\": mean_revert_L, # NEW\n",
    "            }\n",
    "\n",
    "            # re-entry (mean reversion back inside band) within N days after being outside\n",
    "            reenter = reenter_band_within(predictions, idx, mdl, horizon=reenter_N)\n",
    "            pct_reenter_after_outside = reenter.mean()\n",
    "\n",
    "            row = {\n",
    "                \"index\": idx,\n",
    "                \"model\": mdl,\n",
    "                \"obs\": n,\n",
    "                \"% inside\": pct_inside,\n",
    "                \"% touchU\": pct_touchU,\n",
    "                \"% touchL\": pct_touchL,\n",
    "                \"% crossU\": pct_crossU,\n",
    "                \"% crossL\": pct_crossL,\n",
    "                \"band_width_med%\": float(np.nanmedian(width))*100.0,\n",
    "                \"band_width_p90%\": float(np.nanpercentile(width.dropna(), 90))*100.0,\n",
    "                f\"% reenter≤{reenter_N}d after outside\": pct_reenter_after_outside,\n",
    "            }\n",
    "\n",
    "            # Hit rates & conditional forward returns\n",
    "            for name, mask in sigs.items():\n",
    "                m = mask.fillna(False)\n",
    "                idx_sig = m[m].index\n",
    "                if len(idx_sig) == 0:\n",
    "                    # fill NaNs for empty signals\n",
    "                    row.update({f\"{name}_n\": 0})\n",
    "                    for h in horizons:\n",
    "                        row.update({\n",
    "                            f\"{name}_hit{h}d%\": np.nan,\n",
    "                            f\"{name}_mean{h}d(bp)\": np.nan,\n",
    "                            f\"{name}_med{h}d(bp)\": np.nan,\n",
    "                            f\"{name}_avg_win{h}d(bp)\": np.nan,\n",
    "                            f\"{name}_avg_loss{h}d(bp)\": np.nan,\n",
    "                        })\n",
    "                    continue\n",
    "\n",
    "                row[f\"{name}_n\"] = int(len(idx_sig))\n",
    "                # Direction for \"hit\": breakout (crossU) expects +; crossL expects −; touches & mean-revert assumption\n",
    "                for h in horizons:\n",
    "                    fr = fwd[h].reindex(idx_sig)  # log fwd return\n",
    "                    if name in (\"crossU\", \"touchL\", \"mean_revert_L\"):\n",
    "                        # bullish expectation\n",
    "                        hit = (fr > 0)\n",
    "                    elif name in (\"crossL\", \"touchU\", \"mean_revert_U\"):\n",
    "                        # bearish expectation\n",
    "                        hit = (fr < 0)\n",
    "                    else:\n",
    "                        hit = fr > 0\n",
    "\n",
    "                    # Calculate Average Win and Loss\n",
    "                    wins = fr[hit]\n",
    "                    losses = fr[~hit] # '~' is the boolean 'not' operator for pandas Series\n",
    "\n",
    "                    avg_win_bp = (wins.mean() * 1e4) if not wins.empty else np.nan\n",
    "                    avg_loss_bp = (losses.mean() * 1e4) if not losses.empty else np.nan\n",
    "\n",
    "                    row[f\"{name}_hit{h}d%\"]   = float(hit.mean())\n",
    "                    # report in basis points for readability\n",
    "                    row[f\"{name}_mean{h}d(bp)\"] = float(fr.mean() * 1e4)\n",
    "                    row[f\"{name}_med{h}d(bp)\"]  = float(fr.median() * 1e4)\n",
    "                    row[f\"{name}_avg_win{h}d(bp)\"] = float(avg_win_bp)\n",
    "                    row[f\"{name}_avg_loss{h}d(bp)\"] = float(avg_loss_bp)\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    # Nice ordering\n",
    "    base_cols = [\"index\",\"model\",\"obs\",\"% inside\",\"% touchU\",\"% touchL\",\"% crossU\",\"% crossL\",\n",
    "                 \"band_width_med%\",\"band_width_p90%\", f\"% reenter≤{reenter_N}d after outside\"]\n",
    "    # We’ll just sort the remaining metric columns alphabetically\n",
    "    metric_cols = [c for c in out.columns if c not in base_cols]\n",
    "    out = out[base_cols + sorted(metric_cols)]\n",
    "    return out\n",
    "\n",
    "# ---- Run it ----\n",
    "set_boll_counts()\n",
    "predictions_train, predictions_test = split_data(predictions)\n",
    "boll_stats_train = summarize_boll_stats(predictions_train, indexes, models, horizons=(1,3,5,10), reenter_N=5)\n",
    "boll_stats_test = summarize_boll_stats(predictions_test, indexes, models, horizons=(1,3,5,10), reenter_N=5)\n",
    "\n",
    "# Example: filter to signals with decent sample size and view\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"--- Training Set Statistics ---\")\n",
    "display(boll_stats_train.sort_values([\"index\",\"model\"]))\n",
    "\n",
    "print(\"\\n--- Test Set Statistics ---\")\n",
    "display(boll_stats_test.sort_values([\"index\",\"model\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b549110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create backtest df copy\n",
    "bt = predictions.copy()\n",
    "\n",
    "# Set initial conditions\n",
    "starting_cash = 10000\n",
    "bt_data = {}\n",
    "\n",
    "for index in indexes:\n",
    "    bt_data[index] = {}\n",
    "\n",
    "    for model in models:\n",
    "        bt[f'{index}_portfolio_value_{model}'] = 0\n",
    "        bt[f'{index}_cash_{model}'] = 0\n",
    "        bt[f'{index}_cash_{model}'].iloc[0] = starting_cash\n",
    "        bt[f'{index}_signal_{model}'] = 0\n",
    "\n",
    "        bt_data[index][model] = {\n",
    "            'last_value': 0,\n",
    "            'last_cash': starting_cash,\n",
    "            'hold': 0\n",
    "        }\n",
    "\n",
    "# Starts at second day\n",
    "for idx, row in bt.iloc[1:].iterrows():\n",
    "    for index in indexes:\n",
    "        for model in models:\n",
    "            last_value = bt_data[index][model]['last_value']\n",
    "            last_cash = bt_data[index][model]['last_cash']\n",
    "\n",
    "            close = row[f'{index}_Close']\n",
    "\n",
    "            mu = np.exp(row[f'{index}_next_ret_pred_ridge'])\n",
    "            vol = row[f'{index}_std_pred_{model}']\n",
    "            rf = (1+row['ten_yr'])**(1/252)-1 # Daily\n",
    "            investment = 0\n",
    "\n",
    "            # Update portfolio value\n",
    "            current_value = last_value * np.exp(row[f'{index}_log_ret'])\n",
    "\n",
    "            # Calculate total equity\n",
    "            total_equity = current_value + last_cash \n",
    "\n",
    "            # Get bet sizing with Merton\n",
    "            #target_pos = get_merton_size(mu, rf, vol) * total_equity # Daily rf\n",
    "            #target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "\n",
    "            # Get Kelly criterion\n",
    "            if bt_data[index][model]['hold'] <= 0:\n",
    "                if row[f'{index}_cross_above_upper_{model}']:\n",
    "                    p = boll_stats_train.loc[(boll_stats_train['index'] == index) & (boll_stats_train['model'] == model), 'crossU_hit3d%'].item()\n",
    "                    q = 1 - p\n",
    "\n",
    "                    avg_win = boll_stats_train.loc[(boll_stats_train['index'] == index) & (boll_stats_train['model'] == model), 'crossU_avg_win3d(bp)'].item()\n",
    "                    avg_loss = boll_stats_train.loc[(boll_stats_train['index'] == index) & (boll_stats_train['model'] == model), 'crossU_avg_loss3d(bp)'].item()\n",
    "                    \n",
    "                    b = abs(avg_win / avg_loss)\n",
    "                    f = min(max(0, get_kelly_criterion(b, p, q)),1)\n",
    "                    target_pos = f * total_equity * .5\n",
    "\n",
    "                    bt_data[index][model]['hold'] = 3\n",
    "                elif row[f'{index}_cross_below_lower_{model}']:\n",
    "                    p = boll_stats_train.loc[(boll_stats_train['index'] == index) & (boll_stats_train['model'] == model), 'crossL_hit3d%'].item()\n",
    "                    q = 1 - p\n",
    "\n",
    "                    avg_win = boll_stats_train.loc[(boll_stats_train['index'] == index) & (boll_stats_train['model'] == model), 'crossL_avg_win3d(bp)'].item()\n",
    "                    avg_loss = boll_stats_train.loc[(boll_stats_train['index'] == index) & (boll_stats_train['model'] == model), 'crossL_avg_loss3d(bp)'].item()\n",
    "                    \n",
    "                    b = abs(avg_win / avg_loss)\n",
    "                    f = -min(max(0, get_kelly_criterion(b, p, q)),1)\n",
    "                    target_pos = f * total_equity * .5\n",
    "\n",
    "                    bt_data[index][model]['hold'] = 3\n",
    "                else:\n",
    "                    target_pos = 0\n",
    "            else:\n",
    "                bt_data[index][model]['hold'] -= 1\n",
    "                if bt_data[index][model]['hold'] == 0:\n",
    "                    target_pos = 0\n",
    "\n",
    "\n",
    "            # Get bet sizing with vol target\n",
    "            #target_pos = get_vol_target_sizing(.1/np.sqrt(252), vol) * total_equity\n",
    "            #target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "            \n",
    "            # Hybrid - Scale it with signed confidence from mu\n",
    "            #base_weight = get_vol_target_sizing(0.1 / np.sqrt(252), vol)\n",
    "            #k=1\n",
    "            #confidence = (mu - rf) / (vol**2 + EPS) # Normalize mu to daily risk-free rate or historical mean\n",
    "            #target_weight = base_weight * np.tanh(confidence * k)  # `k` is a scaling factor\n",
    "            #target_pos = target_weight * total_equity\n",
    "            #target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "\n",
    "            # Bet sizing based on size of potential increase/decrease, signed\n",
    "            #target_pos = (mu - rf)/abs(mu - rf) * total_equity # super leverage\n",
    "            #target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "\n",
    "            # Execute trade / Update investment amount to reach target position\n",
    "            investment = target_pos - current_value\n",
    "\n",
    "            # Update cash\n",
    "            last_cash = bt_data[index][model]['last_cash'] = last_cash - investment\n",
    "            bt.loc[idx, f'{index}_cash_{model}'] = last_cash\n",
    "\n",
    "            # Store portfolio value as last value for calculation\n",
    "            bt_data[index][model]['last_value'] = current_value + investment\n",
    "            bt.loc[idx, f'{index}_portfolio_value_{model}'] = bt_data[index][model]['last_value']\n",
    "\n",
    "            # Set signal for plotting later\n",
    "            if investment > 0:\n",
    "                bt.loc[idx, f'{index}_signal_{model}'] = 1\n",
    "            elif investment < 0:\n",
    "                bt.loc[idx, f'{index}_signal_{model}'] = -1\n",
    "    \n",
    "for index in indexes:\n",
    "    for model in models:\n",
    "        bt[f'{index}_total_value_{model}'] = bt[f'{index}_portfolio_value_{model}'] + bt[f'{index}_cash_{model}']\n",
    "        final_value = bt[f'{index}_total_value_{model}'].iloc[-1]\n",
    "        print(f'Final value for {index}, {model}: {final_value:.2f}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e83a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(bt, 'baseline_wealth', ['total_value'], 'Backtest Performance', 'Wealth', indexes, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c9c1ac",
   "metadata": {},
   "source": [
    "# MS-Garch Volatility Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04dea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.regime_switching.markov_regression import MarkovRegression\n",
    "from arch.univariate import ConstantMean, GARCH, StudentsT\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d02d26c",
   "metadata": {},
   "source": [
    "Test pulled from GPT (python doesn't have a MS-GARCH library, only separate implementations with reduced functionality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf5d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSGARCH:\n",
    "    \"\"\"\n",
    "    Markov switching means & transitions via statsmodels.MarkovRegression,\n",
    "    per-state GARCH(1,1) via arch; combine with Gray (1996) approximation.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=2, thresh=0.6, max_iter=1, verbose=False, dist='normal'):\n",
    "        self.k = int(k)\n",
    "        self.thresh = float(thresh)  # posterior cutoff for state-specific GARCH fit\n",
    "        self.max_iter = int(max_iter)\n",
    "        self.verbose = verbose\n",
    "        self.dist = dist.lower()\n",
    "        assert self.dist in (\"normal\", \"t\"), \"dist must be 'normal' or 't'\"\n",
    "        self.nu_ = None   # will hold per-state dof if dist='t'\n",
    "\n",
    "    def fit(self, r: pd.Series | np.ndarray):\n",
    "        r = np.asarray(pd.Series(r).astype(float).dropna())\n",
    "        T = len(r)\n",
    "        assert T > 50, \"Need enough data\"\n",
    "\n",
    "        # 1) HMM on returns with state-dependent intercept (mean) and variance\n",
    "        #    (statsmodels allows switching_variance=True, which helps separate regimes)\n",
    "        mod = MarkovRegression(r, k_regimes=self.k,\n",
    "                               trend='c',  # intercept per regime\n",
    "                               switching_variance=True)\n",
    "        res = mod.fit(disp=False)\n",
    "        self._mr_mod_ = mod\n",
    "        self._mr_res_ = res\n",
    "\n",
    "        # --- 1) Extract regime-specific means (intercepts) ---\n",
    "        names = np.asarray(res.model.param_names)\n",
    "        vals  = np.asarray(res.params)\n",
    "\n",
    "        mask = (np.char.find(names, 'intercept') >= 0)\n",
    "        if not mask.any():\n",
    "            mask = (np.char.find(names, 'const') >= 0)\n",
    "\n",
    "        intercept_names = names[mask]\n",
    "        intercept_vals  = vals[mask]\n",
    "\n",
    "        pairs = []\n",
    "        for n, v in zip(intercept_names, intercept_vals):\n",
    "            m = re.search(r'\\[(\\d+)\\]', n)\n",
    "            idx = int(m.group(1)) if m else 10**9\n",
    "            pairs.append((idx, float(v)))\n",
    "        pairs.sort(key=lambda t: t[0])\n",
    "        mu = np.array([p[1] for p in pairs])  # length K\n",
    "\n",
    "        # --- 2) Transition matrix ---\n",
    "        P_left = mod.regime_transition_matrix(res.params)  # columns sum to 1\n",
    "        P = P_left.T                                      # rows sum to 1, matches our code convention\n",
    "\n",
    "        # --- 3) Smoothed posteriors ---\n",
    "        gamma_obj = res.smoothed_marginal_probabilities\n",
    "        if hasattr(gamma_obj, \"to_numpy\"):\n",
    "            gamma = gamma_obj.to_numpy()\n",
    "        else:\n",
    "            gamma = np.asarray(gamma_obj)                     # shape (T, K)\n",
    "\n",
    "        # 2) Fit GARCH(1,1) separately for each regime using high-probability times\n",
    "        self._arch_results_ = []\n",
    "        garch_params = []\n",
    "        nus = []\n",
    "        for k in range(self.k):\n",
    "            mask = (gamma[:, k] == gamma.max(1)) & (gamma[:, k] >= self.thresh)\n",
    "            idx = np.where(mask)[0]\n",
    "            if len(idx) < 30:\n",
    "                idx = np.argsort(gamma[:, k])[-max(30, T // self.k):]\n",
    "            r_k = r[idx] - mu[k]  # center by state mean\n",
    "\n",
    "            am = ConstantMean(r_k)\n",
    "            am.volatility = GARCH(1, 0, 1)\n",
    "            if self.dist == \"t\":\n",
    "                am.distribution = StudentsT()\n",
    "            res_k = am.fit(disp=\"off\")\n",
    "\n",
    "            self._arch_results_.append(res_k)\n",
    "            v = res_k.params\n",
    "            omega = float(v[\"omega\"])\n",
    "            alpha = float(v[\"alpha[1]\"])\n",
    "            beta  = float(v[\"beta[1]\"])\n",
    "\n",
    "            # (optional) safety clamp on persistence\n",
    "            s = max(alpha + beta, 1e-6)\n",
    "            if s >= 0.998:\n",
    "                shrink = (s / 0.98)\n",
    "                alpha /= shrink; beta /= shrink\n",
    "\n",
    "            garch_params.append((omega, alpha, beta))\n",
    "\n",
    "            if self.dist == \"t\":\n",
    "                # ARCH names this 'nu'\n",
    "                nu = float(v[\"nu\"])\n",
    "                # ensure nu > 2 so variance exists\n",
    "                nu = max(nu, 2.01)\n",
    "                nus.append(nu)\n",
    "\n",
    "        garch_params = np.array(garch_params)\n",
    "        omega = garch_params[:, 0]; alpha = garch_params[:, 1]; beta = garch_params[:, 2]\n",
    "        self.omega_, self.alpha_, self.beta_ = omega, alpha, beta\n",
    "        self.nu_ = np.array(nus) if self.dist == \"t\" else None\n",
    "\n",
    "        # 3) Gray recursion using gamma as pi_t\n",
    "        h, m_mix, h_mix = self._gray_recursion(r, mu, omega, alpha, beta, gamma)\n",
    "\n",
    "        # (Optional) one extra iteration: re-center residuals with mixture mean and refit GARCH\n",
    "        for it in range(self.max_iter):\n",
    "            if self.max_iter <= 1: break\n",
    "            # reselect by posterior again (same gamma) but use residuals r - mu_k\n",
    "            new_params = []\n",
    "            for k in range(self.k):\n",
    "                mask = (gamma[:, k] == gamma.max(1)) & (gamma[:, k] >= self.thresh)\n",
    "                idx = np.where(mask)[0]\n",
    "                if len(idx) < 30:\n",
    "                    idx = np.argsort(gamma[:, k])[-max(30, T // self.k):]\n",
    "                r_k = r[idx] - mu[k]\n",
    "                am = ConstantMean(r_k)\n",
    "                am.volatility = GARCH(1, 0, 1)\n",
    "                res_k = am.fit(disp='off')\n",
    "                self._arch_results_[k] = res_k\n",
    "                v = res_k.params\n",
    "                omega = float(v['omega']); alpha = float(v['alpha[1]']); beta = float(v['beta[1]'])\n",
    "                s = max(alpha + beta, 1e-6)\n",
    "                if s >= 0.998:\n",
    "                    shrink = (s / 0.98)\n",
    "                    alpha /= shrink; beta /= shrink\n",
    "                new_params.append((omega, alpha, beta))\n",
    "            new_params = np.array(new_params)\n",
    "            omega, alpha, beta = new_params[:,0], new_params[:,1], new_params[:,2]\n",
    "            h, m_mix, h_mix = self._gray_recursion(r, mu, omega, alpha, beta, gamma)\n",
    "\n",
    "        # Store\n",
    "        self.mu_ = mu\n",
    "        self.P_ = P\n",
    "        self.gamma_ = gamma\n",
    "        self.h_ = h                 # (T,K) per-state conditional variances\n",
    "        self.h_mix_ = h_mix         # (T,) mixture variance\n",
    "        self.m_mix_ = m_mix         # (T,) mixture mean\n",
    "        self.omega_ = omega; self.alpha_ = alpha; self.beta_ = beta\n",
    "        self.r_ = r\n",
    "        return self\n",
    "\n",
    "    def _gray_recursion(self, r, mu, omega, alpha, beta, pi):\n",
    "        T = len(r); K = len(mu)\n",
    "        h = np.zeros((T, K))\n",
    "        m_mix = np.zeros(T)\n",
    "        H_mix = np.zeros(T)\n",
    "\n",
    "        # initialize with unconditional per state\n",
    "        den = np.maximum(1.0 - (alpha + beta), 1e-3)\n",
    "        h0 = omega / den\n",
    "        h[0] = np.maximum(h0, 1e-8)\n",
    "\n",
    "        m_mix[0] = (pi[0] @ mu)\n",
    "        H_mix[0] = (pi[0] @ h[0])\n",
    "\n",
    "        for t in range(1, T):\n",
    "            m_mix[t] = pi[t-1] @ mu\n",
    "            H_mix[t] = pi[t-1] @ h[t-1]\n",
    "            innov2 = (r[t-1] - m_mix[t])**2\n",
    "            h[t] = omega + alpha * innov2 + beta * H_mix[t]\n",
    "            h[t] = np.maximum(h[t], 1e-12)\n",
    "\n",
    "        h_mix = (pi * h).sum(1)\n",
    "        return h, m_mix, h_mix\n",
    "\n",
    "    # One-step-ahead (no new data)\n",
    "    def predict_next(self):\n",
    "        r_T = self.r_[-1]\n",
    "        m_T = self.m_mix_[-1]\n",
    "        H_T = self.h_mix_[-1]\n",
    "        pi_T = self.gamma_[-1]\n",
    "        mu   = self.mu_\n",
    "        omega, alpha, beta = self.omega_, self.alpha_, self.beta_\n",
    "        h_next = omega + alpha * (r_T - m_T)**2 + beta * H_T\n",
    "        h_next = np.maximum(h_next, 1e-12)\n",
    "        pi_next = pi_T @ self.P_\n",
    "        mean_next = float(pi_next @ mu)\n",
    "        var_next  = float(pi_next @ h_next)\n",
    "        return {\"pi_next\": pi_next, \"h_next_per_state\": h_next,\n",
    "                \"mean_next\": mean_next, \"var_next\": var_next}\n",
    "\n",
    "    def _loglik_state(self, r_new, mu_k, h_k, nu_k=None):\n",
    "        \"\"\"\n",
    "        Log-likelihood of r_new under state k with mean mu_k, variance h_k,\n",
    "        using Normal or Student-t (df=nu_k) innovations.\n",
    "        \"\"\"\n",
    "        h = max(h_k, 1e-12)\n",
    "        x = r_new - mu_k\n",
    "        if self.dist == \"normal\":\n",
    "            # N(mu, h)\n",
    "            return -0.5 * (math.log(2*math.pi*h) + (x*x)/h)\n",
    "        else:\n",
    "            # Student-t with df=nu, using *standard* t scaled so that Var = h\n",
    "            # Standard t(df) with scale s has pdf: log Γ((ν+1)/2) - log(√(νπ) s Γ(ν/2))\n",
    "            #                                  - (ν+1)/2 * log(1 + ((x)/s)^2 / ν)\n",
    "            nu = max(float(nu_k), 2.01)\n",
    "            # choose scale so that Var(X)=h -> s = sqrt(h * ν/(ν-2))\n",
    "            s = math.sqrt(h * nu/(nu-2.0))\n",
    "            z2 = (x/s)**2\n",
    "            return (\n",
    "                math.lgamma((nu+1)/2.0)\n",
    "                - math.lgamma(nu/2.0)\n",
    "                - 0.5*math.log(nu*math.pi) - math.log(s)\n",
    "                - 0.5*(nu+1.0)*math.log(1.0 + z2/nu)\n",
    "            )\n",
    "\n",
    "    # Online filter step (fixed params)\n",
    "    def filter_update(self, r_new):\n",
    "        r_new = float(r_new)\n",
    "        fc = self.predict_next()\n",
    "        h_next = fc[\"h_next_per_state\"]     # per-state conditional variances for the new time\n",
    "        pi_pred = fc[\"pi_next\"]\n",
    "        mu = self.mu_\n",
    "\n",
    "        # per-state log-likelihood\n",
    "        loglik = np.empty(self.k)\n",
    "        for k in range(self.k):\n",
    "            nu_k = None if self.dist == \"normal\" else self.nu_[k]\n",
    "            loglik[k] = self._loglik_state(r_new, mu[k], h_next[k], nu_k=nu_k)\n",
    "\n",
    "        # stabilize\n",
    "        loglik -= loglik.max()\n",
    "        lik = np.exp(loglik)\n",
    "\n",
    "        pi_post = pi_pred * lik\n",
    "        pi_post = pi_post / pi_post.sum()\n",
    "\n",
    "        # append updated series\n",
    "        self.r_ = np.append(self.r_, r_new)\n",
    "        self.gamma_ = np.vstack([self.gamma_, pi_post])\n",
    "\n",
    "        mix_var = float(pi_post @ h_next)\n",
    "        mix_mean = float(pi_post @ mu)\n",
    "        self.h_mix_ = np.append(self.h_mix_, mix_var)\n",
    "        self.m_mix_ = np.append(self.m_mix_, mix_mean)\n",
    "\n",
    "        return {\"pi_post\": pi_post, \"mix_mean\": mix_mean, \"mix_var\": mix_var}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e310645",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_garch_data = vol_data.copy()\n",
    "\n",
    "training_data, testing_data = split_data(ms_garch_data)\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "garches = {}\n",
    "\n",
    "\n",
    "for index in indexes:\n",
    "    r = np.exp(training_data[f'{index}_log_squared_residual'])\n",
    "    garches[index] = MSGARCH(k=3, thresh=0.8, max_iter=1, verbose=True, dist='t')\n",
    "    garches[index].fit(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create backtest df copy\n",
    "bt = predictions.copy()\n",
    "\n",
    "# Set initial conditions\n",
    "starting_cash = 10000\n",
    "bt_data = {}\n",
    "\n",
    "# Model\n",
    "model_name = 'MS_GARCH'\n",
    "\n",
    "trade_futures = False\n",
    "futures_columns = '_F' if trade_futures else ''\n",
    "\n",
    "for index in indexes:\n",
    "    bt_data[index] = {}\n",
    "\n",
    "\n",
    "    bt[f'{index}_portfolio_value_{model_name}'] = 0\n",
    "    bt[f'{index}_cash_{model_name}'] = 0\n",
    "    bt[f'{index}_cash_{model_name}'].iloc[0] = starting_cash\n",
    "    bt[f'{index}_signal_{model_name}'] = 0\n",
    "\n",
    "    bt_data[index][model_name] = {\n",
    "        'last_value': 0,\n",
    "        'last_cash': starting_cash\n",
    "    }\n",
    "\n",
    "# Starts at second day\n",
    "for idx, row in bt.iloc[1:].iterrows():\n",
    "    for index in indexes:\n",
    "        last_value = bt_data[index][model_name]['last_value']\n",
    "        last_cash = bt_data[index][model_name]['last_cash']\n",
    "\n",
    "        close = row[f'{index}_Close']\n",
    "        rf = (1+row['ten_yr'])**(1/252)-1 # Daily\n",
    "\n",
    "        div_dict = {'S&P': 'SPY', 'NASDAQ': 'QQQ', 'DJIA': 'DIA'}\n",
    "        div_name = f'{div_dict[index]}_div'\n",
    "        div = (1+row[div_name])**(1/252)-1 # Daily\n",
    "\n",
    "        fc = garches[index].predict_next()\n",
    "        garches[index].filter_update(row[f'{index}{futures_columns}_log_ret'])\n",
    "\n",
    "        #mu = (fc['mean_next']/100) / close - 1 # Use garch prediction for returns\n",
    "        mu = row[f'{index}_next_ret_pred_enet'] / close - 1 # Use our regression model prediction for returns\n",
    "        #mu = rf - div # Use risk free assumption\n",
    "        vol = np.sqrt(fc['var_next'])\n",
    "        \n",
    "        \n",
    "        investment = 0\n",
    "\n",
    "        # Update portfolio value\n",
    "        current_value = last_value * np.exp(row[f'{index}{futures_columns}_log_ret'])\n",
    "\n",
    "        # Calculate total equity\n",
    "        total_equity = current_value + last_cash \n",
    "\n",
    "        # Get bet sizing with Merton-Kelly\n",
    "        #target_pos = .5 * get_merton_kelly_size(mu, rf, vol) * total_equity # Daily rf\n",
    "        #target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "\n",
    "        # Get bet sizing with vol target\n",
    "        target_pos = get_vol_target_sizing(.1/np.sqrt(252), vol) * total_equity\n",
    "        target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "        \n",
    "        # Bet sizing based on size of potential increase/decrease, signed\n",
    "        #target_pos = (mu - rf)/abs(mu - rf) * total_equity # super leverage\n",
    "        #target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "\n",
    "        # Execute trade / Update investment amount to reach target position\n",
    "        investment = target_pos - current_value\n",
    "\n",
    "        # Update cash\n",
    "        last_cash = bt_data[index][model_name]['last_cash'] = last_cash - investment\n",
    "        bt.loc[idx, f'{index}_cash_{model_name}'] = last_cash\n",
    "\n",
    "        # Store portfolio value as last value for calculation\n",
    "        bt_data[index][model_name]['last_value'] = current_value + investment\n",
    "        bt.loc[idx, f'{index}_portfolio_value_{model_name}'] = bt_data[index][model_name]['last_value']\n",
    "\n",
    "        # Set signal for plotting later\n",
    "        if investment > 0:\n",
    "            bt.loc[idx, f'{index}_signal_{model_name}'] = 1\n",
    "        elif investment < 0:\n",
    "            bt.loc[idx, f'{index}_signal_{model_name}'] = -1\n",
    "\n",
    "    \n",
    "for index in indexes:\n",
    "    bt[f'{index}_total_value_{model_name}'] = bt[f'{index}_portfolio_value_{model_name}'] + bt[f'{index}_cash_{model_name}']\n",
    "    final_value = bt[f'{index}_total_value_{model_name}'].iloc[-1]\n",
    "    print(f'Final value for {index}, {model_name}: {final_value:.2f}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40987d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in indexes:\n",
    "    bt[f'{index}_baseline'] = np.exp(bt[f'{index}_log_ret'].cumsum()) * 10000\n",
    "\n",
    "colors = { 'MS_GARCH': 'red' }\n",
    "\n",
    "# --- Plotting Code ---\n",
    "\n",
    "# Initialize a figure with a row for each index\n",
    "fig = make_subplots(\n",
    "    rows=len(indexes),\n",
    "    cols=1,\n",
    "    subplot_titles=[f'{index} Model Performance' for index in indexes],\n",
    "    shared_xaxes=True # Link the x-axes\n",
    ")\n",
    "\n",
    "# Enumerate through indexes to get the row number (i)\n",
    "for i, index in enumerate(indexes):\n",
    "    # Add the baseline trace for the current index\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=bt.index,\n",
    "        y=bt[f'{index}_baseline'],\n",
    "        mode='lines',\n",
    "        name=f'{index} Baseline',\n",
    "        legendgroup=f'group{i}', # Group legend items by subplot\n",
    "        line=dict(color='blue', width=1)\n",
    "    ), row=i + 1, col=1) # row is 1-indexed\n",
    "\n",
    "    # Add the main performance line for the model\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=bt.index,\n",
    "        y=bt[f'{index}_total_value_{model_name}'],\n",
    "        mode='lines',\n",
    "        name=f'{index} {model_name}',\n",
    "        legendgroup=f'group{i}',\n",
    "        line=dict(color=colors[model_name], width=1.5, dash='dot')\n",
    "    ), row=i + 1, col=1)\n",
    "\n",
    "    # --- Corrected Signal Plotting ---\n",
    "\n",
    "    # Create Series for buy/sell signals\n",
    "    # This puts the portfolio value on the y-axis for the marker, and NaN otherwise\n",
    "    buy_signals_y = bt.loc[bt[f'{index}_signal_{model_name}'] == 1, f'{index}_total_value_{model_name}']\n",
    "    sell_signals_y = bt.loc[bt[f'{index}_signal_{model_name}'] == -1, f'{index}_total_value_{model_name}']\n",
    "\n",
    "\"\"\" These are too crowded\n",
    "        # Add BUY signal markers\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=buy_signals_y.index,\n",
    "            y=buy_signals_y,\n",
    "            mode='markers',\n",
    "            name=f'Buy Signal',\n",
    "            legendgroup=f'group{i}',\n",
    "            marker=dict(size=10, symbol='triangle-up', color='green'),\n",
    "            showlegend=False # Hide from legend to avoid clutter\n",
    "        ), row=i + 1, col=1)\n",
    "\n",
    "        # Add SELL signal markers\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=sell_signals_y.index,\n",
    "            y=sell_signals_y,\n",
    "            mode='markers',\n",
    "            name=f'Sell Signal',\n",
    "            legendgroup=f'group{i}',\n",
    "            marker=dict(size=10, symbol='triangle-down', color='red'),\n",
    "            showlegend=False # Hide from legend to avoid clutter\n",
    "        ), row=i + 1, col=1)\n",
    "        \"\"\"\n",
    "\n",
    "# --- Update the Layout ---\n",
    "fig.update_layout(\n",
    "    title_text='Backtest Performance: Model Comparison by Index',\n",
    "    title_x=0.5, # Center the title\n",
    "    legend_title='Metrics',\n",
    "    height=800 # Adjust height to make plots more readable\n",
    ")\n",
    "\n",
    "# --- Display the Chart ---\n",
    "fig.show()\n",
    "\n",
    "cutoff = math.floor(len(bt)*.8)\n",
    "training_returns = bt.iloc[:cutoff]\n",
    "testing_returns = bt.iloc[cutoff:]\n",
    "\n",
    "for index in indexes:\n",
    "    total_value = f'{index}_total_value_{model_name}'\n",
    "    final_training_value = training_returns[total_value].iloc[-1]\n",
    "    training_cagr = 100 * ((final_training_value/training_returns[total_value].iloc[0])**(1/(training_returns.index[-1].year - training_returns.index[0].year + training_returns.index[-1].month/12))-1)\n",
    "    print(f'Final training value for {index}, {model_name}: {final_training_value}, CAGR: {training_cagr:.2f}%')\n",
    "\n",
    "    final_testing_value = testing_returns[total_value].iloc[-1]\n",
    "    testing_cagr = 100*((final_testing_value/testing_returns[total_value].iloc[0])**(1/(testing_returns.index[-1].year - testing_returns.index[0].year + testing_returns.index[-1].month/12))-1)\n",
    "    print(f'Final testing value for {index}, {model_name}: {final_testing_value}, CAGR: {testing_cagr:.2f}%')\n",
    "\n",
    "    # ---- Training metrics ----\n",
    "    train_curve = training_returns[total_value]\n",
    "    train_sharpe, train_mdd = sharpe_and_mdd(training_returns, total_value, col_rf=\"ten_yr\")\n",
    "    print(f\"Training Sharpe: {train_sharpe:.2f}, Max Drawdown: {train_mdd:.2%}\")\n",
    "\n",
    "    # ---- Testing metrics ----\n",
    "    test_curve = testing_returns[total_value]\n",
    "    test_sharpe, test_mdd = sharpe_and_mdd(testing_returns, total_value, col_rf=\"ten_yr\")\n",
    "    print(f\"Testing Sharpe: {test_sharpe:.2f}, Max Drawdown: {test_mdd:.2%}\")\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2f1f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e228d747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
