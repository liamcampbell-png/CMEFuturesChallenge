{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef69f29",
   "metadata": {},
   "source": [
    "# Notebook for CME Futures Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a5936d",
   "metadata": {},
   "source": [
    "### The Rough Idea\n",
    "\n",
    "Model indices as geometric brownian motion (dS/S = mudt + sigmadB)  \n",
    "Model mu (market line) as a linear regression with numerous factors including economic, credit measures, etc  \n",
    "Model sigma as a function of volatility including recent volatility and EMA (decay)  \n",
    "Long/short based on futures mispricings based on our model  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64da73",
   "metadata": {},
   "source": [
    "# Downloading historical data for indices (S&P, NASDAQ, DJIA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f4420",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f8bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8196f13",
   "metadata": {},
   "source": [
    "Make get_data function for downloading from yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe = '1000mo' # set timeframe\n",
    "\n",
    "def get_data(tickers: List):\n",
    "    data_dictionary = {}\n",
    "    for ticker in tickers:\n",
    "        data_dictionary[ticker] = yf.download(ticker, period=timeframe, interval='1d')\n",
    "    return data_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fd38e6",
   "metadata": {},
   "source": [
    "Now let's get data for indices and display with pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78115057",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = ['^GSPC', '^IXIC', '^DJI'] # S&P, NASDAQ, DJIA\n",
    "indexes = ['S&P', 'NASDAQ', 'DJIA'] # Names for reference later\n",
    "\n",
    "etfs = ['SPY', 'QQQ', 'DIA']\n",
    "futures = ['ES=F', 'NQ=F', 'YM=F']\n",
    "\n",
    "data_dictionary = get_data(indices + etfs + futures)\n",
    "\n",
    "#s_p = pd.DataFrame(data_dictionary['^GSPC'])\n",
    "#nasdaq = pd.DataFrame(data_dictionary['^IXIC'])\n",
    "#djia = pd.DataFrame(data_dictionary['^DJI'])\n",
    "\n",
    "s_p = pd.DataFrame(data_dictionary['SPY'])\n",
    "nasdaq = pd.DataFrame(data_dictionary['QQQ'])\n",
    "djia = pd.DataFrame(data_dictionary['DIA'])\n",
    "\n",
    "s_p_F = pd.DataFrame(data_dictionary['ES=F'])\n",
    "nasdaq_F = pd.DataFrame(data_dictionary['NQ=F'])\n",
    "djia_F = pd.DataFrame(data_dictionary['YM=F'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bbf334",
   "metadata": {},
   "source": [
    "We need to flatten this - notice ticker header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4612f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes\n",
    "s_p = s_p.droplevel(1, axis=1)\n",
    "nasdaq = nasdaq.droplevel(1, axis=1)\n",
    "djia = djia.droplevel(1, axis=1)\n",
    "\n",
    "# Futures\n",
    "s_p_F = s_p_F.droplevel(1, axis=1)\n",
    "nasdaq_F = nasdaq_F.droplevel(1, axis=1)\n",
    "djia_F = djia_F.droplevel(1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b7fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2ee66",
   "metadata": {},
   "source": [
    "Let's drop high, low, and open and rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices\n",
    "s_p.drop(columns=['High', 'Low', 'Open'], inplace=True)\n",
    "nasdaq.drop(columns=['High', 'Low', 'Open'], inplace=True)\n",
    "djia.drop(columns=['High', 'Low', 'Open'], inplace=True)\n",
    "\n",
    "s_p = s_p.rename(columns={'Close': 'S&P_Close', 'Volume': 'S&P_Volume'})\n",
    "nasdaq = nasdaq.rename(columns={'Close': 'NASDAQ_Close', 'Volume': 'NASDAQ_Volume'})\n",
    "djia = djia.rename(columns={'Close': 'DJIA_Close', 'Volume': 'DJIA_Volume'})\n",
    "\n",
    "# Futures\n",
    "s_p_F.drop(columns=['High', 'Low', 'Open'], inplace=True)\n",
    "nasdaq_F.drop(columns=['High', 'Low', 'Open'], inplace=True)\n",
    "djia_F.drop(columns=['High', 'Low', 'Open'], inplace=True)\n",
    "\n",
    "s_p_F = s_p_F.rename(columns={'Close': 'S&P_Close', 'Volume': 'S&P_Volume'})\n",
    "nasdaq_F = nasdaq_F.rename(columns={'Close': 'NASDAQ_Close', 'Volume': 'NASDAQ_Volume'})\n",
    "djia_F = djia_F.rename(columns={'Close': 'DJIA_Close', 'Volume': 'DJIA_Volume'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a960dc62",
   "metadata": {},
   "source": [
    "Let's get a quick plot of an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c6444",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(s_p, x=s_p.index, y=\"S&P_Close\", title=\"S&P Daily Past 30 Years\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb7ee6e",
   "metadata": {},
   "source": [
    "# Downloading historical data for our factor model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7547dc9e",
   "metadata": {},
   "source": [
    "We are going to model the index as a geometric brownian motion, with the mu factor being a linear regression model with numerous inputs.  \n",
    "\n",
    "## Factor considerations:  \n",
    "### <u>Term structure</u>\n",
    "###### Term spread (10Y-3M)\n",
    "\n",
    "### <u>Credit conditions</u>\n",
    "###### IG spread (BAA-AAA)\n",
    "\n",
    "### <u>Valuation</u>\n",
    "###### Forward E/P - real 10Y\n",
    "###### Dividend yield\n",
    "\n",
    "### <u>Economic</u>\n",
    "###### Fed funds\n",
    "###### Inflation (CPI)\n",
    "###### DXY change (dollar index)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb92a74",
   "metadata": {},
   "source": [
    "### Some of these we can get from yahoo finance:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13407860",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\n",
    "    # Term structure\n",
    "    '^TNX', # 10yr CBOE\n",
    "    '^IRX', # 3m bill (on discount basis, need to convert to yield)\n",
    "\n",
    "    # Economic\n",
    "    'DX-Y.NYB', # Dollar index\n",
    "]\n",
    "\n",
    "data_dictionary = get_data(tickers)\n",
    "\n",
    "ten_yr = pd.DataFrame(data_dictionary['^TNX']['Close'])\n",
    "three_m = pd.DataFrame(data_dictionary['^IRX']['Close'])\n",
    "dollar_index = pd.DataFrame(data_dictionary['DX-Y.NYB']['Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ae1f78",
   "metadata": {},
   "source": [
    "Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01270a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_yr = ten_yr.rename(columns={'^TNX': 'ten_yr'})\n",
    "three_m = three_m.rename(columns={'^IRX': 'three_m'})\n",
    "dollar_index = dollar_index.rename(columns={'DX-Y.NYB': 'dollar_index'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9734f02",
   "metadata": {},
   "source": [
    "We should get dividend yield too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "div_data = {}\n",
    "\n",
    "for etf in etfs:\n",
    "    ticker = yf.Ticker(etf)\n",
    "    div = ticker.dividends\n",
    "    price = ticker.history(timeframe)['Close']\n",
    "\n",
    "    # Calculate dividend yield\n",
    "    div_12m = div.rolling(window='365D', min_periods=1).sum()\n",
    "    div_12m = div_12m.reindex(price.index, method='ffill')\n",
    "    div_yield = div_12m / price\n",
    "    div_data[etf] = div_yield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fffc352",
   "metadata": {},
   "source": [
    "Fix index for all 3 and rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac371df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "div_data['SPY'].index = pd.to_datetime(div_data['SPY'].index).normalize().tz_localize(None) # Normalize puts date in format we want\n",
    "div_data['QQQ'].index = pd.to_datetime(div_data['QQQ'].index).normalize().tz_localize(None) # Localize (none) makes sure it doesn't add our timezone\n",
    "div_data['DIA'].index = pd.to_datetime(div_data['DIA'].index).normalize().tz_localize(None)\n",
    "\n",
    "div_data['SPY'].name = 'SPY_div'\n",
    "div_data['QQQ'].name = 'QQQ_div'\n",
    "div_data['DIA'].name = 'DIA_div'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "div_data['SPY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52265c2b",
   "metadata": {},
   "source": [
    "### pandas_datareader lets us download fred data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f617557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(1990,1,1) # Start date for download\n",
    "\n",
    "# Macroeconomic data\n",
    "gdp = pdr.DataReader(\"GDP\", \"fred\", start)\n",
    "cpi = pdr.DataReader(\"CPIAUCSL\", \"fred\", start)\n",
    "fedfunds = pdr.DataReader(\"FEDFUNDS\", \"fred\", start)\n",
    "\n",
    "# For some reason this download doesn't have the most recent fed funds rate\n",
    "fedfunds = pd.concat([fedfunds['FEDFUNDS'], pd.Series([4.08], index=[datetime(2025,9,17)])])\n",
    "\n",
    "# Credit risk data\n",
    "ig_spread = pdr.DataReader(\"BAMLC0A4CBBB\", \"fred\", start)   # BofA BBB corp minus Treasuries\n",
    "#hy_spread = pdr.DataReader(\"BAMLH0A0HYM2\", \"fred\", start)   # BofA US High Yield spread\n",
    "#baa_spread = pdr.DataReader(\"BAA10Y\", \"fred\", start)        # Moody’s Baa – 10Y Treasury"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60aa584",
   "metadata": {},
   "source": [
    "Rename series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c792c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi.name = 'CPI'\n",
    "fedfunds.name = 'fed_funds'\n",
    "ig_spread.name = 'credit_spread'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def72632",
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_data = [gdp, cpi, fedfunds, ig_spread]\n",
    "\n",
    "# Last business day <= today\n",
    "last_bday = pd.bdate_range(end=pd.Timestamp.today().normalize().tz_localize(None), periods=1)[0]\n",
    "\n",
    "for i, df in enumerate(fred_data):\n",
    "    s = df.squeeze() # make it a Series\n",
    "    # Build a business-day index from the series start to last_bday\n",
    "    bidx = pd.bdate_range(start=s.index.min(), end=last_bday)\n",
    "    # Reindex to business days and forward-fill\n",
    "    s = s.reindex(bidx, method='ffill')\n",
    "    # Write back as a 1-col DataFrame with a proper name\n",
    "    name = s.name if s.name else f\"series_{i}\"\n",
    "    fred_data[i] = s.to_frame(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ea15b",
   "metadata": {},
   "source": [
    "Let's build a master dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf2055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "toggle_futures = True\n",
    "if toggle_futures:\n",
    "    s_p = s_p_F\n",
    "    nasdaq = nasdaq_F\n",
    "    djia = djia_F\n",
    "\n",
    "data = s_p.join([nasdaq, djia, div_data['SPY'], div_data['QQQ'], div_data['DIA'], ten_yr, three_m, dollar_index, fred_data[0], fred_data[1], fred_data[2], fred_data[3]])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00b0cb",
   "metadata": {},
   "source": [
    "# Linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c572d98d",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1957e49",
   "metadata": {},
   "source": [
    "We need to be careful to not include things such as raw moving averages that will leak volatility information into our drift prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd3bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aef311",
   "metadata": {},
   "source": [
    "Function definitions to help out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_mean(data, window):\n",
    "    return data.rolling(window, min_periods=window).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e655271",
   "metadata": {},
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdec175e",
   "metadata": {},
   "source": [
    "#  (TODO: look at making features like diffs for economic metrics, figure out when economic metrics are released vs reported in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make log prices / volumes of our data, then log normal assumptions are better and everything is additive\n",
    "\n",
    "for index in indexes:\n",
    "    data[f'{index}_log_price'] = np.log(data[f'{index}_Close']) # Log prices\n",
    "    data[f'{index}_log_volume'] = np.log(data[f'{index}_Volume']) # Log volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ee4db",
   "metadata": {},
   "source": [
    "Setting our target returns metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log returns (21 = 1 month)\n",
    "days = 1\n",
    "\n",
    "for index in indexes:\n",
    "    data[f'{index}_log_ret'] = data[f'{index}_log_price'].diff(days)\n",
    "\n",
    "# Next month log returns -- This will be our target variable\n",
    "data[['S&P_next_ret','NASDAQ_next_ret','DJIA_next_ret']] = data[['S&P_log_ret','NASDAQ_log_ret','DJIA_log_ret']].shift(-days).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e8b758",
   "metadata": {},
   "source": [
    "ETF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a0279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in indexes:\n",
    "    # Price-based\n",
    "    data[f'{index}_mom_1w'] = data[f'{index}_log_price'].diff(5) # Total price change / momentum indicator\n",
    "    data[f'{index}_mom_3m'] = data[f'{index}_log_price'].diff(63)\n",
    "    data[f'{index}_3m_rolling_price'] = rolling_mean(data[f'{index}_log_price'], 63)\n",
    "    data[f'{index}_trend_speed_price'] = data[f'{index}_3m_rolling_price'].diff(5)  # How fast the 3m trend is changing on a weekly basis\n",
    "    data[f'{index}_trend_dist_price'] = data[f'{index}_log_price'] - data[f'{index}_3m_rolling_price']\n",
    "\n",
    "\n",
    "    # Volume-based (essentially the same as price for now)\n",
    "    data[f'{index}_vlm_1w'] = data[f'{index}_log_volume'].diff(5) # Total volume change / momentum indicator\n",
    "    data[f'{index}_vlm_1m'] = data[f'{index}_log_volume'].diff(21)\n",
    "    data[f'{index}_vlm_3m'] = data[f'{index}_log_volume'].diff(63)\n",
    "    data[f'{index}_3m_rolling_volume'] = rolling_mean(data[f'{index}_log_volume'], 63)\n",
    "    data[f'{index}_trend_speed_volume'] = data[f'{index}_3m_rolling_volume'].diff(5)  # How fast the 3m trend is changing on a weekly basis\n",
    "    data[f'{index}_trend_dist_volume'] = data[f'{index}_log_volume'] - data[f'{index}_3m_rolling_volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978bc76f",
   "metadata": {},
   "source": [
    "Macro features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a9458d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a6ccd9c",
   "metadata": {},
   "source": [
    "Volatility / Price Features (From ChatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad2524",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-12\n",
    "\n",
    "# If you already have a rolling_mean helper, keep it. Otherwise:\n",
    "def rolling_mean(s, w):\n",
    "    return s.rolling(w, min_periods=max(2, int(w*0.6))).mean()\n",
    "\n",
    "def ewma_vol(r, lam=0.94):\n",
    "    # EWMA variance per RiskMetrics: sigma_t^2 = (1-lam)*r_{t-1}^2 + lam*sigma_{t-1}^2\n",
    "    # Use pandas ewm for convenience\n",
    "    return r.pow(2).ewm(alpha=(1-lam), adjust=False).mean().clip(lower=0)\n",
    "\n",
    "def rolling_autocorr(x, lag=1, window=63):\n",
    "    # Rolling autocorrelation of x at a given lag\n",
    "    # For stability, require at least ~60% of window\n",
    "    minp = max(10, int(window*0.6))\n",
    "    x0 = x\n",
    "    x1 = x.shift(lag)\n",
    "    return x0.rolling(window, min_periods=minp).corr(x1)\n",
    "\n",
    "def realized_quarticity(r, window=63):\n",
    "    # 3-month robust quarticity proxy (if daily): sum r^4 * (n / 3) approximation\n",
    "    # Here we simply provide rolling sum of r^4; scaling optional depending on use\n",
    "    minp = max(10, int(window*0.6))\n",
    "    return (r.pow(4)).rolling(window, min_periods=minp).sum()\n",
    "\n",
    "def build_vol_features(data, prefix, day_w=21, qtr_w=63, yr_w=252, ewma_lambda=0.94):\n",
    "    \"\"\"\n",
    "    Expects:\n",
    "      data[f'{prefix}_log_price'] (daily log price)\n",
    "      data[f'{prefix}_log_volume'] (daily log volume)\n",
    "    Produces a suite of volatility-centric features for that prefix.\n",
    "    \"\"\"\n",
    "    lp = data[f\"{prefix}_log_price\"]\n",
    "    lv = data.get(f\"{prefix}_log_volume\", None)\n",
    "\n",
    "    # Daily log return\n",
    "    r = lp.diff()  # already log-price, so diff = log-return\n",
    "\n",
    "    # --- Realized volatility proxies ---\n",
    "    data[f\"{prefix}_rv_1m\"]  = r.rolling(day_w, min_periods=int(day_w*0.6)).var().clip(lower=0)          # variance\n",
    "    data[f\"{prefix}_rv_3m\"]  = r.rolling(qtr_w, min_periods=int(qtr_w*0.6)).var().clip(lower=0)\n",
    "    data[f\"{prefix}_rv_1y\"]  = r.rolling(yr_w,  min_periods=int(yr_w*0.6)).var().clip(lower=0)\n",
    "    data[f\"{prefix}_absrv_1m\"] = r.abs().rolling(day_w, min_periods=int(day_w*0.6)).mean()               # mean |r|\n",
    "    data[f\"{prefix}_absrv_3m\"] = r.abs().rolling(qtr_w, min_periods=int(qtr_w*0.6)).mean()\n",
    "\n",
    "    # EWMA volatility (RiskMetrics-style)\n",
    "    data[f\"{prefix}_ewma_var\"] = ewma_vol(r, lam=ewma_lambda)\n",
    "    data[f\"{prefix}_ewma_vol\"] = np.sqrt(data[f\"{prefix}_ewma_var\"])\n",
    "\n",
    "    # Volatility-of-volatility (how fast vol is changing)\n",
    "    data[f\"{prefix}_vol_speed_1w\"] = data[f\"{prefix}_rv_3m\"].diff(5)                                      # weekly change in 3m var\n",
    "    data[f\"{prefix}_vol_mom_1m\"]   = data[f\"{prefix}_rv_3m\"] - data[f\"{prefix}_rv_1m\"]                    # 3m vs 1m\n",
    "    data[f\"{prefix}_vol_mom_1y\"]   = data[f\"{prefix}_rv_1y\"] - data[f\"{prefix}_rv_3m\"]\n",
    "\n",
    "    # Volatility clustering proxies\n",
    "    data[f\"{prefix}_acf_sqret_lag1_3m\"] = rolling_autocorr(r.pow(2), lag=1, window=qtr_w)\n",
    "    data[f\"{prefix}_acf_absret_lag1_3m\"] = rolling_autocorr(r.abs(), lag=1, window=qtr_w)\n",
    "\n",
    "    # Leverage effect proxy (contemporaneous corr between return and next day's vol)\n",
    "    # Negative returns often precede higher vol; we proxy with corr(r_t, |r|_{t+1})\n",
    "    data[f\"{prefix}_lev_proxy_3m\"] = r.rolling(qtr_w, min_periods=int(qtr_w*0.6)).corr(r.abs().shift(-1))\n",
    "\n",
    "    # Quarticity (heavy tails proxy)\n",
    "    data[f\"{prefix}_quarticity_3m\"] = realized_quarticity(r, window=qtr_w)\n",
    "\n",
    "    # Ratio features (normalized vol levels)\n",
    "    data[f\"{prefix}_vol_ratio_1m_3m\"] = (data[f\"{prefix}_rv_1m\"] / (data[f\"{prefix}_rv_3m\"] + EPS))\n",
    "    data[f\"{prefix}_vol_ratio_3m_1y\"] = (data[f\"{prefix}_rv_3m\"] / (data[f\"{prefix}_rv_1y\"] + EPS))\n",
    "    data[f\"{prefix}_ewma_over_3m\"]    = (data[f\"{prefix}_ewma_var\"] / (data[f\"{prefix}_rv_3m\"] + EPS))\n",
    "\n",
    "    # Price–volatility relation: distance from trend as a stress proxy\n",
    "    data[f\"{prefix}_price_trend_3m\"]  = rolling_mean(lp, qtr_w)\n",
    "    data[f\"{prefix}_price_trend_dist\"] = lp - data[f\"{prefix}_price_trend_3m\"]\n",
    "    # Volatility when far below trend often spikes; include interaction\n",
    "    data[f\"{prefix}_vol_x_trend_dist\"] = data[f\"{prefix}_rv_1m\"] * data[f\"{prefix}_price_trend_dist\"]\n",
    "\n",
    "    # Volume–volatility links (if volume available)\n",
    "    if lv is not None:\n",
    "        dv = lv.diff()  # log-volume change\n",
    "        data[f\"{prefix}_vlm_var_1m\"] = dv.rolling(day_w, min_periods=int(day_w*0.6)).var().clip(lower=0)\n",
    "        data[f\"{prefix}_vlm_var_3m\"] = dv.rolling(qtr_w, min_periods=int(qtr_w*0.6)).var().clip(lower=0)\n",
    "        # Corr between |r| and volume changes (vol–volume clustering)\n",
    "        data[f\"{prefix}_corr_absr_dlv_3m\"] = r.abs().rolling(qtr_w, min_periods=int(qtr_w*0.6)).corr(dv)\n",
    "        # Volume surprise proxy: current vs 3m trend\n",
    "        data[f\"{prefix}_vlm_trend_3m\"] = rolling_mean(lv, qtr_w)\n",
    "        data[f\"{prefix}_vlm_trend_dist\"] = lv - data[f\"{prefix}_vlm_trend_3m\"]\n",
    "        # Vol reacts to volume surprises\n",
    "        data[f\"{prefix}_vol_x_vlm_surprise\"] = data[f\"{prefix}_rv_1m\"] * data[f\"{prefix}_vlm_trend_dist\"]\n",
    "\n",
    "    # Optional: implied vs realized vol spread if you have VIX-like series\n",
    "    # if f\"{prefix}_impl_vol\" in data.columns:\n",
    "    #     data[f\"{prefix}_ivr_spread\"] = data[f\"{prefix}_impl_vol\"]**2 - data[f\"{prefix}_rv_1m\"]\n",
    "\n",
    "    # Forward-looking realized vol target example (if needed)\n",
    "    # data[f\"{prefix}_fwd_rv_1m\"] = r.shift(-1).rolling(day_w, min_periods=int(day_w*0.6)).var()\n",
    "\n",
    "    return data\n",
    "\n",
    "# ---- Apply to all indices ----\n",
    "for index in indexes:\n",
    "    data = build_vol_features(data, index, day_w=21, qtr_w=63, yr_w=252, ewma_lambda=0.94)\n",
    "\n",
    "# ---- Cross-index spillover features (optional but useful) ----\n",
    "# Differences/spreads in contemporaneous vol across indices capture contagion/regime moves\n",
    "data[\"SPX_minus_NDX_vol_1m\"] = data[\"S&P_rv_1m\"] - data[\"NASDAQ_rv_1m\"]\n",
    "data[\"SPX_minus_DJIA_vol_1m\"] = data[\"S&P_rv_1m\"] - data[\"DJIA_rv_1m\"]\n",
    "data[\"NDX_minus_DJIA_vol_1m\"] = data[\"NASDAQ_rv_1m\"] - data[\"DJIA_rv_1m\"]\n",
    "\n",
    "# A simple global vol factor: first PC of 3m realized vars (if you want a single factor)\n",
    "try:\n",
    "    _X = data[[\"S&P_rv_3m\", \"NASDAQ_rv_3m\", \"DJIA_rv_3m\"]].dropna()\n",
    "    _Xc = (_X - _X.mean()) / (_X.std(ddof=0) + EPS)\n",
    "    # first PC (no scikit-learn to keep it lightweight)\n",
    "    U, S, Vt = np.linalg.svd(_Xc.values, full_matrices=False)\n",
    "    gvol = pd.Series(U[:, 0]*S[0], index=_X.index, name=\"global_vol_pc1\")\n",
    "    data[\"global_vol_pc1\"] = gvol.reindex(data.index)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dde6b5f",
   "metadata": {},
   "source": [
    "Other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_dummies = pd.get_dummies(data.index.month, prefix=\"month\")\n",
    "month_dummies.set_index(data.index, inplace=True)\n",
    "data = data.join(month_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33bfda",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce437cd7",
   "metadata": {},
   "source": [
    "Let's check for NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9877752",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc89ed8c",
   "metadata": {},
   "source": [
    "Impute some NaNs with average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83154e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_columns = ['ten_yr', 'three_m', 'dollar_index', 'BAMLC0A4CBBB']\n",
    "\n",
    "for column in impute_columns:\n",
    "    data[column] = data[column].fillna(data[column].mean())\n",
    "\n",
    "\n",
    "#data['S&P_ret'] = data['S&P_ret'].fillna(data['S&P_ret'].mean())\n",
    "#data['NASDAQ_ret'] = data['NASDAQ_ret'].fillna(data['NASDAQ_ret'].mean())\n",
    "#data['DJIA_ret'] = data['DJIA_ret'].fillna(data['DJIA_ret'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2a0b3d",
   "metadata": {},
   "source": [
    "Drop others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e5424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e57460",
   "metadata": {},
   "source": [
    "### Split data\n",
    "\n",
    "Training/testing 80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b1b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fdde3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split=0.8):\n",
    "    cutoff = math.floor(len(data)*split)\n",
    "    training_data = data.iloc[:cutoff]\n",
    "    testing_data = data.iloc[cutoff:]\n",
    "    return training_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbaea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = split_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f599689",
   "metadata": {},
   "source": [
    "### Normalize inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4039bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(training_data, testing_data, features):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(training_data[features]) # Fitting on training data\n",
    "\n",
    "    train_scaled = training_data.copy()\n",
    "    test_scaled = testing_data.copy()\n",
    "\n",
    "    train_scaled[features] = scaler.transform(training_data[features])\n",
    "    test_scaled[features] = scaler.transform(testing_data[features])\n",
    "\n",
    "    # Save info on standardization for later\n",
    "    scaler_mu = pd.Series(scaler.mean_, index=features)\n",
    "    scaler_std = pd.Series(scaler.scale_, index=features)\n",
    "    return train_scaled, test_scaled, scaler_mu, scaler_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29737374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we only fit on training_data and explanatory variables\n",
    "targets = [f'{index}_next_ret' for index in indexes]\n",
    "dummies = [f'month_{month}' for month in range(1,13)]\n",
    "columns_to_ignore = [] #['NASDAQ_next_ret', 'DJIA_next_ret']\n",
    "columns_to_ignore.extend(dummies)\n",
    "features = [column for column in training_data.columns if column not in targets and column not in columns_to_ignore]\n",
    "\n",
    "train_scaled, test_scaled, scaler_mu, scaler_std = scale_features(training_data, testing_data, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678fe365",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df85bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ac9fb",
   "metadata": {},
   "source": [
    "We are going to test with and without ridge (which will help reduce the impact of collinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8090b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get print results from the models\n",
    "def eval_and_report(y_true, y_pred, model_name):\n",
    "    print(f\"{model_name:18s} | R^2: {r2_score(y_true, y_pred):.4f} | RMSE: {root_mean_squared_error(y_true, y_pred):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf60305",
   "metadata": {},
   "source": [
    "Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train_scaled, test_scaled, targets):\n",
    "    results = {}\n",
    "    for target in targets:\n",
    "        print(f\"\\n=== Target: {target} ===\")\n",
    "        X_train = train_scaled[features].copy()\n",
    "        y_train = train_scaled[target].copy()\n",
    "        X_test = test_scaled[features].copy()\n",
    "        y_test = test_scaled[target].copy()\n",
    "\n",
    "        # 1. Ordinary Least Squares (OLS)\n",
    "        ols = LinearRegression()\n",
    "        ols.fit(X_train, y_train)\n",
    "        yhat_ols = ols.predict(X_test)\n",
    "        eval_and_report(y_test, yhat_ols, \"OLS\")\n",
    "\n",
    "        # Print top coefficients\n",
    "        ols_coef = pd.Series(ols.coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "        print(\"Top OLS coeffs:\\n\", ols_coef.head(10))\n",
    "\n",
    "        # 2. Ridge with CV over alphas (time-series CV)\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        alphas = np.logspace(-4, 3, 30)\n",
    "\n",
    "        ridge = RidgeCV(alphas=alphas, cv=tscv, fit_intercept=True)\n",
    "        ridge.fit(X_train, y_train)\n",
    "        yhat_ridge = ridge.predict(X_test)\n",
    "        eval_and_report(y_test, yhat_ridge, f\"Ridge (alpha={ridge.alpha_:.4g})\")\n",
    "\n",
    "        # Print top coefficients\n",
    "        ridge_coef = pd.Series(ridge.coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "        print(\"Top Ridge coeffs:\\n\", ridge_coef.head(10))\n",
    "\n",
    "        # 3. Lasso with CV over alphas (time-series CV)\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        alphas = np.logspace(-4, 3, 30)\n",
    "\n",
    "        lasso = LassoCV(alphas=alphas, cv=tscv, fit_intercept=True)\n",
    "        lasso.fit(X_train, y_train)\n",
    "        yhat_lasso = lasso.predict(X_test)\n",
    "        eval_and_report(y_test, yhat_lasso, f\"Lasso (alpha={lasso.alpha_:.4g})\")\n",
    "\n",
    "        # Print top coefficients\n",
    "        lasso_coef = pd.Series(lasso.coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "        print(\"Top Lasso coeffs:\\n\", lasso_coef.head(10))\n",
    "\n",
    "        # 4. ElasticNet with CV over alphas and l1_ratios (time-series CV)\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        alphas = np.logspace(-4, 3, 40)\n",
    "        l1_ratios = np.arange(.1, 1, .1)   # 1.0 == Lasso, 0.0 == Ridge\n",
    "\n",
    "        enet = ElasticNetCV(\n",
    "            alphas=alphas,\n",
    "            l1_ratio=l1_ratios,\n",
    "            cv=tscv,\n",
    "            fit_intercept=True,\n",
    "            max_iter=20000,\n",
    "        )\n",
    "        enet.fit(X_train, y_train)\n",
    "\n",
    "        yhat_enet = enet.predict(X_test)\n",
    "        eval_and_report(y_test, yhat_enet, f\"ElasticNet (alpha={enet.alpha_:.4g}, l1_ratio={enet.l1_ratio_})\")\n",
    "\n",
    "        # Print top coefficients\n",
    "        enet_coef = pd.Series(enet.coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "        print(\"Top ElasticNet coeffs:\\n\", enet_coef.head(10))\n",
    "\n",
    "        # 5. PCA on OLS\n",
    "        pca = PCA(n_components=.95).fit(train_scaled) # keep 95% of variance and fit to training set\n",
    "        train_pca = pca.transform(train_scaled)\n",
    "        test_pca = pca.transform(test_scaled)\n",
    "\n",
    "        ols_pca = LinearRegression()\n",
    "        ols_pca.fit(train_pca, y_train)\n",
    "        yhat_pca = ols_pca.predict(test_pca)\n",
    "        eval_and_report(y_test, yhat_pca, \"OLS+PCA\")\n",
    "\n",
    "        # Store for later use\n",
    "        results[target] = {\n",
    "            \"ols_model\": ols,\n",
    "            \"ridge_model\": ridge,\n",
    "            \"ols_coefs\": ols_coef,\n",
    "            \"ridge_coefs\": ridge_coef,\n",
    "            \"lasso_coefs\": lasso_coef,\n",
    "            \"enet_coegs\": enet_coef,\n",
    "            \"pca_model\": ols_pca,\n",
    "            \"train_data_ols\": pd.Series(ols.predict(X_train), index=y_train.index, name=f\"ols_train\"),\n",
    "            \"train_data_ridge\": pd.Series(ridge.predict(X_train), index=y_train.index, name=f\"ridge_train\"),\n",
    "            \"train_data_lasso\": pd.Series(lasso.predict(X_train), index=y_train.index, name=f\"lasso_train\"),\n",
    "            \"train_data_enet\": pd.Series(enet.predict(X_train), index=y_train.index, name=f\"enet_train\"),\n",
    "            \"train_data_pca\": pd.Series(ols_pca.predict(train_pca), index=y_train.index, name=f\"pca_train\"),\n",
    "            \"yhat_ols\": pd.Series(yhat_ols, index=y_test.index, name=f\"{target}_ols_pred\"),\n",
    "            \"yhat_ridge\": pd.Series(yhat_ridge, index=y_test.index, name=f\"{target}_ridge_pred\"),\n",
    "            \"yhat_lasso\": pd.Series(yhat_lasso, index=y_test.index, name=f\"{target}_lasso_pred\"),\n",
    "            \"yhat_enet\": pd.Series(yhat_lasso, index=y_test.index, name=f\"{target}_enet_pred\"),\n",
    "            \"yhat_pca\": pd.Series(yhat_pca, index=y_test.index, name=f\"{target}_pca_pred\")\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d9f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_models(train_scaled, test_scaled, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5724d19",
   "metadata": {},
   "source": [
    "Make into new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120748fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['ridge', 'lasso', 'enet', 'pca']\n",
    "drift = data.copy()\n",
    "\n",
    "for target in targets:\n",
    "    for model in models:\n",
    "        drift[f'{target}_pred_{model}'] = pd.concat([results[target][f'train_data_{model}'],results[target][f'yhat_{model}']])\n",
    "drift.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595774e",
   "metadata": {},
   "source": [
    "Plot these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9644fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(data, baseline, measures, title, subtitles, indexes, models):\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.06,\n",
    "        subplot_titles=[index for index in indexes]\n",
    "    )\n",
    "\n",
    "    for i, (index) in enumerate(indexes, start=1):\n",
    "        show_leg = (i == 1)\n",
    "\n",
    "        # True next_ret\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=data.index, y=data[f'{index}_{baseline}'], name=f'{index}_{baseline}',\n",
    "                    mode=\"lines\", line=dict(width=1.6),\n",
    "                    showlegend=show_leg, legendgroup=\"true\"),\n",
    "            row=i, col=1\n",
    "        )\n",
    "\n",
    "        # Plot predictions\n",
    "        for model in models:\n",
    "            for measure in measures:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=data.index, y=data[f'{index}_{measure}_{model}'], name=f\"{model} prediction\",\n",
    "                            mode=\"lines\", line=dict(width=1.4, dash=\"dot\"),\n",
    "                            showlegend=show_leg, legendgroup=\"ridge\"),\n",
    "                    row=i, col=1\n",
    "                )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        height=900,\n",
    "        hovermode=\"x unified\",\n",
    "        template=\"plotly_white\",\n",
    "        margin=dict(t=80, r=30, b=80, l=70),\n",
    "        legend=dict(orientation=\"h\", yanchor=\"top\", y=-0.12, xanchor=\"left\", x=0)\n",
    "    )\n",
    "\n",
    "    for r in range(1, 4):\n",
    "        fig.update_yaxes(title_text=subtitles, row=r, col=1)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7117fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(drift, 'log_ret', ['next_ret_pred'], 'Linear Regression on Drift', 'Log Return', indexes, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9793f600",
   "metadata": {},
   "source": [
    "Overall returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67800fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_prices = {index: drift[f'{index}_Close'].iloc[0] for index in indexes}\n",
    "\n",
    "for index in indexes:\n",
    "    for model in models:\n",
    "        drift[f'{index}_price_pred_{model}'] = starting_prices[index] * np.exp(drift[f'{index}_next_ret_pred_{model}'].cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(drift, 'Close', ['price_pred'], 'Price Path Evolution of Drift', 'Price', indexes, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0bf273",
   "metadata": {},
   "source": [
    "# It looks like this may be an ok base for mu. Let's try to build sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c40168",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_data = data.copy()\n",
    "vol_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared residuals from previous -- using ElasticNet\n",
    "# Turns out we actually need to also take the log because they are so tiny\n",
    "\n",
    "for index in indexes:\n",
    "    vol_data[f'{index}_log_squared_residual'] = np.log(np.square(drift[f'{index}_next_ret'] - drift[f'{index}_next_ret_pred_enet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c11cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = split_data(vol_data)\n",
    "\n",
    "# Make sure we only fit on training_data and explanatory variables\n",
    "targets = [f'{index}_log_squared_residual' for index in indexes]\n",
    "dummies = [f'month_{month}' for month in range(1,13)]\n",
    "features = [column for column in training_data.columns if column not in targets and column not in dummies]\n",
    "\n",
    "train_scaled, test_scaled, vol_scaler_mu, vol_scaler_std = scale_features(training_data, testing_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e386364",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_results = train_models(train_scaled, test_scaled, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdeb113",
   "metadata": {},
   "source": [
    "Make into new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea156743",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = vol_data.copy()\n",
    "\n",
    "for target in targets:\n",
    "    for model in models:\n",
    "        diffusion[f'{target}_pred_{model}'] = pd.concat([vol_results[target][f'train_data_{model}'], vol_results[target][f'yhat_{model}']])\n",
    "\n",
    "diffusion.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3602b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(diffusion, 'log_squared_residual', ['log_squared_residual_pred'], 'Linear Regression on Diffusion', 'Log Residuals', indexes, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0a352",
   "metadata": {},
   "source": [
    "No clue how PCA could be at r^2 = .999 in the test set while all other models are around .1-.2 ... clearly some sort of overfitting going on there. Also looks like log transform loses a significant amount of data, but I think the distribution is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2080f5be",
   "metadata": {},
   "source": [
    "Convert to variance / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in indexes:\n",
    "    for model in models:\n",
    "        diffusion[f'{index}_variance_pred_{model}'] = np.exp(diffusion[f'{index}_log_squared_residual_pred_{model}'])\n",
    "        diffusion[f'{index}_std_pred_{model}'] = np.sqrt(diffusion[f'{index}_variance_pred_{model}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.merge(drift, diffusion, left_index=True, right_index=True, how='inner', suffixes=('', '_to_drop'))\n",
    "predictions = predictions.drop(columns=[col for col in predictions.columns if col.endswith('_to_drop')])\n",
    "\n",
    "\n",
    "# Boll bands with enet\n",
    "def create_boll_bands(vol=None, std=2, window=20):\n",
    "    if vol != None:\n",
    "        use_vol = True\n",
    "    else:\n",
    "        use_vol = False # Could be a better way to do this, but allowing later for different vol df\n",
    "\n",
    "    for index in indexes:\n",
    "        for model in models:\n",
    "            if not use_vol:\n",
    "                vol = predictions[f'{index}_std_pred_{model}']\n",
    "            predictions[f'{index}_sma'] = predictions[f'{index}_Close'].rolling(window=window, min_periods=window).mean()\n",
    "            predictions[f'{index}_upper_boll_{model}'] = std * vol*predictions[f'{index}_sma'] + predictions[f'{index}_sma']\n",
    "            predictions[f'{index}_lower_boll_{model}'] = -std * vol*predictions[f'{index}_sma'] + predictions[f'{index}_sma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5406cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boll_bands()\n",
    "plot_results(predictions, 'Close', ['upper_boll', 'lower_boll', 'price_pred'], 'Bollinger Bands', 'Price', indexes, ['enet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81be090",
   "metadata": {},
   "source": [
    "Let's see some stats on boll bands here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd1925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_boll_counts():\n",
    "    for index in indexes:\n",
    "        px_col  = f'{index}_Close'\n",
    "        sma_col = f'{index}_sma'\n",
    "\n",
    "        for model in models:\n",
    "            std_col   = f'{index}_std_pred_{model}'\n",
    "            up_col    = f'{index}_upper_boll_{model}'\n",
    "            low_col   = f'{index}_lower_boll_{model}'\n",
    "            sig_col   = f'{index}_boll_signal_{model}'            # -1 lower break, +1 upper break, 0 inside\n",
    "            touchU    = f'{index}_touch_upper_{model}'            # boolean\n",
    "            touchL    = f'{index}_touch_lower_{model}'\n",
    "            crossU    = f'{index}_cross_above_upper_{model}'      # boolean: crossed today\n",
    "            crossL    = f'{index}_cross_below_lower_{model}'\n",
    "            countAny  = f'{index}_cum_band_breaks_{model}'        # cumulative count\n",
    "            countU    = f'{index}_cum_upper_breaks_{model}'\n",
    "            countL    = f'{index}_cum_lower_breaks_{model}'\n",
    "            inBand    = f'{index}_inside_band_{model}'\n",
    "            widthCol  = f'{index}_band_width_{model}'             # relative width\n",
    "\n",
    "            # 3) Booleans: touches (price outside band)\n",
    "            price = predictions[px_col]\n",
    "            upper = predictions[up_col]\n",
    "            lower = predictions[low_col]\n",
    "\n",
    "            predictions[touchU] = (price >= upper)\n",
    "            predictions[touchL] = (price <= lower)\n",
    "            predictions[inBand] = (~predictions[touchU] & ~predictions[touchL])\n",
    "\n",
    "            # 4) True \"cross\" events (crossed today vs yesterday)\n",
    "            prev_price = price.shift(1)\n",
    "            prev_up    = upper.shift(1)\n",
    "            prev_low   = lower.shift(1)\n",
    "\n",
    "            # Cross above upper: was <= upper yesterday and > upper today\n",
    "            predictions[crossU] = (prev_price <= prev_up) & (price > upper)\n",
    "            # Cross below lower: was >= lower yesterday and < lower today\n",
    "            predictions[crossL] = (prev_price >= prev_low) & (price < lower)\n",
    "\n",
    "            # 5) Compact signal: +1 if price above upper, -1 if below lower, else 0\n",
    "            predictions[sig_col] = np.select(\n",
    "                [predictions[touchU], predictions[touchL]],\n",
    "                [1, -1],\n",
    "                default=0\n",
    "            ).astype(int)\n",
    "\n",
    "            # 6) Cumulative counts you can tally quickly\n",
    "            predictions[countU]   = predictions[crossU].cumsum()\n",
    "            predictions[countL]   = predictions[crossL].cumsum()\n",
    "            predictions[countAny] = (predictions[crossU] | predictions[crossL]).cumsum()\n",
    "\n",
    "            # 7) Optional: rolling 20-day counts if you want “recent frequency”\n",
    "            predictions[f'{index}_roll20_breaks_{model}'] = (\n",
    "                (predictions[crossU] | predictions[crossL]).rolling(20, min_periods=1).sum()\n",
    "            )\n",
    "\n",
    "            # 8) Band width (relative, helpful for diagnostics/screening)\n",
    "            predictions[widthCol] = (upper - lower) / predictions[sma_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc1c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_returns(price, horizons=(1,3,5,10)):\n",
    "    out = {}\n",
    "    lp = np.log(price.astype(float))\n",
    "    for h in horizons:\n",
    "        out[h] = (lp.shift(-h) - lp)            # r_{t→t+h}, aligned at t\n",
    "    return pd.DataFrame(out, index=price.index)\n",
    "\n",
    "def reenter_band_within(pred_df, idx, mdl, horizon=5):\n",
    "    \"\"\"Boolean: did price re-enter band within 'horizon' days after being outside?\"\"\"\n",
    "    px   = pred_df[f\"{idx}_Close\"]\n",
    "    up   = pred_df[f\"{idx}_upper_boll_{mdl}\"]\n",
    "    low  = pred_df[f\"{idx}_lower_boll_{mdl}\"]\n",
    "    outside = (px > up) | (px < low)\n",
    "    inside  = ~(outside)\n",
    "    # rolling forward \"any inside\" within next N days (including next day)\n",
    "    # Build a forward-looking window using shift(-k). We’ll OR across 1..N.\n",
    "    any_inside_nextN = pd.Series(False, index=pred_df.index)\n",
    "    for k in range(1, horizon+1):\n",
    "        any_inside_nextN = any_inside_nextN | inside.shift(-k)\n",
    "    return outside & any_inside_nextN\n",
    "\n",
    "def summarize_boll_stats(predictions, indexes, models, horizons=(1,3,5,10), reenter_N=5):\n",
    "    rows = []\n",
    "    for idx in indexes:\n",
    "        px = predictions[f\"{idx}_Close\"]\n",
    "        fwd = forward_returns(px, horizons)\n",
    "        for mdl in models:\n",
    "            # Columns built earlier\n",
    "            up     = predictions[f\"{idx}_upper_boll_{mdl}\"]\n",
    "            low    = predictions[f\"{idx}_lower_boll_{mdl}\"]\n",
    "            touchU = predictions[f\"{idx}_touch_upper_{mdl}\"].astype(bool)\n",
    "            touchL = predictions[f\"{idx}_touch_lower_{mdl}\"].astype(bool)\n",
    "            crossU = predictions[f\"{idx}_cross_above_upper_{mdl}\"].astype(bool)\n",
    "            crossL = predictions[f\"{idx}_cross_below_lower_{mdl}\"].astype(bool)\n",
    "            inside = predictions[f\"{idx}_inside_band_{mdl}\"].astype(bool)\n",
    "            width  = (up - low) / predictions[f\"{idx}_sma\"]\n",
    "\n",
    "            n = len(px.dropna())\n",
    "            # base rates\n",
    "            pct_inside = inside.mean()\n",
    "            pct_touchU = touchU.mean()\n",
    "            pct_touchL = touchL.mean()\n",
    "            pct_crossU = crossU.mean()\n",
    "            pct_crossL = crossL.mean()\n",
    "\n",
    "            # forward return frames aligned to signal dates\n",
    "            sigs = {\n",
    "                \"touchU\": touchU,\n",
    "                \"touchL\": touchL,\n",
    "                \"crossU\": crossU,\n",
    "                \"crossL\": crossL,\n",
    "            }\n",
    "\n",
    "            # re-entry (mean reversion back inside band) within N days after being outside\n",
    "            reenter = reenter_band_within(predictions, idx, mdl, horizon=reenter_N)\n",
    "            pct_reenter_after_outside = reenter.mean()\n",
    "\n",
    "            row = {\n",
    "                \"index\": idx,\n",
    "                \"model\": mdl,\n",
    "                \"obs\": n,\n",
    "                \"% inside\": pct_inside,\n",
    "                \"% touchU\": pct_touchU,\n",
    "                \"% touchL\": pct_touchL,\n",
    "                \"% crossU\": pct_crossU,\n",
    "                \"% crossL\": pct_crossL,\n",
    "                \"band_width_med%\": float(np.nanmedian(width))*100.0,\n",
    "                \"band_width_p90%\": float(np.nanpercentile(width.dropna(), 90))*100.0,\n",
    "                f\"% reenter≤{reenter_N}d after outside\": pct_reenter_after_outside,\n",
    "            }\n",
    "\n",
    "            # Hit rates & conditional forward returns\n",
    "            for name, mask in sigs.items():\n",
    "                m = mask.fillna(False)\n",
    "                idx_sig = m[m].index\n",
    "                if len(idx_sig) == 0:\n",
    "                    # fill NaNs for empty signals\n",
    "                    row.update({f\"{name}_n\": 0})\n",
    "                    for h in horizons:\n",
    "                        row.update({\n",
    "                            f\"{name}_hit{h}d%\": np.nan,\n",
    "                            f\"{name}_mean{h}d(bp)\": np.nan,\n",
    "                            f\"{name}_med{h}d(bp)\": np.nan,\n",
    "                        })\n",
    "                    continue\n",
    "\n",
    "                row[f\"{name}_n\"] = int(len(idx_sig))\n",
    "                # Direction for \"hit\": breakout (crossU) expects +; crossL expects −; touches: mean-revert assumption\n",
    "                for h in horizons:\n",
    "                    fr = fwd[h].reindex(idx_sig)  # log fwd return\n",
    "                    if name in (\"crossU\", \"touchL\"):\n",
    "                        # bullish expectation\n",
    "                        hit = (fr > 0)\n",
    "                    elif name in (\"crossL\", \"touchU\"):\n",
    "                        # bearish expectation\n",
    "                        hit = (fr < 0)\n",
    "                    else:\n",
    "                        hit = fr > 0\n",
    "                    row[f\"{name}_hit{h}d%\"]   = float(hit.mean())\n",
    "                    # report in basis points for readability\n",
    "                    row[f\"{name}_mean{h}d(bp)\"] = float(fr.mean() * 1e4)\n",
    "                    row[f\"{name}_med{h}d(bp)\"]  = float(fr.median() * 1e4)\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    # Nice ordering\n",
    "    base_cols = [\"index\",\"model\",\"obs\",\"% inside\",\"% touchU\",\"% touchL\",\"% crossU\",\"% crossL\",\n",
    "                 \"band_width_med%\",\"band_width_p90%\", f\"% reenter≤{reenter_N}d after outside\"]\n",
    "    # dynamically add signal metrics\n",
    "    sig_cols = []\n",
    "    for s in (\"touchU\",\"touchL\",\"crossU\",\"crossL\"):\n",
    "        sig_cols += [f\"{s}_n\"] + \\\n",
    "                    [f\"{s}_hit{h}d% for h in horizons\"]  # placeholder for ordering logic\n",
    "    # We’ll just sort columns\n",
    "    metric_cols = [c for c in out.columns if c not in base_cols]\n",
    "    out = out[base_cols + sorted(metric_cols)]\n",
    "    return out\n",
    "\n",
    "# ---- run it ----\n",
    "set_boll_counts()\n",
    "predictions_train, predictions_test = split_data(predictions)\n",
    "summary_train = summarize_boll_stats(predictions_train, indexes, models, horizons=(1,3,5,10), reenter_N=5)\n",
    "summary_test = summarize_boll_stats(predictions_test, indexes, models, horizons=(1,3,5,10), reenter_N=5)\n",
    "\n",
    "# Example: filter to signals with decent sample size and view\n",
    "pd.set_option('display.max_columns', None)\n",
    "display(summary_train.sort_values([\"index\",\"model\"]))\n",
    "display(summary_test.sort_values([\"index\",\"model\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73fe15",
   "metadata": {},
   "source": [
    "# Backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c757b4d1",
   "metadata": {},
   "source": [
    "Sizing strategy 1: Using Merton-Kelly criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5255f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merton_kelly_size(mu: float, rf: float, sigma: float):\n",
    "    size = (mu - rf) / sigma**2\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kelly_criterion(odds, p, q):\n",
    "    return (odds*p - q)/odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a684f672",
   "metadata": {},
   "source": [
    "Sizing strategy 2: Using vol target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a58ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vol_target_sizing(target: float, vol: float):\n",
    "    size = (target/vol)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44378db9",
   "metadata": {},
   "source": [
    "## Trading strategy 1: Trade based on our models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2f6bf",
   "metadata": {},
   "source": [
    "### Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73c1a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in indexes:\n",
    "    predictions[f'{index}_baseline_wealth'] = np.exp(bt[f'{index}_log_ret'].cumsum()) * 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9001c47",
   "metadata": {},
   "source": [
    "Might be slower, but much easier to implement strategy using iterative process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e22b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create backtest df copy\n",
    "bt = predictions.copy()\n",
    "\n",
    "# Set initial conditions\n",
    "starting_cash = 10000\n",
    "bt_data = {}\n",
    "\n",
    "for index in indexes:\n",
    "    bt_data[index] = {}\n",
    "\n",
    "    for model in models:\n",
    "        bt[f'{index}_portfolio_value_{model}'] = 0\n",
    "        bt[f'{index}_cash_{model}'] = 0\n",
    "        bt[f'{index}_cash_{model}'].iloc[0] = starting_cash\n",
    "        bt[f'{index}_signal_{model}'] = 0\n",
    "\n",
    "        bt_data[index][model] = {\n",
    "            'last_value': 0,\n",
    "            'last_cash': starting_cash\n",
    "        }\n",
    "\n",
    "# Starts at second day\n",
    "for idx, row in bt.iloc[1:].iterrows():\n",
    "    for index in indexes:\n",
    "        for model in models:\n",
    "            last_value = bt_data[index][model]['last_value']\n",
    "            last_cash = bt_data[index][model]['last_cash']\n",
    "\n",
    "            close = row[f'{index}_Close']\n",
    "\n",
    "            mu = np.exp(row[f'{index}_next_ret_pred_enet'])\n",
    "            vol = row[f'{index}_std_pred_{model}']\n",
    "            rf = (1+row['ten_yr'])**(1/252)-1 # Daily\n",
    "            investment = 0\n",
    "\n",
    "            # Update portfolio value\n",
    "            current_value = last_value * np.exp(row[f'{index}_log_ret'])\n",
    "\n",
    "            # Calculate total equity\n",
    "            total_equity = current_value + last_cash \n",
    "\n",
    "            # Get bet sizing with Merton-Kelly\n",
    "            #target_pos = get_merton_kelly_size(mu, rf, vol) * total_equity # Daily rf\n",
    "            #target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "\n",
    "\n",
    "            # Get bet sizing with vol target\n",
    "            target_pos = get_vol_target_sizing(.1/np.sqrt(252), vol) * total_equity\n",
    "            target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "            \n",
    "            # Bet sizing based on size of potential increase/decrease, signed\n",
    "            #target_pos = (mu - rf)/abs(mu - rf) * total_equity # super leverage\n",
    "            #target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "\n",
    "            # Execute trade / Update investment amount to reach target position\n",
    "            investment = target_pos - current_value\n",
    "\n",
    "            # Update cash\n",
    "            last_cash = bt_data[index][model]['last_cash'] = last_cash - investment\n",
    "            bt.loc[idx, f'{index}_cash_{model}'] = last_cash\n",
    "\n",
    "            # Store portfolio value as last value for calculation\n",
    "            bt_data[index][model]['last_value'] = current_value + investment\n",
    "            bt.loc[idx, f'{index}_portfolio_value_{model}'] = bt_data[index][model]['last_value']\n",
    "\n",
    "            # Set signal for plotting later\n",
    "            if investment > 0:\n",
    "                bt.loc[idx, f'{index}_signal_{model}'] = 1\n",
    "            elif investment < 0:\n",
    "                bt.loc[idx, f'{index}_signal_{model}'] = -1\n",
    "    \n",
    "for index in indexes:\n",
    "    for model in models:\n",
    "        bt[f'{index}_total_value_{model}'] = bt[f'{index}_portfolio_value_{model}'] + bt[f'{index}_cash_{model}']\n",
    "        final_value = bt[f'{index}_total_value_{model}'].iloc[-1]\n",
    "        print(f'Final value for {index}, {model}: {final_value:.2f}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63437aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(bt, 'baseline_wealth', ['total_value'], 'Backtest Performance', 'Wealth', indexes, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689260f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b158f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_and_mdd(df, col_equity, col_rf=\"ten_yr\", periods_per_year=252):\n",
    "    \"\"\"\n",
    "    df: DataFrame with equity column and risk-free rate column\n",
    "    col_equity: name of the equity curve column\n",
    "    col_rf: name of risk-free column in same df\n",
    "    periods_per_year: 252 for daily data\n",
    "    \"\"\"\n",
    "    eq = df[col_equity]\n",
    "    rf = df[col_rf]\n",
    "\n",
    "    # compute portfolio simple returns\n",
    "    port_rets = eq.pct_change().dropna()\n",
    "    # align rf\n",
    "    rf_aligned = rf.reindex(port_rets.index).astype(float)\n",
    "\n",
    "    # excess returns (assuming rf is already per-period, e.g. daily)\n",
    "    excess = port_rets - ((1+rf_aligned/100)**(1/periods_per_year)-1)\n",
    "    sharpe = (excess.mean() / excess.std()) * np.sqrt(periods_per_year) if excess.std() > 0 else np.nan\n",
    "\n",
    "    # max drawdown\n",
    "    running_max = eq.cummax()\n",
    "    drawdown = eq / running_max - 1.0\n",
    "    max_dd = drawdown.min()\n",
    "\n",
    "    return sharpe, max_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = math.floor(len(bt)*.8)\n",
    "training_returns = bt.iloc[:cutoff]\n",
    "testing_returns = bt.iloc[cutoff:]\n",
    "\n",
    "for index in indexes:\n",
    "    for model in models:\n",
    "        total_value = f'{index}_total_value_{model}'\n",
    "        final_training_value = training_returns[total_value].iloc[-1]\n",
    "        training_cagr = 100 * ((final_training_value/training_returns[total_value].iloc[0])**(1/(training_returns.index[-1].year - training_returns.index[0].year + training_returns.index[-1].month/12))-1)\n",
    "        print(f'Final training value for {index}, {model}: {final_training_value}, CAGR: {training_cagr:.2f}%')\n",
    "\n",
    "        final_testing_value = testing_returns[total_value].iloc[-1]\n",
    "        testing_cagr = 100*((final_testing_value/testing_returns[total_value].iloc[0])**(1/(testing_returns.index[-1].year - testing_returns.index[0].year + testing_returns.index[-1].month/12))-1)\n",
    "        print(f'Final testing value for {index}, {model}: {final_testing_value}, CAGR: {testing_cagr:.2f}%')\n",
    "        \n",
    "                # ---- Training metrics ----\n",
    "        train_curve = training_returns[total_value]\n",
    "        train_sharpe, train_mdd = sharpe_and_mdd(training_returns, total_value, col_rf=\"ten_yr\")\n",
    "        print(f\"Training Sharpe: {train_sharpe:.2f}, Max Drawdown: {train_mdd:.2%}\")\n",
    "\n",
    "        # ---- Testing metrics ----\n",
    "        test_curve = testing_returns[total_value]\n",
    "        test_sharpe, test_mdd = sharpe_and_mdd(testing_returns, total_value, col_rf=\"ten_yr\")\n",
    "        print(f\"Testing Sharpe: {test_sharpe:.2f}, Max Drawdown: {test_mdd:.2%}\")\n",
    "\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a38e46",
   "metadata": {},
   "source": [
    "Trading strategy 2: Trading using bollinger bands and moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b549110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- helpers ----\n",
    "def forward_log_returns(price: pd.Series, horizons=(1,3,5,10)) -> pd.DataFrame:\n",
    "    lp = np.log(price.astype(float))\n",
    "    out = {h: (lp.shift(-h) - lp) for h in horizons}  # log P_{t+h} - log P_t\n",
    "    return pd.DataFrame(out, index=price.index)\n",
    "\n",
    "def simple_from_log(x: pd.Series) -> pd.Series:\n",
    "    return np.exp(x) - 1.0\n",
    "\n",
    "def rf_over_h(annual_rf: float, H: int, periods_per_year=252) -> float:\n",
    "    return (1.0 + annual_rf) ** (H/periods_per_year) - 1.0\n",
    "\n",
    "def kelly_empirical(mean_R: float, var_R: float, rf_H: float,\n",
    "                    frac: float=0.25, cap: float=1.0, eps: float=1e-12) -> float:\n",
    "    var_R = max(var_R, eps)\n",
    "    f_star = (mean_R - rf_H) / var_R\n",
    "    return float(np.clip(frac * f_star, -cap, cap))\n",
    "\n",
    "# ---- 1) build empirical horizon stats per (index, model) and signal ----\n",
    "def build_signal_horizon_stats(pred, indexes, models, horizons=(1,3,5,10), min_signals=20):\n",
    "    stats = {}  # stats[(idx,mdl)] = {\"crossU\": {\"H\":..,\"mean_R\":..,\"var_R\":..,\"n\":..}, \"crossL\": {...}}\n",
    "\n",
    "    for idx in indexes:\n",
    "        px   = pred[f\"{idx}_Close\"]\n",
    "        fwdL = forward_log_returns(px, horizons=horizons)  # log returns t->t+H\n",
    "        fwdS = fwdL.apply(simple_from_log)                 # simple returns\n",
    "\n",
    "        for mdl in models:\n",
    "            crossU = pred[f\"{idx}_cross_above_upper_{mdl}\"].astype(bool)\n",
    "            crossL = pred[f\"{idx}_cross_below_lower_{mdl}\"].astype(bool)\n",
    "\n",
    "            bestU = {\"H\": None, \"mean_R\": np.nan, \"var_R\": np.nan, \"n\": 0}\n",
    "            bestL = {\"H\": None, \"mean_R\": np.nan, \"var_R\": np.nan, \"n\": 0}\n",
    "\n",
    "            for H in horizons:\n",
    "                # UP: we want big positive mean_R over crossU events\n",
    "                ru = fwdS[H].where(crossU).dropna()\n",
    "                if len(ru) >= min_signals:\n",
    "                    mu_u, vu = float(ru.mean()), float(ru.var(ddof=1))\n",
    "                    if (bestU[\"H\"] is None) or (mu_u > bestU[\"mean_R\"]):\n",
    "                        bestU = {\"H\": H, \"mean_R\": mu_u, \"var_R\": vu, \"n\": int(len(ru))}\n",
    "                # DOWN: we want big negative mean_R (i.e., large +mean of -R for shorts)\n",
    "                rl = fwdS[H].where(crossL).dropna()\n",
    "                if len(rl) >= min_signals:\n",
    "                    mu_l, vl = float(rl.mean()), float(rl.var(ddof=1))\n",
    "                    # pick the most negative mean_R\n",
    "                    if (bestL[\"H\"] is None) or (mu_l < bestL[\"mean_R\"]):\n",
    "                        bestL = {\"H\": H, \"mean_R\": mu_l, \"var_R\": vl, \"n\": int(len(rl))}\n",
    "\n",
    "            stats[(idx, mdl)] = {\"crossU\": bestU, \"crossL\": bestL}\n",
    "    return stats\n",
    "\n",
    "# ---- 2) backtest: enter on cross, lock position H days, size with Kelly(mean_R_H, var_R_H) ----\n",
    "def backtest_cross_empirical_kelly(pred, indexes, models, stats,\n",
    "                                   rf_col=\"ten_yr\", starting_cash=10_000,\n",
    "                                   enter_next_bar=True, kelly_frac=0.25, kelly_cap=1.0):\n",
    "    bt = pred.copy()\n",
    "    for idx in indexes:\n",
    "        for mdl in models:\n",
    "            bt[f\"{idx}_pos_{mdl}\"] = 0.0\n",
    "            bt[f\"{idx}_equity_{mdl}\"] = np.nan\n",
    "\n",
    "    eq   = {(idx, mdl): starting_cash for idx in indexes for mdl in models}\n",
    "    hold = {(idx, mdl): 0 for idx in indexes for mdl in models}\n",
    "\n",
    "    dates = bt.index\n",
    "    for i, t in enumerate(dates):\n",
    "        t_sig = dates[i-1] if (enter_next_bar and i-1 >= 0) else t\n",
    "        ann_rf_t = float(bt.loc[t, rf_col]) if rf_col in bt.columns and pd.notna(bt.loc[t, rf_col]) else 0.0\n",
    "\n",
    "        for idx in indexes:\n",
    "            for mdl in models:\n",
    "                # carry pos\n",
    "                if i > 0:\n",
    "                    bt.loc[t, f\"{idx}_pos_{mdl}\"] = bt.loc[dates[i-1], f\"{idx}_pos_{mdl}\"]\n",
    "                pos = float(bt.loc[t, f\"{idx}_pos_{mdl}\"])\n",
    "\n",
    "                # countdown & exit\n",
    "                if hold[(idx, mdl)] > 0:\n",
    "                    hold[(idx, mdl)] -= 1\n",
    "                    if hold[(idx, mdl)] == 0:\n",
    "                        pos = 0.0\n",
    "\n",
    "                # enter if flat and we have a signal + stats\n",
    "                if pos == 0.0 and t_sig is not None:\n",
    "                    crossU = bool(bt.loc[t_sig, f\"{idx}_cross_above_upper_{mdl}\"]) if f\"{idx}_cross_above_upper_{mdl}\" in bt.columns else False\n",
    "                    crossL = bool(bt.loc[t_sig, f\"{idx}_cross_below_lower_{mdl}\"]) if f\"{idx}_cross_below_lower_{mdl}\" in bt.columns else False\n",
    "\n",
    "                    if crossU:\n",
    "                        s = stats[(idx, mdl)][\"crossU\"]\n",
    "                        if s[\"H\"] is not None and s[\"n\"] > 0 and s[\"var_R\"] > 0:\n",
    "                            rf_H = rf_over_h(ann_rf_t, s[\"H\"])\n",
    "                            f = kelly_empirical(s[\"mean_R\"], s[\"var_R\"], rf_H, kelly_frac, kelly_cap)\n",
    "                            pos = +abs(f)\n",
    "                            hold[(idx, mdl)] = int(s[\"H\"])\n",
    "\n",
    "                    elif crossL:\n",
    "                        s = stats[(idx, mdl)][\"crossL\"]\n",
    "                        if s[\"H\"] is not None and s[\"n\"] > 0 and s[\"var_R\"] > 0:\n",
    "                            rf_H = rf_over_h(ann_rf_t, s[\"H\"])\n",
    "                            f = kelly_empirical(-s[\"mean_R\"], s[\"var_R\"], rf_H, kelly_frac, kelly_cap)\n",
    "                            # note: for shorts, we used mean PnL of short = -mean_R; keep same var\n",
    "                            pos = -abs(f)\n",
    "                            hold[(idx, mdl)] = int(s[\"H\"])\n",
    "\n",
    "                bt.loc[t, f\"{idx}_pos_{mdl}\"] = pos\n",
    "\n",
    "                # apply PnL with daily log return\n",
    "                r_t = float(bt.loc[t, f\"{idx}_log_ret\"]) if f\"{idx}_log_ret\" in bt.columns and pd.notna(bt.loc[t, f\"{idx}_log_ret\"]) else 0.0\n",
    "                prev_eq = eq[(idx, mdl)]\n",
    "                eq[(idx, mdl)] = prev_eq * np.exp(pos * r_t)\n",
    "                bt.loc[t, f\"{idx}_equity_{mdl}\"] = eq[(idx, mdl)]\n",
    "    return bt\n",
    "\n",
    "# ---- 3) run it ----\n",
    "horizons = (1,3,5,10)\n",
    "sig_stats = build_signal_horizon_stats(predictions, indexes, models, horizons=horizons, min_signals=20)\n",
    "bt = backtest_cross_empirical_kelly(predictions, indexes, models, sig_stats,\n",
    "                                    rf_col=\"ten_yr\",\n",
    "                                    starting_cash=10_000,\n",
    "                                    enter_next_bar=True,\n",
    "                                    kelly_frac=0.25,  # fractional Kelly\n",
    "                                    kelly_cap=1.0)    # leverage cap\n",
    "\n",
    "# ---- 4) quick reporting ----\n",
    "def sharpe_and_mdd_from_equity(eq: pd.Series, periods_per_year=252):\n",
    "    rets = np.log(eq).diff().dropna()\n",
    "    sharpe = (rets.mean() / rets.std()) * np.sqrt(periods_per_year) if rets.std() > 0 else np.nan\n",
    "    mdd = (eq / eq.cummax() - 1.0).min()\n",
    "    return float(sharpe), float(mdd)\n",
    "\n",
    "for idx in indexes:\n",
    "    for mdl in models:\n",
    "        col = f\"{idx}_equity_{mdl}\"\n",
    "        s, m = sharpe_and_mdd_from_equity(bt[col].dropna())\n",
    "        fv = bt[col].dropna().iloc[-1]\n",
    "        ss = sig_stats[(idx, mdl)]\n",
    "        print(f\"{idx}/{mdl}: Final ${fv:,.0f} | Sharpe {s:.2f} | MaxDD {m:.1%} | \"\n",
    "              f\"H* U={ss['crossU']['H']} (n={ss['crossU']['n']} mean={ss['crossU']['mean_R']:.4f}) \"\n",
    "              f\"L={ss['crossL']['H']} (n={ss['crossL']['n']} mean={ss['crossL']['mean_R']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e83a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(bt, 'baseline_wealth', ['equity'], 'Backtest Performance', 'Wealth', indexes, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c9c1ac",
   "metadata": {},
   "source": [
    "# MS-Garch Volatility Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04dea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.regime_switching.markov_regression import MarkovRegression\n",
    "from arch.univariate import ConstantMean, GARCH, StudentsT\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d02d26c",
   "metadata": {},
   "source": [
    "Test pulled from GPT (python doesn't have a MS-GARCH library, only separate implementations with reduced functionality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf5d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSGARCH:\n",
    "    \"\"\"\n",
    "    Markov switching means & transitions via statsmodels.MarkovRegression,\n",
    "    per-state GARCH(1,1) via arch; combine with Gray (1996) approximation.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=2, thresh=0.6, max_iter=1, verbose=False, dist='normal'):\n",
    "        self.k = int(k)\n",
    "        self.thresh = float(thresh)  # posterior cutoff for state-specific GARCH fit\n",
    "        self.max_iter = int(max_iter)\n",
    "        self.verbose = verbose\n",
    "        self.dist = dist.lower()\n",
    "        assert self.dist in (\"normal\", \"t\"), \"dist must be 'normal' or 't'\"\n",
    "        self.nu_ = None   # will hold per-state dof if dist='t'\n",
    "\n",
    "    def fit(self, r: pd.Series | np.ndarray):\n",
    "        r = np.asarray(pd.Series(r).astype(float).dropna())\n",
    "        T = len(r)\n",
    "        assert T > 50, \"Need enough data\"\n",
    "\n",
    "        # 1) HMM on returns with state-dependent intercept (mean) and variance\n",
    "        #    (statsmodels allows switching_variance=True, which helps separate regimes)\n",
    "        mod = MarkovRegression(r, k_regimes=self.k,\n",
    "                               trend='c',  # intercept per regime\n",
    "                               switching_variance=True)\n",
    "        res = mod.fit(disp=False)\n",
    "        self._mr_mod_ = mod\n",
    "        self._mr_res_ = res\n",
    "\n",
    "        # --- 1) Extract regime-specific means (intercepts) ---\n",
    "        names = np.asarray(res.model.param_names)\n",
    "        vals  = np.asarray(res.params)\n",
    "\n",
    "        mask = (np.char.find(names, 'intercept') >= 0)\n",
    "        if not mask.any():\n",
    "            mask = (np.char.find(names, 'const') >= 0)\n",
    "\n",
    "        intercept_names = names[mask]\n",
    "        intercept_vals  = vals[mask]\n",
    "\n",
    "        pairs = []\n",
    "        for n, v in zip(intercept_names, intercept_vals):\n",
    "            m = re.search(r'\\[(\\d+)\\]', n)\n",
    "            idx = int(m.group(1)) if m else 10**9\n",
    "            pairs.append((idx, float(v)))\n",
    "        pairs.sort(key=lambda t: t[0])\n",
    "        mu = np.array([p[1] for p in pairs])  # length K\n",
    "\n",
    "        # --- 2) Transition matrix ---\n",
    "        P_left = mod.regime_transition_matrix(res.params)  # columns sum to 1\n",
    "        P = P_left.T                                      # rows sum to 1, matches our code convention\n",
    "\n",
    "        # --- 3) Smoothed posteriors ---\n",
    "        gamma_obj = res.smoothed_marginal_probabilities\n",
    "        if hasattr(gamma_obj, \"to_numpy\"):\n",
    "            gamma = gamma_obj.to_numpy()\n",
    "        else:\n",
    "            gamma = np.asarray(gamma_obj)                     # shape (T, K)\n",
    "\n",
    "        # 2) Fit GARCH(1,1) separately for each regime using high-probability times\n",
    "        self._arch_results_ = []\n",
    "        garch_params = []\n",
    "        nus = []\n",
    "        for k in range(self.k):\n",
    "            mask = (gamma[:, k] == gamma.max(1)) & (gamma[:, k] >= self.thresh)\n",
    "            idx = np.where(mask)[0]\n",
    "            if len(idx) < 30:\n",
    "                idx = np.argsort(gamma[:, k])[-max(30, T // self.k):]\n",
    "            r_k = r[idx] - mu[k]  # center by state mean\n",
    "\n",
    "            am = ConstantMean(r_k)\n",
    "            am.volatility = GARCH(1, 0, 1)\n",
    "            if self.dist == \"t\":\n",
    "                am.distribution = StudentsT()\n",
    "            res_k = am.fit(disp=\"off\")\n",
    "\n",
    "            self._arch_results_.append(res_k)\n",
    "            v = res_k.params\n",
    "            omega = float(v[\"omega\"])\n",
    "            alpha = float(v[\"alpha[1]\"])\n",
    "            beta  = float(v[\"beta[1]\"])\n",
    "\n",
    "            # (optional) safety clamp on persistence\n",
    "            s = max(alpha + beta, 1e-6)\n",
    "            if s >= 0.998:\n",
    "                shrink = (s / 0.98)\n",
    "                alpha /= shrink; beta /= shrink\n",
    "\n",
    "            garch_params.append((omega, alpha, beta))\n",
    "\n",
    "            if self.dist == \"t\":\n",
    "                # ARCH names this 'nu'\n",
    "                nu = float(v[\"nu\"])\n",
    "                # ensure nu > 2 so variance exists\n",
    "                nu = max(nu, 2.01)\n",
    "                nus.append(nu)\n",
    "\n",
    "        garch_params = np.array(garch_params)\n",
    "        omega = garch_params[:, 0]; alpha = garch_params[:, 1]; beta = garch_params[:, 2]\n",
    "        self.omega_, self.alpha_, self.beta_ = omega, alpha, beta\n",
    "        self.nu_ = np.array(nus) if self.dist == \"t\" else None\n",
    "\n",
    "        # 3) Gray recursion using gamma as pi_t\n",
    "        h, m_mix, h_mix = self._gray_recursion(r, mu, omega, alpha, beta, gamma)\n",
    "\n",
    "        # (Optional) one extra iteration: re-center residuals with mixture mean and refit GARCH\n",
    "        for it in range(self.max_iter):\n",
    "            if self.max_iter <= 1: break\n",
    "            # reselect by posterior again (same gamma) but use residuals r - mu_k\n",
    "            new_params = []\n",
    "            for k in range(self.k):\n",
    "                mask = (gamma[:, k] == gamma.max(1)) & (gamma[:, k] >= self.thresh)\n",
    "                idx = np.where(mask)[0]\n",
    "                if len(idx) < 30:\n",
    "                    idx = np.argsort(gamma[:, k])[-max(30, T // self.k):]\n",
    "                r_k = r[idx] - mu[k]\n",
    "                am = ConstantMean(r_k)\n",
    "                am.volatility = GARCH(1, 0, 1)\n",
    "                res_k = am.fit(disp='off')\n",
    "                self._arch_results_[k] = res_k\n",
    "                v = res_k.params\n",
    "                omega = float(v['omega']); alpha = float(v['alpha[1]']); beta = float(v['beta[1]'])\n",
    "                s = max(alpha + beta, 1e-6)\n",
    "                if s >= 0.998:\n",
    "                    shrink = (s / 0.98)\n",
    "                    alpha /= shrink; beta /= shrink\n",
    "                new_params.append((omega, alpha, beta))\n",
    "            new_params = np.array(new_params)\n",
    "            omega, alpha, beta = new_params[:,0], new_params[:,1], new_params[:,2]\n",
    "            h, m_mix, h_mix = self._gray_recursion(r, mu, omega, alpha, beta, gamma)\n",
    "\n",
    "        # Store\n",
    "        self.mu_ = mu\n",
    "        self.P_ = P\n",
    "        self.gamma_ = gamma\n",
    "        self.h_ = h                 # (T,K) per-state conditional variances\n",
    "        self.h_mix_ = h_mix         # (T,) mixture variance\n",
    "        self.m_mix_ = m_mix         # (T,) mixture mean\n",
    "        self.omega_ = omega; self.alpha_ = alpha; self.beta_ = beta\n",
    "        self.r_ = r\n",
    "        return self\n",
    "\n",
    "    def _gray_recursion(self, r, mu, omega, alpha, beta, pi):\n",
    "        T = len(r); K = len(mu)\n",
    "        h = np.zeros((T, K))\n",
    "        m_mix = np.zeros(T)\n",
    "        H_mix = np.zeros(T)\n",
    "\n",
    "        # initialize with unconditional per state\n",
    "        den = np.maximum(1.0 - (alpha + beta), 1e-3)\n",
    "        h0 = omega / den\n",
    "        h[0] = np.maximum(h0, 1e-8)\n",
    "\n",
    "        m_mix[0] = (pi[0] @ mu)\n",
    "        H_mix[0] = (pi[0] @ h[0])\n",
    "\n",
    "        for t in range(1, T):\n",
    "            m_mix[t] = pi[t-1] @ mu\n",
    "            H_mix[t] = pi[t-1] @ h[t-1]\n",
    "            innov2 = (r[t-1] - m_mix[t])**2\n",
    "            h[t] = omega + alpha * innov2 + beta * H_mix[t]\n",
    "            h[t] = np.maximum(h[t], 1e-12)\n",
    "\n",
    "        h_mix = (pi * h).sum(1)\n",
    "        return h, m_mix, h_mix\n",
    "\n",
    "    # One-step-ahead (no new data)\n",
    "    def predict_next(self):\n",
    "        r_T = self.r_[-1]\n",
    "        m_T = self.m_mix_[-1]\n",
    "        H_T = self.h_mix_[-1]\n",
    "        pi_T = self.gamma_[-1]\n",
    "        mu   = self.mu_\n",
    "        omega, alpha, beta = self.omega_, self.alpha_, self.beta_\n",
    "        h_next = omega + alpha * (r_T - m_T)**2 + beta * H_T\n",
    "        h_next = np.maximum(h_next, 1e-12)\n",
    "        pi_next = pi_T @ self.P_\n",
    "        mean_next = float(pi_next @ mu)\n",
    "        var_next  = float(pi_next @ h_next)\n",
    "        return {\"pi_next\": pi_next, \"h_next_per_state\": h_next,\n",
    "                \"mean_next\": mean_next, \"var_next\": var_next}\n",
    "\n",
    "    def _loglik_state(self, r_new, mu_k, h_k, nu_k=None):\n",
    "        \"\"\"\n",
    "        Log-likelihood of r_new under state k with mean mu_k, variance h_k,\n",
    "        using Normal or Student-t (df=nu_k) innovations.\n",
    "        \"\"\"\n",
    "        h = max(h_k, 1e-12)\n",
    "        x = r_new - mu_k\n",
    "        if self.dist == \"normal\":\n",
    "            # N(mu, h)\n",
    "            return -0.5 * (math.log(2*math.pi*h) + (x*x)/h)\n",
    "        else:\n",
    "            # Student-t with df=nu, using *standard* t scaled so that Var = h\n",
    "            # Standard t(df) with scale s has pdf: log Γ((ν+1)/2) - log(√(νπ) s Γ(ν/2))\n",
    "            #                                  - (ν+1)/2 * log(1 + ((x)/s)^2 / ν)\n",
    "            nu = max(float(nu_k), 2.01)\n",
    "            # choose scale so that Var(X)=h -> s = sqrt(h * ν/(ν-2))\n",
    "            s = math.sqrt(h * nu/(nu-2.0))\n",
    "            z2 = (x/s)**2\n",
    "            return (\n",
    "                math.lgamma((nu+1)/2.0)\n",
    "                - math.lgamma(nu/2.0)\n",
    "                - 0.5*math.log(nu*math.pi) - math.log(s)\n",
    "                - 0.5*(nu+1.0)*math.log(1.0 + z2/nu)\n",
    "            )\n",
    "\n",
    "    # Online filter step (fixed params)\n",
    "    def filter_update(self, r_new):\n",
    "        r_new = float(r_new)\n",
    "        fc = self.predict_next()\n",
    "        h_next = fc[\"h_next_per_state\"]     # per-state conditional variances for the new time\n",
    "        pi_pred = fc[\"pi_next\"]\n",
    "        mu = self.mu_\n",
    "\n",
    "        # per-state log-likelihood\n",
    "        loglik = np.empty(self.k)\n",
    "        for k in range(self.k):\n",
    "            nu_k = None if self.dist == \"normal\" else self.nu_[k]\n",
    "            loglik[k] = self._loglik_state(r_new, mu[k], h_next[k], nu_k=nu_k)\n",
    "\n",
    "        # stabilize\n",
    "        loglik -= loglik.max()\n",
    "        lik = np.exp(loglik)\n",
    "\n",
    "        pi_post = pi_pred * lik\n",
    "        pi_post = pi_post / pi_post.sum()\n",
    "\n",
    "        # append updated series\n",
    "        self.r_ = np.append(self.r_, r_new)\n",
    "        self.gamma_ = np.vstack([self.gamma_, pi_post])\n",
    "\n",
    "        mix_var = float(pi_post @ h_next)\n",
    "        mix_mean = float(pi_post @ mu)\n",
    "        self.h_mix_ = np.append(self.h_mix_, mix_var)\n",
    "        self.m_mix_ = np.append(self.m_mix_, mix_mean)\n",
    "\n",
    "        return {\"pi_post\": pi_post, \"mix_mean\": mix_mean, \"mix_var\": mix_var}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e310645",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_garch_data = vol_data.copy()\n",
    "\n",
    "training_data, testing_data = split_data(ms_garch_data)\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "garches = {}\n",
    "\n",
    "\n",
    "for index in indexes:\n",
    "    r = np.exp(training_data[f'{index}_log_squared_residual'])\n",
    "    garches[index] = MSGARCH(k=3, thresh=0.8, max_iter=1, verbose=True, dist='t')\n",
    "    garches[index].fit(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create backtest df copy\n",
    "bt = predictions.copy()\n",
    "\n",
    "# Set initial conditions\n",
    "starting_cash = 10000\n",
    "bt_data = {}\n",
    "\n",
    "# Model\n",
    "model_name = 'MS_GARCH'\n",
    "\n",
    "trade_futures = False\n",
    "futures_columns = '_F' if trade_futures else ''\n",
    "\n",
    "for index in indexes:\n",
    "    bt_data[index] = {}\n",
    "\n",
    "\n",
    "    bt[f'{index}_portfolio_value_{model_name}'] = 0\n",
    "    bt[f'{index}_cash_{model_name}'] = 0\n",
    "    bt[f'{index}_cash_{model_name}'].iloc[0] = starting_cash\n",
    "    bt[f'{index}_signal_{model_name}'] = 0\n",
    "\n",
    "    bt_data[index][model_name] = {\n",
    "        'last_value': 0,\n",
    "        'last_cash': starting_cash\n",
    "    }\n",
    "\n",
    "# Starts at second day\n",
    "for idx, row in bt.iloc[1:].iterrows():\n",
    "    for index in indexes:\n",
    "        last_value = bt_data[index][model_name]['last_value']\n",
    "        last_cash = bt_data[index][model_name]['last_cash']\n",
    "\n",
    "        close = row[f'{index}_Close']\n",
    "        rf = (1+row['ten_yr'])**(1/252)-1 # Daily\n",
    "\n",
    "        div_dict = {'S&P': 'SPY', 'NASDAQ': 'QQQ', 'DJIA': 'DIA'}\n",
    "        div_name = f'{div_dict[index]}_div'\n",
    "        div = (1+row[div_name])**(1/252)-1 # Daily\n",
    "\n",
    "        fc = garches[index].predict_next()\n",
    "        garches[index].filter_update(row[f'{index}{futures_columns}_log_ret'])\n",
    "\n",
    "        #mu = (fc['mean_next']/100) / close - 1 # Use garch prediction for returns\n",
    "        mu = row[f'{index}_next_ret_pred_enet'] / close - 1 # Use our regression model prediction for returns\n",
    "        #mu = rf - div # Use risk free assumption\n",
    "        vol = np.sqrt(fc['var_next'])\n",
    "        \n",
    "        \n",
    "        investment = 0\n",
    "\n",
    "        # Update portfolio value\n",
    "        current_value = last_value * np.exp(row[f'{index}{futures_columns}_log_ret'])\n",
    "\n",
    "        # Calculate total equity\n",
    "        total_equity = current_value + last_cash \n",
    "\n",
    "        # Get bet sizing with Merton-Kelly\n",
    "        #target_pos = .5 * get_merton_kelly_size(mu, rf, vol) * total_equity # Daily rf\n",
    "        #target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "\n",
    "        # Get bet sizing with vol target\n",
    "        target_pos = get_vol_target_sizing(.1/np.sqrt(252), vol) * total_equity\n",
    "        target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "        \n",
    "        # Bet sizing based on size of potential increase/decrease, signed\n",
    "        #target_pos = (mu - rf)/abs(mu - rf) * total_equity # super leverage\n",
    "        #target_pos = max(-total_equity, min(target_pos, total_equity)) # Make sure we don't exceed equity\n",
    "\n",
    "        # Execute trade / Update investment amount to reach target position\n",
    "        investment = target_pos - current_value\n",
    "\n",
    "        # Update cash\n",
    "        last_cash = bt_data[index][model_name]['last_cash'] = last_cash - investment\n",
    "        bt.loc[idx, f'{index}_cash_{model_name}'] = last_cash\n",
    "\n",
    "        # Store portfolio value as last value for calculation\n",
    "        bt_data[index][model_name]['last_value'] = current_value + investment\n",
    "        bt.loc[idx, f'{index}_portfolio_value_{model_name}'] = bt_data[index][model_name]['last_value']\n",
    "\n",
    "        # Set signal for plotting later\n",
    "        if investment > 0:\n",
    "            bt.loc[idx, f'{index}_signal_{model_name}'] = 1\n",
    "        elif investment < 0:\n",
    "            bt.loc[idx, f'{index}_signal_{model_name}'] = -1\n",
    "\n",
    "    \n",
    "for index in indexes:\n",
    "    bt[f'{index}_total_value_{model_name}'] = bt[f'{index}_portfolio_value_{model_name}'] + bt[f'{index}_cash_{model_name}']\n",
    "    final_value = bt[f'{index}_total_value_{model_name}'].iloc[-1]\n",
    "    print(f'Final value for {index}, {model_name}: {final_value:.2f}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40987d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in indexes:\n",
    "    bt[f'{index}_baseline'] = np.exp(bt[f'{index}_log_ret'].cumsum()) * 10000\n",
    "\n",
    "colors = { 'MS_GARCH': 'red' }\n",
    "\n",
    "# --- Plotting Code ---\n",
    "\n",
    "# Initialize a figure with a row for each index\n",
    "fig = make_subplots(\n",
    "    rows=len(indexes),\n",
    "    cols=1,\n",
    "    subplot_titles=[f'{index} Model Performance' for index in indexes],\n",
    "    shared_xaxes=True # Link the x-axes\n",
    ")\n",
    "\n",
    "# Enumerate through indexes to get the row number (i)\n",
    "for i, index in enumerate(indexes):\n",
    "    # Add the baseline trace for the current index\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=bt.index,\n",
    "        y=bt[f'{index}_baseline'],\n",
    "        mode='lines',\n",
    "        name=f'{index} Baseline',\n",
    "        legendgroup=f'group{i}', # Group legend items by subplot\n",
    "        line=dict(color='blue', width=1)\n",
    "    ), row=i + 1, col=1) # row is 1-indexed\n",
    "\n",
    "    # Add the main performance line for the model\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=bt.index,\n",
    "        y=bt[f'{index}_total_value_{model_name}'],\n",
    "        mode='lines',\n",
    "        name=f'{index} {model_name}',\n",
    "        legendgroup=f'group{i}',\n",
    "        line=dict(color=colors[model_name], width=1.5, dash='dot')\n",
    "    ), row=i + 1, col=1)\n",
    "\n",
    "    # --- Corrected Signal Plotting ---\n",
    "\n",
    "    # Create Series for buy/sell signals\n",
    "    # This puts the portfolio value on the y-axis for the marker, and NaN otherwise\n",
    "    buy_signals_y = bt.loc[bt[f'{index}_signal_{model_name}'] == 1, f'{index}_total_value_{model_name}']\n",
    "    sell_signals_y = bt.loc[bt[f'{index}_signal_{model_name}'] == -1, f'{index}_total_value_{model_name}']\n",
    "\n",
    "\"\"\" These are too crowded\n",
    "        # Add BUY signal markers\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=buy_signals_y.index,\n",
    "            y=buy_signals_y,\n",
    "            mode='markers',\n",
    "            name=f'Buy Signal',\n",
    "            legendgroup=f'group{i}',\n",
    "            marker=dict(size=10, symbol='triangle-up', color='green'),\n",
    "            showlegend=False # Hide from legend to avoid clutter\n",
    "        ), row=i + 1, col=1)\n",
    "\n",
    "        # Add SELL signal markers\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=sell_signals_y.index,\n",
    "            y=sell_signals_y,\n",
    "            mode='markers',\n",
    "            name=f'Sell Signal',\n",
    "            legendgroup=f'group{i}',\n",
    "            marker=dict(size=10, symbol='triangle-down', color='red'),\n",
    "            showlegend=False # Hide from legend to avoid clutter\n",
    "        ), row=i + 1, col=1)\n",
    "        \"\"\"\n",
    "\n",
    "# --- Update the Layout ---\n",
    "fig.update_layout(\n",
    "    title_text='Backtest Performance: Model Comparison by Index',\n",
    "    title_x=0.5, # Center the title\n",
    "    legend_title='Metrics',\n",
    "    height=800 # Adjust height to make plots more readable\n",
    ")\n",
    "\n",
    "# --- Display the Chart ---\n",
    "fig.show()\n",
    "\n",
    "cutoff = math.floor(len(bt)*.8)\n",
    "training_returns = bt.iloc[:cutoff]\n",
    "testing_returns = bt.iloc[cutoff:]\n",
    "\n",
    "for index in indexes:\n",
    "    total_value = f'{index}_total_value_{model_name}'\n",
    "    final_training_value = training_returns[total_value].iloc[-1]\n",
    "    training_cagr = 100 * ((final_training_value/training_returns[total_value].iloc[0])**(1/(training_returns.index[-1].year - training_returns.index[0].year + training_returns.index[-1].month/12))-1)\n",
    "    print(f'Final training value for {index}, {model_name}: {final_training_value}, CAGR: {training_cagr:.2f}%')\n",
    "\n",
    "    final_testing_value = testing_returns[total_value].iloc[-1]\n",
    "    testing_cagr = 100*((final_testing_value/testing_returns[total_value].iloc[0])**(1/(testing_returns.index[-1].year - testing_returns.index[0].year + testing_returns.index[-1].month/12))-1)\n",
    "    print(f'Final testing value for {index}, {model_name}: {final_testing_value}, CAGR: {testing_cagr:.2f}%')\n",
    "\n",
    "    # ---- Training metrics ----\n",
    "    train_curve = training_returns[total_value]\n",
    "    train_sharpe, train_mdd = sharpe_and_mdd(training_returns, total_value, col_rf=\"ten_yr\")\n",
    "    print(f\"Training Sharpe: {train_sharpe:.2f}, Max Drawdown: {train_mdd:.2%}\")\n",
    "\n",
    "    # ---- Testing metrics ----\n",
    "    test_curve = testing_returns[total_value]\n",
    "    test_sharpe, test_mdd = sharpe_and_mdd(testing_returns, total_value, col_rf=\"ten_yr\")\n",
    "    print(f\"Testing Sharpe: {test_sharpe:.2f}, Max Drawdown: {test_mdd:.2%}\")\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2f1f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e228d747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
